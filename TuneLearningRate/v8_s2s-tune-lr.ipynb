{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI_OL6WFnwvQ"
      },
      "source": [
        "# Sketch AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VERSION\n",
        "model_version = 8\n",
        "model_name    = f\"v{model_version}-s2s\"\n",
        "model_bt_file = model_name + \"-bt.pt\"\n",
        "model_bv_file = model_name + \"-bv.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODE\n",
        "TUNE_HIDDEN = False \n",
        "TUNE_LR     = True    \n",
        "TRAIN_MODEL = False\n",
        "LOAD_MODEL  = False\n",
        "TEST_MODEL  = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parametros testeados para tuning:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Division de ejecucion\n",
        "- Eric: Learning Rate\n",
        "- Grover: Hidden sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TUNE HYPERPARAMETERS\n",
        "hidden_size_encoder_ls = [128,256,512]\n",
        "hidden_size_decoder_ls = [256,512,2048]\n",
        "learning_rate_ls       = [1e-2,1e-3,1e-4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL HYPERPARAMETERS\n",
        "#Encoder-Decoder config\n",
        "encoding_size       = 5\n",
        "latent_size         = 128\n",
        "hidden_size_encoder = 512\n",
        "hidden_size_decoder = 2048\n",
        "encoder_layers      = 1\n",
        "decoder_layers      = 1\n",
        "# TRAINING HYPERPARAMETERS\n",
        "num_epochs    = 100\n",
        "batch_size    = 64\n",
        "learning_rate = 1e-3\n",
        "CLASS_WEIGTHS = (1,10,100)\n",
        "MOV_WEIGHT    = 1\n",
        "ACT_WEIGHT    = 10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DATASET CONFIG\n",
        "TRAIN_MAX_SIZE = 6400\n",
        "VALID_MAX_SIZE = 320\n",
        "TEST_MAX_SIZE = 320"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CANVAS CONFIG\n",
        "WIDTH  = 1084\n",
        "HEIGHT = 526"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RANDOM CONFIG\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ENCODING CONFIG\n",
        "DELTA_SIZE = 2\n",
        "CLASS_SIZE = 3\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "START_TOKEN         = (0,0,1,0,0)\n",
        "PAD_MARKER          = -100\n",
        "PAD_TOKEN           = (0,0,PAD_MARKER,PAD_MARKER,PAD_MARKER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H-BhfDgoFDb"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgTu6s6HqYje"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N-RzpO-vjIk",
        "outputId": "06c1b78c-3632-4dd5-9c9a-192aa509a882"
      },
      "outputs": [],
      "source": [
        "%pip install -qU drawsvg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZqKBDt4vjsa"
      },
      "outputs": [],
      "source": [
        "import drawsvg as draw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpmLfCDTTaqA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Runtime Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seed\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9g4wWGQ2rNm",
        "outputId": "8b9a6c58-4a5d-4afe-abdf-57afdbc022a7"
      },
      "outputs": [],
      "source": [
        "#device = torch.device('cpu')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qfnR3ZDoJec"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E72l7SyPrMvR"
      },
      "source": [
        "### Local storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMhkCyxVrBaS"
      },
      "outputs": [],
      "source": [
        "dataset_dir = \"dataset\"\n",
        "dataset_filename = \"dataset.npz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQpM5fWrqxTq"
      },
      "outputs": [],
      "source": [
        "!mkdir $dataset_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jGLIoVqrTHc"
      },
      "source": [
        "### Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAVso7DooI_U"
      },
      "outputs": [],
      "source": [
        "dataset_url = \"https://storage.googleapis.com/quickdraw_dataset/sketchrnn/airplane.npz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3sdqAZfri6l",
        "outputId": "eda3d62a-6b26-4076-c3a9-451f9fae30b5"
      },
      "outputs": [],
      "source": [
        "!curl -o \"$dataset_dir\"/\"$dataset_filename\" \"$dataset_url\"\n",
        "!echo \"FILE SIZE:\"\n",
        "!du -h \"$dataset_dir\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOjjpsUEt6xT"
      },
      "source": [
        "### Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2bXk3Rtt2PI"
      },
      "outputs": [],
      "source": [
        "dataset = np.load(dataset_dir+\"/\"+dataset_filename, encoding='latin1', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRq4oFkMuG0i"
      },
      "outputs": [],
      "source": [
        "raw_train_dataset = dataset[\"train\"][:TRAIN_MAX_SIZE]\n",
        "raw_valid_dataset = dataset[\"valid\"][:VALID_MAX_SIZE]\n",
        "raw_test_dataset = dataset[\"test\"][:TEST_MAX_SIZE]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_len = 0\n",
        "for stroke3 in raw_train_dataset:\n",
        "    max_len = max(max_len,stroke3.shape[0])\n",
        "for stroke3 in raw_valid_dataset:\n",
        "    max_len = max(max_len,stroke3.shape[0])\n",
        "for stroke3 in raw_test_dataset:\n",
        "    max_len = max(max_len,stroke3.shape[0])\n",
        "print(max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ecNdoCgxdxR"
      },
      "outputs": [],
      "source": [
        "def render_sheep(stroke_3):\n",
        "  d = draw.Drawing(WIDTH, HEIGHT, origin=\"center\")\n",
        "  d.append(draw.Rectangle(-WIDTH/2,-HEIGHT/2,WIDTH,HEIGHT, fill='rgb(255,255,255)'))\n",
        "  x=0\n",
        "  y=0\n",
        "  svg_points = [0,0]\n",
        "  for dx,dy,end in stroke_3:\n",
        "    x+=dx\n",
        "    y+=dy\n",
        "    svg_points.append(x)\n",
        "    svg_points.append(y)\n",
        "    if end:\n",
        "      lines = draw.Lines(*svg_points,fill='none', stroke='black')\n",
        "      svg_points = []\n",
        "      d.append(lines)\n",
        "  # use d.as_svg() to extract full svg\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "HtJd6XQFuru9",
        "outputId": "769c795e-f828-490d-fc28-e1a7d55c6b2d"
      },
      "outputs": [],
      "source": [
        "print(\"Train size:\", raw_train_dataset.shape[0])\n",
        "print(\"Test size:\", raw_test_dataset.shape[0])\n",
        "print(\"Validate size:\", raw_valid_dataset.shape[0])\n",
        "print(\"Stroke 3 input shape: n x\",raw_train_dataset[0].shape[1])\n",
        "print(\"*(n max = 250)\")\n",
        "\n",
        "d = render_sheep(raw_train_dataset[5]) #100\n",
        "d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kECiQL_I6Qou"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xa1weZDw8aDj"
      },
      "outputs": [],
      "source": [
        "def render_sequence(sequence):\n",
        "  d = draw.Drawing(WIDTH, HEIGHT, origin=\"center\")\n",
        "  d.append(draw.Rectangle(-WIDTH/2,-HEIGHT/2,WIDTH,HEIGHT, fill='rgb(255,255,255)'))\n",
        "  x=0\n",
        "  y=0\n",
        "  svg_points = []\n",
        "  for i, (dx,dy,line,lift,end) in enumerate(sequence):\n",
        "    x+=dx.item()\n",
        "    y+=dy.item()\n",
        "    svg_points.append(x)\n",
        "    svg_points.append(y)\n",
        "    if lift or end:\n",
        "      lines = draw.Lines(*svg_points,fill='none', stroke='black')\n",
        "      svg_points = []\n",
        "      d.append(lines)\n",
        "    if end:\n",
        "      break\n",
        "    if i == len(sequence)-1:\n",
        "      # Force draw\n",
        "      lines = draw.Lines(*svg_points,fill='none', stroke='black')\n",
        "      svg_points = []\n",
        "      d.append(lines)\n",
        "  # use d.as_svg() to extract full svg\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGCnxaXF6QLb"
      },
      "outputs": [],
      "source": [
        "def preprocess_sequence(stroke_3):\n",
        "  result = [START_TOKEN,]\n",
        "  for i in range(len(stroke_3)):\n",
        "    dx     = stroke_3[i][0]\n",
        "    dy     = stroke_3[i][1]\n",
        "    action = stroke_3[i][2]\n",
        "\n",
        "    end = int(i == len(stroke_3)-1)\n",
        "    lift = action if i != len(stroke_3)-1 else 0\n",
        "    line = 1-action if i != len(stroke_3)-1 else 0\n",
        "\n",
        "    fv = (dx,dy,line,lift,end)\n",
        "    result.append(fv)\n",
        "  # PAD ENDING\n",
        "  for i in range(MAX_SEQUENCE_LENGTH-len(stroke_3)):\n",
        "    fv = PAD_TOKEN\n",
        "    result.append(fv)\n",
        "  return np.array(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3GRelPo9aOO"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(dataset):\n",
        "  new_dataset = np.array([preprocess_sequence(elem) for elem in dataset])\n",
        "  new_dataset = torch.from_numpy(new_dataset)\n",
        "  new_dataset = new_dataset.float()\n",
        "  return new_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3IR_AXX9__7"
      },
      "outputs": [],
      "source": [
        "train_dataset = preprocess_dataset(raw_train_dataset)\n",
        "valid_dataset = preprocess_dataset(raw_valid_dataset)\n",
        "test_dataset = preprocess_dataset(raw_test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmVeQ425Drbc",
        "outputId": "05ce3417-bb9a-44df-8d77-e963f23260ae"
      },
      "outputs": [],
      "source": [
        "#With padding\n",
        "train_dataset.shape\n",
        "train_dataset[0][3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "EdJSwOIr8SD4",
        "outputId": "a87b7d5e-9671-4f13-e3fd-27f4be532b6d"
      },
      "outputs": [],
      "source": [
        "#s = preprocess_sequence(raw_train_dataset[100])\n",
        "s = train_dataset[5]\n",
        "s=s.to(device)\n",
        "print(s)\n",
        "render_sequence(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUyMn7NC-qVy"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Referencias:\n",
        "https://github.com/bentrevett/pytorch-seq2seq/blob/rewrite/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBjsAQsWmN5v"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size_in, rnn_layers, latent_size, dropout=0.1):\n",
        "    super(Encoder,self).__init__()\n",
        "    # Input config\n",
        "    self.input_size  = input_size\n",
        "    # RNN config\n",
        "    self.hidden_size_in  = hidden_size_in\n",
        "    self.latent_size = latent_size\n",
        "    self.rnn_layers  = rnn_layers\n",
        "    # Layers\n",
        "    self.rnn = nn.LSTM(self.input_size,\n",
        "                       self.hidden_size_in,\n",
        "                       self.rnn_layers,\n",
        "                       bidirectional=True)\n",
        "    self.fc_latent = nn.Linear(2*2*self.rnn_layers*self.hidden_size_in,\n",
        "                      self.latent_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x):\n",
        "    # x shape: (seq_length, batch, input_size)\n",
        "    num_batch = x.shape[1]\n",
        "    # RNN\n",
        "    # output shape: (seq_length, batch, D * hidden_size)\n",
        "    # hidden shape: (D * num_layers, batch, hidden_size)\n",
        "    # cell   shape: (D * num_layers, batch, hidden_size)\n",
        "    # * Note: D=2 if bidiretional=True else 1\n",
        "    outputs, (h, c) = self.rnn(x)\n",
        "    # hc   shape: (D * num_layers, batch, 2*hidden_size)\n",
        "    hc = torch.cat( (h,c), 2 )\n",
        "    hc = torch.movedim(hc,0,2)\n",
        "    # hc   shape: (batch, 2*D*num_layers*hidden_size)\n",
        "    hc = hc.reshape(num_batch,-1)\n",
        "    # context  shape: (batch, latent_size)\n",
        "    context =  self.fc_latent(self.dropout(hc))\n",
        "\n",
        "    ### RETURN CONTEXT VECTOR\n",
        "    return context.contiguous()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djBHv5ZRs7_F"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_size, output_size_delta, output_size_class, hidden_size, rnn_layers, latent_size, dropout=0.1):\n",
        "    super(Decoder,self).__init__()\n",
        "    # Input-Output config\n",
        "    self.input_size        = input_size\n",
        "    self.latent_size       = latent_size\n",
        "    self.output_size_delta = output_size_delta\n",
        "    self.output_size_class = output_size_class\n",
        "    # RNN config\n",
        "    self.hidden_size = hidden_size\n",
        "    self.rnn_layers  = rnn_layers\n",
        "    # Layers\n",
        "    self.fc_hidden = nn.Linear(self.latent_size,\n",
        "                      self.hidden_size*self.rnn_layers)\n",
        "    self.fc_cell   = nn.Linear(self.latent_size,\n",
        "                      self.hidden_size*self.rnn_layers)\n",
        "    self.rnn = nn.LSTM(self.input_size+self.latent_size,\n",
        "                       self.hidden_size,\n",
        "                       self.rnn_layers)\n",
        "    self.fc_dlt  = nn.Linear(self.hidden_size,\n",
        "                        self.output_size_delta)\n",
        "    self.fc_cls  = nn.Linear(self.hidden_size,\n",
        "                        self.output_size_class)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x, context, hidden, cell):\n",
        "    ### GET RNN INPUT\n",
        "    # x shape:         (batch, input_size) but need N batches of seq_length 1\n",
        "    x      = x.unsqueeze(0) #(1, batch, input_size)\n",
        "    # context shape:  (batch, latent_size) but need N batches of seq_length 1\n",
        "    context = context.unsqueeze(0) #(1, batch, input_size)\n",
        "    # rnn_input shape: (1, batch, input_size+latent_size)\n",
        "    rnn_input = torch.cat((x, context),2).contiguous()\n",
        "\n",
        "    ### RNN\n",
        "    # output shape: (1, batch, D * hidden_size)\n",
        "    # hidden shape: (D * num_layers, batch, hidden_size)\n",
        "    # cell   shape: (D * num_layers, batch, hidden_size)\n",
        "    # * Note: D=2 if bidiretional=True else 1\n",
        "    output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "\n",
        "    ### GET PREDICTIONS\n",
        "    # pred_delta shape: (1, batch, output_size_delta)\n",
        "    # pred_class shape: (1, batch, output_size_class)\n",
        "    pred_delta =  self.fc_dlt(self.dropout(output))\n",
        "    pred_class =  self.fc_cls(self.dropout((output))) #torch.sigmoid(self.fc_cls(self.dropout((output))))\n",
        "    # pred_delta shape: (batch, output_size_delta)\n",
        "    # pred_class shape: (batch, output_size_class)\n",
        "    pred_delta = pred_delta.squeeze(0)\n",
        "    pred_class = pred_class.squeeze(0)\n",
        "\n",
        "    ### RETURN PREDICTIONS AND STATE\n",
        "    return pred_delta, pred_class, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9oIC3Giw-qH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Stroke2Stroke(nn.Module):\n",
        "  def __init__(self, encoding_size, delta_size, class_size, hidden_size_encoder, hidden_size_decoder, encoder_layers, decoder_layers, latent_size, device, dropout=0.1):\n",
        "    super(Stroke2Stroke, self).__init__()\n",
        "    assert encoding_size == delta_size+class_size\n",
        "    # SIZE CONFIG\n",
        "    self.encoding_size = encoding_size\n",
        "    self.delta_size    = delta_size\n",
        "    self.class_size    = class_size\n",
        "    self.latent_size   = latent_size\n",
        "    # ENCODER/DECODER\n",
        "    # Encoder: (input_size, hidden_size_in, rnn_layers, latent_size, dropout=0.1)\n",
        "    # Decoder: (input_size, output_size_delta, output_size_class, hidden_size, rnn_layers, latent_size, dropout=0.1):\n",
        "    self.encoder = Encoder(encoding_size, hidden_size_encoder, encoder_layers, latent_size)\n",
        "    self.decoder = Decoder(encoding_size, self.delta_size, self.class_size, hidden_size_decoder, decoder_layers, latent_size)\n",
        "    # DEVICE\n",
        "    self.device = device\n",
        "    # INITIAL STATE LAYERS\n",
        "    self.fc_hidden = nn.Linear(self.latent_size,\n",
        "                      hidden_size_decoder*decoder_layers)\n",
        "    self.fc_cell   = nn.Linear(self.latent_size,\n",
        "                      hidden_size_decoder*decoder_layers)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def get_initial_states(self, context):\n",
        "    num_batch = context.shape[0]\n",
        "    ### EXTRACT HIDDEN AND CELL FROM CONTEXT VECTOR\n",
        "    # context shape: (batch, latent_size)\n",
        "    # hidden  shape: (batch, num_layers*hidden_size)\n",
        "    # cell    shape: (batch, num_layers*hidden_size)\n",
        "    hidden =  self.fc_hidden(self.dropout(context))\n",
        "    cell   =  self.fc_cell(self.dropout(context))\n",
        "    # hidden  shape: (num_layers, batch, hidden_size)\n",
        "    # cell    shape: (num_layers, batch, hidden_size)\n",
        "    hidden = hidden.reshape(num_batch,self.decoder.rnn_layers,self.decoder.hidden_size)\n",
        "    cell   = cell.reshape(num_batch,self.decoder.rnn_layers,self.decoder.hidden_size)\n",
        "    hidden = torch.movedim(hidden,0,1).contiguous()\n",
        "    cell   = torch.movedim(cell,0,1).contiguous()\n",
        "    ### RETURN INITIAL STATES\n",
        "    return hidden, cell\n",
        "\n",
        "  def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "    ### GET SHAPES\n",
        "    # source shape: (src_seq_length, batch, encoding_size)\n",
        "    # target shape: (trg_seq_length, batch, encoding_size)\n",
        "    batch_size  = source.shape[1]\n",
        "    target_len  = target.shape[0]\n",
        "    output_size = target.shape[2]\n",
        "\n",
        "    ### RESREVE DELTA AND CLASS PREDICTION TENSORS\n",
        "    # predictions_dlt shape: (src_seq_length, batch, output_size_delta)\n",
        "    # predictions_cls shape: (src_seq_length, batch, output_size_class)\n",
        "    predictions_dlt = torch.zeros(target_len, batch_size, self.delta_size).to(device)\n",
        "    predictions_cls = torch.zeros(target_len, batch_size, self.class_size).to(device)\n",
        "\n",
        "    ### GET CONTEXT FROM ENCODER\n",
        "    # hidden shape: (D * num_layers, batch, hidden_size)\n",
        "    # cell   shape: (D * num_layers, batch, hidden_size)\n",
        "    context = self.encoder(source)\n",
        "\n",
        "    ### GET INITIAL STATES\n",
        "    hidden, cell = self.get_initial_states(context)\n",
        "\n",
        "    ### START TOKEN\n",
        "    x = target[0]\n",
        "    ### GENERATE\n",
        "    for t in range(1, target_len):\n",
        "      # pred_delta shape: (batch, output_size_delta)\n",
        "      # pred_class shape: (batch, output_size_class)\n",
        "      pred_delta, pred_class, hidden, cell = self.decoder(x, context, hidden, cell)\n",
        "      predictions_dlt[t] = pred_delta\n",
        "      predictions_cls[t] = pred_class\n",
        "\n",
        "      # Get max from one hot encoded actions\n",
        "      #action = pred_class.argmax(1)\n",
        "      #best_action_guess = F.one_hot(action, num_classes=self.class_size).float()\n",
        "\n",
        "      # Create best_guess\n",
        "      # best_guess shape: (batch, encoding_size)\n",
        "      #best_guess = torch.cat((pred_delta, best_action_guess),1)\n",
        "      best_guess = torch.cat((pred_delta, pred_class),1)\n",
        "\n",
        "      # Select next input\n",
        "      x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "    return predictions_dlt, predictions_cls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbnW2lw2BW__"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IzsFqHzOdgs",
        "outputId": "607a32ed-e146-4128-8319-3e23ed0eab71"
      },
      "outputs": [],
      "source": [
        "# Parameter count\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKT5p24NS_cT"
      },
      "outputs": [],
      "source": [
        "# DATA LOADERS\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9Ty529kSkHs"
      },
      "outputs": [],
      "source": [
        "# Train function\n",
        "c_weights = torch.tensor(CLASS_WEIGTHS)\n",
        "mse_fn = nn.MSELoss(reduction=\"none\")\n",
        "cross_entropy_fn =  nn.CrossEntropyLoss(ignore_index=PAD_MARKER)#, weight=c_weights)\n",
        "\n",
        "def train_fn(model, data_loader, optimizer, clip=5.0, teacher_forcing_ratio=0.5): #clip=1.0\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_mov_loss = 0\n",
        "    epoch_act_loss = 0\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        # src = [batch_size, src_length, encoding_size]\n",
        "        # trg = [batch_size, trg_length, encoding_size]\n",
        "        src = batch.to(device)\n",
        "        trg = batch.to(device)\n",
        "\n",
        "        # src = [src_length, batch_size, encoding_size]\n",
        "        # trg = [trg_length, batch_size, encoding_size]\n",
        "        src = torch.movedim(src,0,1)\n",
        "        trg = torch.movedim(trg,0,1)\n",
        "\n",
        "        # Zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # predictions_dlt = [trg_length, batch_size, delta_size]\n",
        "        # predictions_cls = [trg_length, batch_size, class_size]\n",
        "        predictions_dlt, predictions_cls = model(src, trg, teacher_forcing_ratio)\n",
        "\n",
        "        encoding_size = src.shape[2]\n",
        "        delta_size    = predictions_dlt.shape[2]\n",
        "        class_size    = predictions_cls.shape[2]\n",
        "\n",
        "\n",
        "        # Compute loss\n",
        "        # mask   = [(trg_length-1), batch_size]\n",
        "        # counts = [1]\n",
        "        mask = ~trg[1:,:,-1].eq(PAD_MARKER)       #[(trg_length-1), batch_size]\n",
        "        counts = torch.sum(mask)                  \n",
        "        ## For one-hot\n",
        "        # action_pred = [(trg length - 1) * batch_size, action_size]\n",
        "        action_pred = predictions_cls[1:].view(-1, class_size)\n",
        "        # action_trg = [(trg length - 1) * batch_size]\n",
        "        action_trg  = trg[1:,:,delta_size:].reshape(-1, class_size) # [(trg length - 1) * batch_size, action_size]\n",
        "        action_trg = action_trg.argmax(1)                           # [(trg length - 1) * batch_size]\n",
        "        # action_mask \n",
        "        action_mask = mask.reshape(-1) # [(trg length - 1) * batch_size]\n",
        "        #            Index  if mask else PAD_MARKER\n",
        "        action_trg = action_trg*action_mask + PAD_MARKER*(~action_mask) # [(trg length - 1) * batch_size]\n",
        "        # ACTION LOSS\n",
        "        action_loss = cross_entropy_fn(action_pred, action_trg)\n",
        "        action_loss = ACT_WEIGHT*action_loss\n",
        "        ## For regression\n",
        "        # movement_prd = [(trg_length-1), batch_size, movement_size]\n",
        "        movement_pred = predictions_dlt[1:]\n",
        "        # movement_trg\n",
        "        movement_trg  = trg[1:,:,:delta_size]\n",
        "        # mv_mask   = [(trg_length-1), batch_size, movement_size]\n",
        "        mv_mask = mask.expand((delta_size,-1,-1))    #[movement_size, (trg_length-1), batch_size]\n",
        "        mv_mask = torch.movedim(mv_mask,0,2)            #[(trg_length-1), batch_size, movement_size]\n",
        "        # Apply mask\n",
        "        # https://discuss.pytorch.org/t/how-to-correctly-weight-mse-loss-for-padded-sequences/176211\n",
        "        movement_pred_masked = movement_pred * mv_mask\n",
        "        movement_trg_masked  = movement_trg  * mv_mask\n",
        "        # Get MSE\n",
        "        movement_loss = mse_fn(movement_pred_masked, movement_trg_masked)\n",
        "        movement_loss = torch.sum(movement_loss)/(counts*DELTA_SIZE)\n",
        "        movement_loss = MOV_WEIGHT*movement_loss\n",
        "        # Total loss\n",
        "        loss = movement_loss + action_loss\n",
        "\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "        # Clip to avoid gradient explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Running total\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_mov_loss += movement_loss.item()\n",
        "        epoch_act_loss += action_loss.item()\n",
        "    return epoch_loss / len(data_loader), epoch_mov_loss/ len(data_loader), epoch_act_loss/ len(data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_fn(model, data_loader):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_mov_loss = 0\n",
        "    epoch_act_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "            src = batch.to(device)\n",
        "            trg = batch.to(device)\n",
        "            src = torch.movedim(src,0,1)\n",
        "            trg = torch.movedim(trg,0,1)\n",
        "            predictions_dlt, predictions_cls = model(src, trg, 0) # turn off teacher forcing\n",
        "\n",
        "            encoding_size = src.shape[2]\n",
        "            delta_size    = predictions_dlt.shape[2]\n",
        "            class_size    = predictions_cls.shape[2]\n",
        "\n",
        "            # Compute loss\n",
        "            # mask   = [(trg_length-1), batch_size]\n",
        "            # counts = [1]\n",
        "            mask = ~trg[1:,:,-1].eq(PAD_MARKER)       #[(trg_length-1), batch_size]\n",
        "            counts = torch.sum(mask)                  \n",
        "            ## For one-hot\n",
        "            # action_pred = [(trg length - 1) * batch_size, action_size]\n",
        "            action_pred = predictions_cls[1:].view(-1, class_size)\n",
        "            # action_trg = [(trg length - 1) * batch_size]\n",
        "            action_trg  = trg[1:,:,delta_size:].reshape(-1, class_size) # [(trg length - 1) * batch_size, action_size]\n",
        "            action_trg = action_trg.argmax(1)                           # [(trg length - 1) * batch_size]\n",
        "            # action_mask \n",
        "            action_mask = mask.reshape(-1) # [(trg length - 1) * batch_size]\n",
        "            #            Index  if mask else PAD_MARKER\n",
        "            action_trg = action_trg*action_mask + PAD_MARKER*(~action_mask) # [(trg length - 1) * batch_size]\n",
        "            # ACTION LOSS\n",
        "            action_loss = cross_entropy_fn(action_pred, action_trg)\n",
        "            action_loss = ACT_WEIGHT*action_loss\n",
        "            ## For regression\n",
        "            # movement_prd = [(trg_length-1), batch_size, movement_size]\n",
        "            movement_pred = predictions_dlt[1:]\n",
        "            # movement_trg\n",
        "            movement_trg  = trg[1:,:,:delta_size]\n",
        "            # mv_mask   = [(trg_length-1), batch_size, movement_size]\n",
        "            mv_mask = mask.expand((delta_size,-1,-1))    #[movement_size, (trg_length-1), batch_size]\n",
        "            mv_mask = torch.movedim(mv_mask,0,2)            #[(trg_length-1), batch_size, movement_size]\n",
        "            # Apply mask\n",
        "            # https://discuss.pytorch.org/t/how-to-correctly-weight-mse-loss-for-padded-sequences/176211\n",
        "            movement_pred_masked = movement_pred * mv_mask\n",
        "            movement_trg_masked  = movement_trg  * mv_mask\n",
        "            # Get MSE\n",
        "            movement_loss = mse_fn(movement_pred_masked, movement_trg_masked)\n",
        "            movement_loss = torch.sum(movement_loss)/(counts*DELTA_SIZE)\n",
        "            movement_loss = MOV_WEIGHT*movement_loss\n",
        "            # Total loss\n",
        "            loss = movement_loss + action_loss\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_mov_loss += movement_loss.item()\n",
        "            epoch_act_loss += action_loss.item()\n",
        "    return epoch_loss / len(data_loader), epoch_mov_loss/ len(data_loader), epoch_act_loss/ len(data_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFijN-Gik-GV",
        "outputId": "f2d92c3f-911b-4145-d2a3-9a8f446ee6a1"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer):\n",
        "  best_valid_loss = float(\"inf\")\n",
        "  best_train_loss = float(\"inf\")\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: [{epoch}/{num_epochs}]\")\n",
        "    \n",
        "    train_loss, train_mov_loss, train_act_loss = train_fn(\n",
        "      model,\n",
        "      train_dataloader,\n",
        "      optimizer,\n",
        "    )\n",
        "    print(f\"Train loss: {train_loss} | Mov Loss {train_mov_loss} | Act Loss {train_act_loss}\")\n",
        "  \n",
        "    valid_loss, valid_mov_loss, valid_act_loss = evaluate_fn(\n",
        "      model,\n",
        "      valid_dataloader,\n",
        "    )\n",
        "    print(f\"Valid loss: {valid_loss} | Mov Loss {valid_mov_loss} | Act Loss {valid_act_loss}\")    \n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      torch.save(model.state_dict(), model_bv_file)\n",
        "\n",
        "    if train_loss < best_train_loss:\n",
        "      best_train_loss = train_loss\n",
        "      torch.save(model.state_dict(), model_bt_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tune Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TUNE_HIDDEN:\n",
        "    for (hidden_size_encoder,hidden_size_decoder) in zip(hidden_size_encoder_ls,hidden_size_decoder_ls):\n",
        "        # Model\n",
        "        model = Stroke2Stroke(encoding_size=encoding_size, delta_size=DELTA_SIZE, class_size=CLASS_SIZE, hidden_size_encoder=hidden_size_encoder, hidden_size_decoder=hidden_size_decoder,\n",
        "                            encoder_layers=encoder_layers,decoder_layers=decoder_layers,latent_size=latent_size,\n",
        "                            device=device).to(device)\n",
        "        print(f\"MODEL:{hidden_size_encoder}he-{hidden_size_decoder}hd\")\n",
        "        print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
        "        model.apply(init_weights)\n",
        "        # Optimizer\n",
        "        optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "        # Set names to save file to\n",
        "        model_bt_file = model_name +f\"-{hidden_size_encoder}he-{hidden_size_decoder}hd\"+ \"-bt.pt\"\n",
        "        model_bv_file = model_name +f\"-{hidden_size_encoder}he-{hidden_size_decoder}hd\"+ \"-bv.pt\"\n",
        "\n",
        "        # TRAIN\n",
        "        train(model,optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TUNE_LR:\n",
        "    for learning_rate in learning_rate_ls:\n",
        "        # Model\n",
        "        model = Stroke2Stroke(encoding_size=encoding_size, delta_size=DELTA_SIZE, class_size=CLASS_SIZE, hidden_size_encoder=hidden_size_encoder, hidden_size_decoder=hidden_size_decoder,\n",
        "                            encoder_layers=encoder_layers,decoder_layers=decoder_layers,latent_size=latent_size,\n",
        "                            device=device).to(device)\n",
        "        print(f\"MODEL:{learning_rate}lr\")\n",
        "        print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
        "        model.apply(init_weights)\n",
        "        # Optimizer\n",
        "        optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "        # Set names to save file to\n",
        "        model_bt_file = model_name +f\"-{learning_rate}lr\"+ \"-bt.pt\"\n",
        "        model_bv_file = model_name +f\"-{learning_rate}lr\"+ \"-bv.pt\"\n",
        "\n",
        "        # TRAIN\n",
        "        train(model,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_MODEL:\n",
        "    # Model\n",
        "    model = Stroke2Stroke(encoding_size=encoding_size, delta_size=DELTA_SIZE, class_size=CLASS_SIZE, hidden_size_encoder=hidden_size_encoder, hidden_size_decoder=hidden_size_decoder,\n",
        "                        encoder_layers=encoder_layers,decoder_layers=decoder_layers,latent_size=latent_size,\n",
        "                        device=device).to(device)\n",
        "    print(f\"MODEL\")\n",
        "    print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
        "    model.apply(init_weights)\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_MODEL:\n",
        "    # Train\n",
        "    train(model,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if LOAD_MODEL:\n",
        "    model.load_state_dict(torch.load(model_bv_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_seq = train_dataset[5].unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUdIUih1BWIX"
      },
      "outputs": [],
      "source": [
        "if TEST_MODEL:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = test_seq.to(device)\n",
        "        trg = test_seq.to(device)\n",
        "        src = torch.movedim(src,0,1)\n",
        "        trg = torch.movedim(trg,0,1)\n",
        "        prediction_dlt, prediction_act = model(src, trg, 0) # turn off teacher forcing\n",
        "        prediction = torch.cat( (prediction_dlt,prediction_act) ,2)\n",
        "        prediction = prediction.squeeze()\n",
        "        action = prediction[:,DELTA_SIZE:]\n",
        "        action = action.argmax(dim=1)\n",
        "        best_guess = F.one_hot(DELTA_SIZE+action, num_classes=5).float()\n",
        "        best_guess[:,:DELTA_SIZE] = prediction[:,:DELTA_SIZE]\n",
        "    best_guess = best_guess[1:]\n",
        "    render_sequence(best_guess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TEST_MODEL:\n",
        "    render_sequence(test_seq[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
