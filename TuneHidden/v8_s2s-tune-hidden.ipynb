{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI_OL6WFnwvQ"
      },
      "source": [
        "# Sketch AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VERSION\n",
        "model_version = 8\n",
        "model_name    = f\"v{model_version}-s2s\"\n",
        "model_bt_file = model_name + \"-bt.pt\"\n",
        "model_bv_file = model_name + \"-bv.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODE\n",
        "TUNE_HIDDEN = True   \n",
        "TUNE_LR     = False \n",
        "TRAIN_MODEL = False\n",
        "LOAD_MODEL  = False\n",
        "TEST_MODEL  = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parametros testeados para tuning:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Division de ejecucion\n",
        "- Eric: Learning Rate\n",
        "- Grover: Hidden sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TUNE HYPERPARAMETERS\n",
        "hidden_size_encoder_ls = [128,256,512]\n",
        "hidden_size_decoder_ls = [256,512,2048]\n",
        "learning_rate_ls       = [1e-2,1e-3,1e-4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL HYPERPARAMETERS\n",
        "#Encoder-Decoder config\n",
        "encoding_size       = 5\n",
        "latent_size         = 128\n",
        "hidden_size_encoder = 512\n",
        "hidden_size_decoder = 2048\n",
        "encoder_layers      = 1\n",
        "decoder_layers      = 1\n",
        "# TRAINING HYPERPARAMETERS\n",
        "num_epochs    = 100\n",
        "batch_size    = 64\n",
        "learning_rate = 1e-3\n",
        "CLASS_WEIGTHS = (1,10,100)\n",
        "MOV_WEIGHT    = 1\n",
        "ACT_WEIGHT    = 10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DATASET CONFIG\n",
        "TRAIN_MAX_SIZE = 6400\n",
        "VALID_MAX_SIZE = 320\n",
        "TEST_MAX_SIZE = 320"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CANVAS CONFIG\n",
        "WIDTH  = 1084\n",
        "HEIGHT = 526"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RANDOM CONFIG\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ENCODING CONFIG\n",
        "DELTA_SIZE = 2\n",
        "CLASS_SIZE = 3\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "START_TOKEN         = (0,0,1,0,0)\n",
        "PAD_MARKER          = -100\n",
        "PAD_TOKEN           = (0,0,PAD_MARKER,PAD_MARKER,PAD_MARKER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H-BhfDgoFDb"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kgTu6s6HqYje"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N-RzpO-vjIk",
        "outputId": "06c1b78c-3632-4dd5-9c9a-192aa509a882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU drawsvg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LZqKBDt4vjsa"
      },
      "outputs": [],
      "source": [
        "import drawsvg as draw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZpmLfCDTTaqA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Runtime Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seed\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9g4wWGQ2rNm",
        "outputId": "8b9a6c58-4a5d-4afe-abdf-57afdbc022a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#device = torch.device('cpu')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qfnR3ZDoJec"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E72l7SyPrMvR"
      },
      "source": [
        "### Local storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sMhkCyxVrBaS"
      },
      "outputs": [],
      "source": [
        "dataset_dir = \"dataset\"\n",
        "dataset_filename = \"dataset.npz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SQpM5fWrqxTq"
      },
      "outputs": [],
      "source": [
        "!mkdir $dataset_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jGLIoVqrTHc"
      },
      "source": [
        "### Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rAVso7DooI_U"
      },
      "outputs": [],
      "source": [
        "dataset_url = \"https://storage.googleapis.com/quickdraw_dataset/sketchrnn/airplane.npz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3sdqAZfri6l",
        "outputId": "eda3d62a-6b26-4076-c3a9-451f9fae30b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0 11.0M    0 17904    0     0  19473      0  0:09:55 --:--:--  0:09:55 19545\n",
            "100 11.0M  100 11.0M    0     0  7248k      0  0:00:01  0:00:01 --:--:-- 7271k\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"FILE SIZE:\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\"du\" no se reconoce como un comando interno o externo,\n",
            "programa o archivo por lotes ejecutable.\n"
          ]
        }
      ],
      "source": [
        "!curl -o \"$dataset_dir\"/\"$dataset_filename\" \"$dataset_url\"\n",
        "!echo \"FILE SIZE:\"\n",
        "!du -h \"$dataset_dir\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOjjpsUEt6xT"
      },
      "source": [
        "### Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "N2bXk3Rtt2PI"
      },
      "outputs": [],
      "source": [
        "dataset = np.load(dataset_dir+\"/\"+dataset_filename, encoding='latin1', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dRq4oFkMuG0i"
      },
      "outputs": [],
      "source": [
        "raw_train_dataset = dataset[\"train\"][:TRAIN_MAX_SIZE]\n",
        "raw_valid_dataset = dataset[\"valid\"][:VALID_MAX_SIZE]\n",
        "raw_test_dataset = dataset[\"test\"][:TEST_MAX_SIZE]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99\n"
          ]
        }
      ],
      "source": [
        "max_len = 0\n",
        "for stroke3 in raw_train_dataset:\n",
        "    max_len = max(max_len,stroke3.shape[0])\n",
        "for stroke3 in raw_valid_dataset:\n",
        "    max_len = max(max_len,stroke3.shape[0])\n",
        "for stroke3 in raw_test_dataset:\n",
        "    max_len = max(max_len,stroke3.shape[0])\n",
        "print(max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6ecNdoCgxdxR"
      },
      "outputs": [],
      "source": [
        "def render_sheep(stroke_3):\n",
        "  d = draw.Drawing(WIDTH, HEIGHT, origin=\"center\")\n",
        "  d.append(draw.Rectangle(-WIDTH/2,-HEIGHT/2,WIDTH,HEIGHT, fill='rgb(255,255,255)'))\n",
        "  x=0\n",
        "  y=0\n",
        "  svg_points = [0,0]\n",
        "  for dx,dy,end in stroke_3:\n",
        "    x+=dx\n",
        "    y+=dy\n",
        "    svg_points.append(x)\n",
        "    svg_points.append(y)\n",
        "    if end:\n",
        "      lines = draw.Lines(*svg_points,fill='none', stroke='black')\n",
        "      svg_points = []\n",
        "      d.append(lines)\n",
        "  # use d.as_svg() to extract full svg\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "HtJd6XQFuru9",
        "outputId": "769c795e-f828-490d-fc28-e1a7d55c6b2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 6400\n",
            "Test size: 320\n",
            "Validate size: 320\n",
            "Stroke 3 input shape: n x 3\n",
            "*(n max = 250)\n"
          ]
        },
        {
          "data": {
            "image/svg+xml": [
              "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
              "     width=\"1084\" height=\"526\" viewBox=\"-542.0 -263.0 1084 526\">\n",
              "<defs>\n",
              "</defs>\n",
              "<rect x=\"-542.0\" y=\"-263.0\" width=\"1084\" height=\"526\" fill=\"rgb(255,255,255)\" />\n",
              "<path d=\"M0,0 L48,0 L154,12 L297,37 L337,48 L406,77 L434,95 L398,133 L372,149 L349,154 L298,153 L210,136 L102,125 L-32,101 L-120,94 L-153,82 L-163,71 L-161,57 L-143,19 L-121,1 L-103,-6 L-72,-6 L-42,9 L-25,12 L-1,12 L22,3\" fill=\"none\" stroke=\"black\" />\n",
              "<path d=\"M97,-9 L230,-181 L247,-208 L253,-92 L251,10\" fill=\"none\" stroke=\"black\" />\n",
              "<path d=\"M69,92 L-2,184 L-81,295 L-74,294 L-40,276 L33,231 L128,178 L161,154 L185,129 L202,116\" fill=\"none\" stroke=\"black\" />\n",
              "<path d=\"M323,69 L321,97 L326,101 L340,106 L386,113\" fill=\"none\" stroke=\"black\" />\n",
              "</svg>"
            ],
            "text/plain": [
              "<drawsvg.drawing.Drawing at 0x158f6685690>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Train size:\", raw_train_dataset.shape[0])\n",
        "print(\"Test size:\", raw_test_dataset.shape[0])\n",
        "print(\"Validate size:\", raw_valid_dataset.shape[0])\n",
        "print(\"Stroke 3 input shape: n x\",raw_train_dataset[0].shape[1])\n",
        "print(\"*(n max = 250)\")\n",
        "\n",
        "d = render_sheep(raw_train_dataset[5]) #100\n",
        "d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kECiQL_I6Qou"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Xa1weZDw8aDj"
      },
      "outputs": [],
      "source": [
        "def render_sequence(sequence):\n",
        "  d = draw.Drawing(WIDTH, HEIGHT, origin=\"center\")\n",
        "  d.append(draw.Rectangle(-WIDTH/2,-HEIGHT/2,WIDTH,HEIGHT, fill='rgb(255,255,255)'))\n",
        "  x=0\n",
        "  y=0\n",
        "  svg_points = []\n",
        "  for i, (dx,dy,line,lift,end) in enumerate(sequence):\n",
        "    x+=dx.item()\n",
        "    y+=dy.item()\n",
        "    svg_points.append(x)\n",
        "    svg_points.append(y)\n",
        "    if lift or end:\n",
        "      lines = draw.Lines(*svg_points,fill='none', stroke='black')\n",
        "      svg_points = []\n",
        "      d.append(lines)\n",
        "    if end:\n",
        "      break\n",
        "    if i == len(sequence)-1:\n",
        "      # Force draw\n",
        "      lines = draw.Lines(*svg_points,fill='none', stroke='black')\n",
        "      svg_points = []\n",
        "      d.append(lines)\n",
        "  # use d.as_svg() to extract full svg\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "eGCnxaXF6QLb"
      },
      "outputs": [],
      "source": [
        "def preprocess_sequence(stroke_3):\n",
        "  result = [START_TOKEN,]\n",
        "  for i in range(len(stroke_3)):\n",
        "    dx     = stroke_3[i][0]\n",
        "    dy     = stroke_3[i][1]\n",
        "    action = stroke_3[i][2]\n",
        "\n",
        "    end = int(i == len(stroke_3)-1)\n",
        "    lift = action if i != len(stroke_3)-1 else 0\n",
        "    line = 1-action if i != len(stroke_3)-1 else 0\n",
        "\n",
        "    fv = (dx,dy,line,lift,end)\n",
        "    result.append(fv)\n",
        "  # PAD ENDING\n",
        "  for i in range(MAX_SEQUENCE_LENGTH-len(stroke_3)):\n",
        "    fv = PAD_TOKEN\n",
        "    result.append(fv)\n",
        "  return np.array(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "y3GRelPo9aOO"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(dataset):\n",
        "  new_dataset = np.array([preprocess_sequence(elem) for elem in dataset])\n",
        "  new_dataset = torch.from_numpy(new_dataset)\n",
        "  new_dataset = new_dataset.float()\n",
        "  return new_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Y3IR_AXX9__7"
      },
      "outputs": [],
      "source": [
        "train_dataset = preprocess_dataset(raw_train_dataset)\n",
        "valid_dataset = preprocess_dataset(raw_valid_dataset)\n",
        "test_dataset = preprocess_dataset(raw_test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmVeQ425Drbc",
        "outputId": "05ce3417-bb9a-44df-8d77-e963f23260ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-24.,  -6.,   1.,   0.,   0.])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#With padding\n",
        "train_dataset.shape\n",
        "train_dataset[0][3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "EdJSwOIr8SD4",
        "outputId": "a87b7d5e-9671-4f13-e3fd-27f4be532b6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[   0.,    0.,    1.,    0.,    0.],\n",
            "        [  48.,    0.,    1.,    0.,    0.],\n",
            "        [ 106.,   12.,    1.,    0.,    0.],\n",
            "        [ 143.,   25.,    1.,    0.,    0.],\n",
            "        [  40.,   11.,    1.,    0.,    0.],\n",
            "        [  69.,   29.,    1.,    0.,    0.],\n",
            "        [  28.,   18.,    1.,    0.,    0.],\n",
            "        [ -36.,   38.,    1.,    0.,    0.],\n",
            "        [ -26.,   16.,    1.,    0.,    0.],\n",
            "        [ -23.,    5.,    1.,    0.,    0.],\n",
            "        [ -51.,   -1.,    1.,    0.,    0.],\n",
            "        [ -88.,  -17.,    1.,    0.,    0.],\n",
            "        [-108.,  -11.,    1.,    0.,    0.],\n",
            "        [-134.,  -24.,    1.,    0.,    0.],\n",
            "        [ -88.,   -7.,    1.,    0.,    0.],\n",
            "        [ -33.,  -12.,    1.,    0.,    0.],\n",
            "        [ -10.,  -11.,    1.,    0.,    0.],\n",
            "        [   2.,  -14.,    1.,    0.,    0.],\n",
            "        [  18.,  -38.,    1.,    0.,    0.],\n",
            "        [  22.,  -18.,    1.,    0.,    0.],\n",
            "        [  18.,   -7.,    1.,    0.,    0.],\n",
            "        [  31.,    0.,    1.,    0.,    0.],\n",
            "        [  30.,   15.,    1.,    0.,    0.],\n",
            "        [  17.,    3.,    1.,    0.,    0.],\n",
            "        [  24.,    0.,    1.,    0.,    0.],\n",
            "        [  23.,   -9.,    0.,    1.,    0.],\n",
            "        [  75.,  -12.,    1.,    0.,    0.],\n",
            "        [ 133., -172.,    1.,    0.,    0.],\n",
            "        [  17.,  -27.,    1.,    0.,    0.],\n",
            "        [   6.,  116.,    1.,    0.,    0.],\n",
            "        [  -2.,  102.,    0.,    1.,    0.],\n",
            "        [-182.,   82.,    1.,    0.,    0.],\n",
            "        [ -71.,   92.,    1.,    0.,    0.],\n",
            "        [ -79.,  111.,    1.,    0.,    0.],\n",
            "        [   7.,   -1.,    1.,    0.,    0.],\n",
            "        [  34.,  -18.,    1.,    0.,    0.],\n",
            "        [  73.,  -45.,    1.,    0.,    0.],\n",
            "        [  95.,  -53.,    1.,    0.,    0.],\n",
            "        [  33.,  -24.,    1.,    0.,    0.],\n",
            "        [  24.,  -25.,    1.,    0.,    0.],\n",
            "        [  17.,  -13.,    0.,    1.,    0.],\n",
            "        [ 121.,  -47.,    1.,    0.,    0.],\n",
            "        [  -2.,   28.,    1.,    0.,    0.],\n",
            "        [   5.,    4.,    1.,    0.,    0.],\n",
            "        [  14.,    5.,    1.,    0.,    0.],\n",
            "        [  46.,    7.,    0.,    0.,    1.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.]], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/svg+xml": [
              "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
              "     width=\"1084\" height=\"526\" viewBox=\"-542.0 -263.0 1084 526\">\n",
              "<defs>\n",
              "</defs>\n",
              "<rect x=\"-542.0\" y=\"-263.0\" width=\"1084\" height=\"526\" fill=\"rgb(255,255,255)\" />\n",
              "<path d=\"M0.0,0.0 L48.0,0.0 L154.0,12.0 L297.0,37.0 L337.0,48.0 L406.0,77.0 L434.0,95.0 L398.0,133.0 L372.0,149.0 L349.0,154.0 L298.0,153.0 L210.0,136.0 L102.0,125.0 L-32.0,101.0 L-120.0,94.0 L-153.0,82.0 L-163.0,71.0 L-161.0,57.0 L-143.0,19.0 L-121.0,1.0 L-103.0,-6.0 L-72.0,-6.0 L-42.0,9.0 L-25.0,12.0 L-1.0,12.0 L22.0,3.0\" fill=\"none\" stroke=\"black\" />\n",
              "<path d=\"M97.0,-9.0 L230.0,-181.0 L247.0,-208.0 L253.0,-92.0 L251.0,10.0\" fill=\"none\" stroke=\"black\" />\n",
              "<path d=\"M69.0,92.0 L-2.0,184.0 L-81.0,295.0 L-74.0,294.0 L-40.0,276.0 L33.0,231.0 L128.0,178.0 L161.0,154.0 L185.0,129.0 L202.0,116.0\" fill=\"none\" stroke=\"black\" />\n",
              "<path d=\"M323.0,69.0 L321.0,97.0 L326.0,101.0 L340.0,106.0 L386.0,113.0\" fill=\"none\" stroke=\"black\" />\n",
              "</svg>"
            ],
            "text/plain": [
              "<drawsvg.drawing.Drawing at 0x158ecae3290>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#s = preprocess_sequence(raw_train_dataset[100])\n",
        "s = train_dataset[5]\n",
        "s=s.to(device)\n",
        "print(s)\n",
        "render_sequence(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUyMn7NC-qVy"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Referencias:\n",
        "https://github.com/bentrevett/pytorch-seq2seq/blob/rewrite/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gBjsAQsWmN5v"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size_in, rnn_layers, latent_size, dropout=0.1):\n",
        "    super(Encoder,self).__init__()\n",
        "    # Input config\n",
        "    self.input_size  = input_size\n",
        "    # RNN config\n",
        "    self.hidden_size_in  = hidden_size_in\n",
        "    self.latent_size = latent_size\n",
        "    self.rnn_layers  = rnn_layers\n",
        "    # Layers\n",
        "    self.rnn = nn.LSTM(self.input_size,\n",
        "                       self.hidden_size_in,\n",
        "                       self.rnn_layers,\n",
        "                       bidirectional=True)\n",
        "    self.fc_latent = nn.Linear(2*2*self.rnn_layers*self.hidden_size_in,\n",
        "                      self.latent_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x):\n",
        "    # x shape: (seq_length, batch, input_size)\n",
        "    num_batch = x.shape[1]\n",
        "    # RNN\n",
        "    # output shape: (seq_length, batch, D * hidden_size)\n",
        "    # hidden shape: (D * num_layers, batch, hidden_size)\n",
        "    # cell   shape: (D * num_layers, batch, hidden_size)\n",
        "    # * Note: D=2 if bidiretional=True else 1\n",
        "    outputs, (h, c) = self.rnn(x)\n",
        "    # hc   shape: (D * num_layers, batch, 2*hidden_size)\n",
        "    hc = torch.cat( (h,c), 2 )\n",
        "    hc = torch.movedim(hc,0,2)\n",
        "    # hc   shape: (batch, 2*D*num_layers*hidden_size)\n",
        "    hc = hc.reshape(num_batch,-1)\n",
        "    # context  shape: (batch, latent_size)\n",
        "    context =  self.fc_latent(self.dropout(hc))\n",
        "\n",
        "    ### RETURN CONTEXT VECTOR\n",
        "    return context.contiguous()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "djBHv5ZRs7_F"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_size, output_size_delta, output_size_class, hidden_size, rnn_layers, latent_size, dropout=0.1):\n",
        "    super(Decoder,self).__init__()\n",
        "    # Input-Output config\n",
        "    self.input_size        = input_size\n",
        "    self.latent_size       = latent_size\n",
        "    self.output_size_delta = output_size_delta\n",
        "    self.output_size_class = output_size_class\n",
        "    # RNN config\n",
        "    self.hidden_size = hidden_size\n",
        "    self.rnn_layers  = rnn_layers\n",
        "    # Layers\n",
        "    self.fc_hidden = nn.Linear(self.latent_size,\n",
        "                      self.hidden_size*self.rnn_layers)\n",
        "    self.fc_cell   = nn.Linear(self.latent_size,\n",
        "                      self.hidden_size*self.rnn_layers)\n",
        "    self.rnn = nn.LSTM(self.input_size+self.latent_size,\n",
        "                       self.hidden_size,\n",
        "                       self.rnn_layers)\n",
        "    self.fc_dlt  = nn.Linear(self.hidden_size,\n",
        "                        self.output_size_delta)\n",
        "    self.fc_cls  = nn.Linear(self.hidden_size,\n",
        "                        self.output_size_class)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x, context, hidden, cell):\n",
        "    ### GET RNN INPUT\n",
        "    # x shape:         (batch, input_size) but need N batches of seq_length 1\n",
        "    x      = x.unsqueeze(0) #(1, batch, input_size)\n",
        "    # context shape:  (batch, latent_size) but need N batches of seq_length 1\n",
        "    context = context.unsqueeze(0) #(1, batch, input_size)\n",
        "    # rnn_input shape: (1, batch, input_size+latent_size)\n",
        "    rnn_input = torch.cat((x, context),2).contiguous()\n",
        "\n",
        "    ### RNN\n",
        "    # output shape: (1, batch, D * hidden_size)\n",
        "    # hidden shape: (D * num_layers, batch, hidden_size)\n",
        "    # cell   shape: (D * num_layers, batch, hidden_size)\n",
        "    # * Note: D=2 if bidiretional=True else 1\n",
        "    output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "\n",
        "    ### GET PREDICTIONS\n",
        "    # pred_delta shape: (1, batch, output_size_delta)\n",
        "    # pred_class shape: (1, batch, output_size_class)\n",
        "    pred_delta =  self.fc_dlt(self.dropout(output))\n",
        "    pred_class =  self.fc_cls(self.dropout((output))) #torch.sigmoid(self.fc_cls(self.dropout((output))))\n",
        "    # pred_delta shape: (batch, output_size_delta)\n",
        "    # pred_class shape: (batch, output_size_class)\n",
        "    pred_delta = pred_delta.squeeze(0)\n",
        "    pred_class = pred_class.squeeze(0)\n",
        "\n",
        "    ### RETURN PREDICTIONS AND STATE\n",
        "    return pred_delta, pred_class, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "b9oIC3Giw-qH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Stroke2Stroke(nn.Module):\n",
        "  def __init__(self, encoding_size, delta_size, class_size, hidden_size_encoder, hidden_size_decoder, encoder_layers, decoder_layers, latent_size, device, dropout=0.1):\n",
        "    super(Stroke2Stroke, self).__init__()\n",
        "    assert encoding_size == delta_size+class_size\n",
        "    # SIZE CONFIG\n",
        "    self.encoding_size = encoding_size\n",
        "    self.delta_size    = delta_size\n",
        "    self.class_size    = class_size\n",
        "    self.latent_size   = latent_size\n",
        "    # ENCODER/DECODER\n",
        "    # Encoder: (input_size, hidden_size_in, rnn_layers, latent_size, dropout=0.1)\n",
        "    # Decoder: (input_size, output_size_delta, output_size_class, hidden_size, rnn_layers, latent_size, dropout=0.1):\n",
        "    self.encoder = Encoder(encoding_size, hidden_size_encoder, encoder_layers, latent_size)\n",
        "    self.decoder = Decoder(encoding_size, self.delta_size, self.class_size, hidden_size_decoder, decoder_layers, latent_size)\n",
        "    # DEVICE\n",
        "    self.device = device\n",
        "    # INITIAL STATE LAYERS\n",
        "    self.fc_hidden = nn.Linear(self.latent_size,\n",
        "                      hidden_size_decoder*decoder_layers)\n",
        "    self.fc_cell   = nn.Linear(self.latent_size,\n",
        "                      hidden_size_decoder*decoder_layers)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def get_initial_states(self, context):\n",
        "    num_batch = context.shape[0]\n",
        "    ### EXTRACT HIDDEN AND CELL FROM CONTEXT VECTOR\n",
        "    # context shape: (batch, latent_size)\n",
        "    # hidden  shape: (batch, num_layers*hidden_size)\n",
        "    # cell    shape: (batch, num_layers*hidden_size)\n",
        "    hidden =  self.fc_hidden(self.dropout(context))\n",
        "    cell   =  self.fc_cell(self.dropout(context))\n",
        "    # hidden  shape: (num_layers, batch, hidden_size)\n",
        "    # cell    shape: (num_layers, batch, hidden_size)\n",
        "    hidden = hidden.reshape(num_batch,self.decoder.rnn_layers,self.decoder.hidden_size)\n",
        "    cell   = cell.reshape(num_batch,self.decoder.rnn_layers,self.decoder.hidden_size)\n",
        "    hidden = torch.movedim(hidden,0,1).contiguous()\n",
        "    cell   = torch.movedim(cell,0,1).contiguous()\n",
        "    ### RETURN INITIAL STATES\n",
        "    return hidden, cell\n",
        "\n",
        "  def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "    ### GET SHAPES\n",
        "    # source shape: (src_seq_length, batch, encoding_size)\n",
        "    # target shape: (trg_seq_length, batch, encoding_size)\n",
        "    batch_size  = source.shape[1]\n",
        "    target_len  = target.shape[0]\n",
        "    output_size = target.shape[2]\n",
        "\n",
        "    ### RESREVE DELTA AND CLASS PREDICTION TENSORS\n",
        "    # predictions_dlt shape: (src_seq_length, batch, output_size_delta)\n",
        "    # predictions_cls shape: (src_seq_length, batch, output_size_class)\n",
        "    predictions_dlt = torch.zeros(target_len, batch_size, self.delta_size).to(device)\n",
        "    predictions_cls = torch.zeros(target_len, batch_size, self.class_size).to(device)\n",
        "\n",
        "    ### GET CONTEXT FROM ENCODER\n",
        "    # hidden shape: (D * num_layers, batch, hidden_size)\n",
        "    # cell   shape: (D * num_layers, batch, hidden_size)\n",
        "    context = self.encoder(source)\n",
        "\n",
        "    ### GET INITIAL STATES\n",
        "    hidden, cell = self.get_initial_states(context)\n",
        "\n",
        "    ### START TOKEN\n",
        "    x = target[0]\n",
        "    ### GENERATE\n",
        "    for t in range(1, target_len):\n",
        "      # pred_delta shape: (batch, output_size_delta)\n",
        "      # pred_class shape: (batch, output_size_class)\n",
        "      pred_delta, pred_class, hidden, cell = self.decoder(x, context, hidden, cell)\n",
        "      predictions_dlt[t] = pred_delta\n",
        "      predictions_cls[t] = pred_class\n",
        "\n",
        "      # Get max from one hot encoded actions\n",
        "      #action = pred_class.argmax(1)\n",
        "      #best_action_guess = F.one_hot(action, num_classes=self.class_size).float()\n",
        "\n",
        "      # Create best_guess\n",
        "      # best_guess shape: (batch, encoding_size)\n",
        "      #best_guess = torch.cat((pred_delta, best_action_guess),1)\n",
        "      best_guess = torch.cat((pred_delta, pred_class),1)\n",
        "\n",
        "      # Select next input\n",
        "      x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "    return predictions_dlt, predictions_cls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbnW2lw2BW__"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IzsFqHzOdgs",
        "outputId": "607a32ed-e146-4128-8319-3e23ed0eab71"
      },
      "outputs": [],
      "source": [
        "# Parameter count\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "bKT5p24NS_cT"
      },
      "outputs": [],
      "source": [
        "# DATA LOADERS\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "G9Ty529kSkHs"
      },
      "outputs": [],
      "source": [
        "# Train function\n",
        "c_weights = torch.tensor(CLASS_WEIGTHS)\n",
        "mse_fn = nn.MSELoss(reduction=\"none\")\n",
        "cross_entropy_fn =  nn.CrossEntropyLoss(ignore_index=PAD_MARKER)#, weight=c_weights)\n",
        "\n",
        "def train_fn(model, data_loader, optimizer, clip=5.0, teacher_forcing_ratio=0.5): #clip=1.0\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_mov_loss = 0\n",
        "    epoch_act_loss = 0\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        # src = [batch_size, src_length, encoding_size]\n",
        "        # trg = [batch_size, trg_length, encoding_size]\n",
        "        src = batch.to(device)\n",
        "        trg = batch.to(device)\n",
        "\n",
        "        # src = [src_length, batch_size, encoding_size]\n",
        "        # trg = [trg_length, batch_size, encoding_size]\n",
        "        src = torch.movedim(src,0,1)\n",
        "        trg = torch.movedim(trg,0,1)\n",
        "\n",
        "        # Zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # predictions_dlt = [trg_length, batch_size, delta_size]\n",
        "        # predictions_cls = [trg_length, batch_size, class_size]\n",
        "        predictions_dlt, predictions_cls = model(src, trg, teacher_forcing_ratio)\n",
        "\n",
        "        encoding_size = src.shape[2]\n",
        "        delta_size    = predictions_dlt.shape[2]\n",
        "        class_size    = predictions_cls.shape[2]\n",
        "\n",
        "\n",
        "        # Compute loss\n",
        "        # mask   = [(trg_length-1), batch_size]\n",
        "        # counts = [1]\n",
        "        mask = ~trg[1:,:,-1].eq(PAD_MARKER)       #[(trg_length-1), batch_size]\n",
        "        counts = torch.sum(mask)                  \n",
        "        ## For one-hot\n",
        "        # action_pred = [(trg length - 1) * batch_size, action_size]\n",
        "        action_pred = predictions_cls[1:].view(-1, class_size)\n",
        "        # action_trg = [(trg length - 1) * batch_size]\n",
        "        action_trg  = trg[1:,:,delta_size:].reshape(-1, class_size) # [(trg length - 1) * batch_size, action_size]\n",
        "        action_trg = action_trg.argmax(1)                           # [(trg length - 1) * batch_size]\n",
        "        # action_mask \n",
        "        action_mask = mask.reshape(-1) # [(trg length - 1) * batch_size]\n",
        "        #            Index  if mask else PAD_MARKER\n",
        "        action_trg = action_trg*action_mask + PAD_MARKER*(~action_mask) # [(trg length - 1) * batch_size]\n",
        "        # ACTION LOSS\n",
        "        action_loss = cross_entropy_fn(action_pred, action_trg)\n",
        "        action_loss = ACT_WEIGHT*action_loss\n",
        "        ## For regression\n",
        "        # movement_prd = [(trg_length-1), batch_size, movement_size]\n",
        "        movement_pred = predictions_dlt[1:]\n",
        "        # movement_trg\n",
        "        movement_trg  = trg[1:,:,:delta_size]\n",
        "        # mv_mask   = [(trg_length-1), batch_size, movement_size]\n",
        "        mv_mask = mask.expand((delta_size,-1,-1))    #[movement_size, (trg_length-1), batch_size]\n",
        "        mv_mask = torch.movedim(mv_mask,0,2)            #[(trg_length-1), batch_size, movement_size]\n",
        "        # Apply mask\n",
        "        # https://discuss.pytorch.org/t/how-to-correctly-weight-mse-loss-for-padded-sequences/176211\n",
        "        movement_pred_masked = movement_pred * mv_mask\n",
        "        movement_trg_masked  = movement_trg  * mv_mask\n",
        "        # Get MSE\n",
        "        movement_loss = mse_fn(movement_pred_masked, movement_trg_masked)\n",
        "        movement_loss = torch.sum(movement_loss)/(counts*DELTA_SIZE)\n",
        "        movement_loss = MOV_WEIGHT*movement_loss\n",
        "        # Total loss\n",
        "        loss = movement_loss + action_loss\n",
        "\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "        # Clip to avoid gradient explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Running total\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_mov_loss += movement_loss.item()\n",
        "        epoch_act_loss += action_loss.item()\n",
        "    return epoch_loss / len(data_loader), epoch_mov_loss/ len(data_loader), epoch_act_loss/ len(data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_fn(model, data_loader):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_mov_loss = 0\n",
        "    epoch_act_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "            src = batch.to(device)\n",
        "            trg = batch.to(device)\n",
        "            src = torch.movedim(src,0,1)\n",
        "            trg = torch.movedim(trg,0,1)\n",
        "            predictions_dlt, predictions_cls = model(src, trg, 0) # turn off teacher forcing\n",
        "\n",
        "            encoding_size = src.shape[2]\n",
        "            delta_size    = predictions_dlt.shape[2]\n",
        "            class_size    = predictions_cls.shape[2]\n",
        "\n",
        "            # Compute loss\n",
        "            # mask   = [(trg_length-1), batch_size]\n",
        "            # counts = [1]\n",
        "            mask = ~trg[1:,:,-1].eq(PAD_MARKER)       #[(trg_length-1), batch_size]\n",
        "            counts = torch.sum(mask)                  \n",
        "            ## For one-hot\n",
        "            # action_pred = [(trg length - 1) * batch_size, action_size]\n",
        "            action_pred = predictions_cls[1:].view(-1, class_size)\n",
        "            # action_trg = [(trg length - 1) * batch_size]\n",
        "            action_trg  = trg[1:,:,delta_size:].reshape(-1, class_size) # [(trg length - 1) * batch_size, action_size]\n",
        "            action_trg = action_trg.argmax(1)                           # [(trg length - 1) * batch_size]\n",
        "            # action_mask \n",
        "            action_mask = mask.reshape(-1) # [(trg length - 1) * batch_size]\n",
        "            #            Index  if mask else PAD_MARKER\n",
        "            action_trg = action_trg*action_mask + PAD_MARKER*(~action_mask) # [(trg length - 1) * batch_size]\n",
        "            # ACTION LOSS\n",
        "            action_loss = cross_entropy_fn(action_pred, action_trg)\n",
        "            action_loss = ACT_WEIGHT*action_loss\n",
        "            ## For regression\n",
        "            # movement_prd = [(trg_length-1), batch_size, movement_size]\n",
        "            movement_pred = predictions_dlt[1:]\n",
        "            # movement_trg\n",
        "            movement_trg  = trg[1:,:,:delta_size]\n",
        "            # mv_mask   = [(trg_length-1), batch_size, movement_size]\n",
        "            mv_mask = mask.expand((delta_size,-1,-1))    #[movement_size, (trg_length-1), batch_size]\n",
        "            mv_mask = torch.movedim(mv_mask,0,2)            #[(trg_length-1), batch_size, movement_size]\n",
        "            # Apply mask\n",
        "            # https://discuss.pytorch.org/t/how-to-correctly-weight-mse-loss-for-padded-sequences/176211\n",
        "            movement_pred_masked = movement_pred * mv_mask\n",
        "            movement_trg_masked  = movement_trg  * mv_mask\n",
        "            # Get MSE\n",
        "            movement_loss = mse_fn(movement_pred_masked, movement_trg_masked)\n",
        "            movement_loss = torch.sum(movement_loss)/(counts*DELTA_SIZE)\n",
        "            movement_loss = MOV_WEIGHT*movement_loss\n",
        "            # Total loss\n",
        "            loss = movement_loss + action_loss\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_mov_loss += movement_loss.item()\n",
        "            epoch_act_loss += action_loss.item()\n",
        "    return epoch_loss / len(data_loader), epoch_mov_loss/ len(data_loader), epoch_act_loss/ len(data_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFijN-Gik-GV",
        "outputId": "f2d92c3f-911b-4145-d2a3-9a8f446ee6a1"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer):\n",
        "  best_valid_loss = float(\"inf\")\n",
        "  best_train_loss = float(\"inf\")\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: [{epoch}/{num_epochs}]\")\n",
        "    \n",
        "    train_loss, train_mov_loss, train_act_loss = train_fn(\n",
        "      model,\n",
        "      train_dataloader,\n",
        "      optimizer,\n",
        "    )\n",
        "    print(f\"Train loss: {train_loss} | Mov Loss {train_mov_loss} | Act Loss {train_act_loss}\")\n",
        "  \n",
        "    valid_loss, valid_mov_loss, valid_act_loss = evaluate_fn(\n",
        "      model,\n",
        "      valid_dataloader,\n",
        "    )\n",
        "    print(f\"Valid loss: {valid_loss} | Mov Loss {valid_mov_loss} | Act Loss {valid_act_loss}\")    \n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      torch.save(model.state_dict(), model_bv_file)\n",
        "\n",
        "    if train_loss < best_train_loss:\n",
        "      best_train_loss = train_loss\n",
        "      torch.save(model.state_dict(), model_bt_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tune Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL:128he-256hd\n",
            "The model has 737,669 trainable parameters\n",
            "Epoch: [0/100]\n",
            "Train loss: 2731.254375 | Mov Loss 2727.766689453125 | Act Loss 3.487679371833801\n",
            "Valid loss: 2715.432861328125 | Mov Loss 2712.189306640625 | Act Loss 3.243561315536499\n",
            "Epoch: [1/100]\n",
            "Train loss: 2469.3333642578127 | Mov Loss 2465.969073486328 | Act Loss 3.36429438829422\n",
            "Valid loss: 2713.087841796875 | Mov Loss 2709.82685546875 | Act Loss 3.2609763622283934\n",
            "Epoch: [2/100]\n",
            "Train loss: 2380.7038891601565 | Mov Loss 2377.242725830078 | Act Loss 3.4611584997177123\n",
            "Valid loss: 2582.95458984375 | Mov Loss 2579.288720703125 | Act Loss 3.665862703323364\n",
            "Epoch: [3/100]\n",
            "Train loss: 2336.575772705078 | Mov Loss 2332.881470947266 | Act Loss 3.69430193901062\n",
            "Valid loss: 2495.7883056640626 | Mov Loss 2491.935791015625 | Act Loss 3.852549123764038\n",
            "Epoch: [4/100]\n",
            "Train loss: 2287.8614086914063 | Mov Loss 2284.120740966797 | Act Loss 3.7406687426567076\n",
            "Valid loss: 2473.73369140625 | Mov Loss 2469.99013671875 | Act Loss 3.743528366088867\n",
            "Epoch: [5/100]\n",
            "Train loss: 2258.5369287109374 | Mov Loss 2254.844942626953 | Act Loss 3.691976091861725\n",
            "Valid loss: 2460.17490234375 | Mov Loss 2456.565185546875 | Act Loss 3.609683322906494\n",
            "Epoch: [6/100]\n",
            "Train loss: 2235.3788244628904 | Mov Loss 2231.7566528320312 | Act Loss 3.6221733713150024\n",
            "Valid loss: 2438.2988037109376 | Mov Loss 2434.75478515625 | Act Loss 3.5440289974212646\n",
            "Epoch: [7/100]\n",
            "Train loss: 2216.1804577636717 | Mov Loss 2212.6034045410156 | Act Loss 3.577053346633911\n",
            "Valid loss: 2416.9433349609376 | Mov Loss 2413.64296875 | Act Loss 3.300379991531372\n",
            "Epoch: [8/100]\n",
            "Train loss: 2188.935302734375 | Mov Loss 2185.4347314453125 | Act Loss 3.500574791431427\n",
            "Valid loss: 2393.871240234375 | Mov Loss 2390.55302734375 | Act Loss 3.318182277679443\n",
            "Epoch: [9/100]\n",
            "Train loss: 2177.100173339844 | Mov Loss 2173.5899560546877 | Act Loss 3.5102166628837583\n",
            "Valid loss: 2405.221630859375 | Mov Loss 2401.93310546875 | Act Loss 3.288539266586304\n",
            "Epoch: [10/100]\n",
            "Train loss: 2153.029083251953 | Mov Loss 2149.5769567871093 | Act Loss 3.452126731872559\n",
            "Valid loss: 2375.0080078125 | Mov Loss 2371.7580078125 | Act Loss 3.2500218391418456\n",
            "Epoch: [11/100]\n",
            "Train loss: 2138.2504748535157 | Mov Loss 2134.7993090820314 | Act Loss 3.4511561298370363\n",
            "Valid loss: 2380.2108642578123 | Mov Loss 2376.937841796875 | Act Loss 3.2730000495910643\n",
            "Epoch: [12/100]\n",
            "Train loss: 2117.0101806640623 | Mov Loss 2113.5865185546877 | Act Loss 3.423664267063141\n",
            "Valid loss: 2357.2142822265623 | Mov Loss 2353.956396484375 | Act Loss 3.257883310317993\n",
            "Epoch: [13/100]\n",
            "Train loss: 2107.5599768066404 | Mov Loss 2104.1663720703127 | Act Loss 3.3935988640785215\n",
            "Valid loss: 2320.850244140625 | Mov Loss 2317.61884765625 | Act Loss 3.2314193725585936\n",
            "Epoch: [14/100]\n",
            "Train loss: 2092.9722534179687 | Mov Loss 2089.57666015625 | Act Loss 3.3955873107910155\n",
            "Valid loss: 2339.6513427734376 | Mov Loss 2336.368798828125 | Act Loss 3.2825165748596192\n",
            "Epoch: [15/100]\n",
            "Train loss: 2081.9646484375 | Mov Loss 2078.5941137695313 | Act Loss 3.370532772541046\n",
            "Valid loss: 2315.131005859375 | Mov Loss 2311.8701904296877 | Act Loss 3.2608482360839846\n",
            "Epoch: [16/100]\n",
            "Train loss: 2065.2375134277345 | Mov Loss 2061.8822692871095 | Act Loss 3.355249352455139\n",
            "Valid loss: 2304.43046875 | Mov Loss 2301.1760009765626 | Act Loss 3.254447889328003\n",
            "Epoch: [17/100]\n",
            "Train loss: 2053.2365368652345 | Mov Loss 2049.893116455078 | Act Loss 3.3434140491485596\n",
            "Valid loss: 2307.7602783203124 | Mov Loss 2304.2728515625 | Act Loss 3.4874297618865966\n",
            "Epoch: [18/100]\n",
            "Train loss: 2036.61134765625 | Mov Loss 2033.2650366210937 | Act Loss 3.346314535140991\n",
            "Valid loss: 2334.2943359375 | Mov Loss 2330.977001953125 | Act Loss 3.3173497200012205\n",
            "Epoch: [19/100]\n",
            "Train loss: 2027.4484106445314 | Mov Loss 2024.1220532226562 | Act Loss 3.3263511776924135\n",
            "Valid loss: 2289.1561767578123 | Mov Loss 2285.6507568359375 | Act Loss 3.5054173946380613\n",
            "Epoch: [20/100]\n",
            "Train loss: 2013.0284118652344 | Mov Loss 2009.7129919433594 | Act Loss 3.3154204535484313\n",
            "Valid loss: 2310.225927734375 | Mov Loss 2306.9738525390626 | Act Loss 3.252037191390991\n",
            "Epoch: [21/100]\n",
            "Train loss: 2006.6688854980468 | Mov Loss 2003.3755603027344 | Act Loss 3.29333021402359\n",
            "Valid loss: 2282.34580078125 | Mov Loss 2279.146484375 | Act Loss 3.1993886470794677\n",
            "Epoch: [22/100]\n",
            "Train loss: 1990.2379138183594 | Mov Loss 1986.963292236328 | Act Loss 3.2746259927749635\n",
            "Valid loss: 2291.8298095703126 | Mov Loss 2288.5243408203123 | Act Loss 3.3054938316345215\n",
            "Epoch: [23/100]\n",
            "Train loss: 1976.7833740234375 | Mov Loss 1973.4891650390625 | Act Loss 3.294213345050812\n",
            "Valid loss: 2300.327978515625 | Mov Loss 2296.955419921875 | Act Loss 3.372538995742798\n",
            "Epoch: [24/100]\n",
            "Train loss: 1970.8632214355468 | Mov Loss 1967.578690185547 | Act Loss 3.284525489807129\n",
            "Valid loss: 2246.9747802734373 | Mov Loss 2243.757080078125 | Act Loss 3.21768741607666\n",
            "Epoch: [25/100]\n",
            "Train loss: 1952.8584875488282 | Mov Loss 1949.5957080078124 | Act Loss 3.262773597240448\n",
            "Valid loss: 2276.1455810546877 | Mov Loss 2272.8436767578123 | Act Loss 3.3019307136535643\n",
            "Epoch: [26/100]\n",
            "Train loss: 1949.405791015625 | Mov Loss 1946.139736328125 | Act Loss 3.266052885055542\n",
            "Valid loss: 2265.4634765625 | Mov Loss 2262.051513671875 | Act Loss 3.4119364738464357\n",
            "Epoch: [27/100]\n",
            "Train loss: 1939.9139562988282 | Mov Loss 1936.6469860839843 | Act Loss 3.266973195075989\n",
            "Valid loss: 2243.317041015625 | Mov Loss 2240.095751953125 | Act Loss 3.2212047576904297\n",
            "Epoch: [28/100]\n",
            "Train loss: 1926.7091723632811 | Mov Loss 1923.4552197265625 | Act Loss 3.253950746059418\n",
            "Valid loss: 2239.88603515625 | Mov Loss 2236.5882568359375 | Act Loss 3.2977623462677004\n",
            "Epoch: [29/100]\n",
            "Train loss: 1910.9558898925782 | Mov Loss 1907.6964038085937 | Act Loss 3.2594816660881043\n",
            "Valid loss: 2266.209716796875 | Mov Loss 2263.043115234375 | Act Loss 3.1665459156036375\n",
            "Epoch: [30/100]\n",
            "Train loss: 1901.657646484375 | Mov Loss 1898.4169177246094 | Act Loss 3.240724422931671\n",
            "Valid loss: 2271.221923828125 | Mov Loss 2267.958544921875 | Act Loss 3.2633083343505858\n",
            "Epoch: [31/100]\n",
            "Train loss: 1890.319578857422 | Mov Loss 1887.070838623047 | Act Loss 3.2487339639663695\n",
            "Valid loss: 2269.409423828125 | Mov Loss 2266.147119140625 | Act Loss 3.262274217605591\n",
            "Epoch: [32/100]\n",
            "Train loss: 1882.284395751953 | Mov Loss 1879.0610607910157 | Act Loss 3.223342304229736\n",
            "Valid loss: 2264.74970703125 | Mov Loss 2261.542626953125 | Act Loss 3.2070664405822753\n",
            "Epoch: [33/100]\n",
            "Train loss: 1875.4749291992186 | Mov Loss 1872.2590368652343 | Act Loss 3.215889050960541\n",
            "Valid loss: 2257.318701171875 | Mov Loss 2254.22060546875 | Act Loss 3.098122072219849\n",
            "Epoch: [34/100]\n",
            "Train loss: 1870.0582299804687 | Mov Loss 1866.8501196289062 | Act Loss 3.20810822725296\n",
            "Valid loss: 2241.278271484375 | Mov Loss 2238.132958984375 | Act Loss 3.1452708721160887\n",
            "Epoch: [35/100]\n",
            "Train loss: 1874.9586364746094 | Mov Loss 1871.7368713378905 | Act Loss 3.221764905452728\n",
            "Valid loss: 2248.6204345703127 | Mov Loss 2245.3589111328124 | Act Loss 3.261490821838379\n",
            "Epoch: [36/100]\n",
            "Train loss: 1859.12966796875 | Mov Loss 1855.9139721679687 | Act Loss 3.2156983852386474\n",
            "Valid loss: 2281.0836669921873 | Mov Loss 2277.9654296875 | Act Loss 3.11823410987854\n",
            "Epoch: [37/100]\n",
            "Train loss: 1846.9648046875 | Mov Loss 1843.7658715820312 | Act Loss 3.1989296770095823\n",
            "Valid loss: 2261.854638671875 | Mov Loss 2258.661669921875 | Act Loss 3.1929578304290773\n",
            "Epoch: [38/100]\n",
            "Train loss: 1835.4874523925782 | Mov Loss 1832.2873364257812 | Act Loss 3.2001209330558775\n",
            "Valid loss: 2271.4237060546875 | Mov Loss 2268.214599609375 | Act Loss 3.2090654373168945\n",
            "Epoch: [39/100]\n",
            "Train loss: 1832.65064453125 | Mov Loss 1829.4269189453125 | Act Loss 3.223726613521576\n",
            "Valid loss: 2210.1653076171874 | Mov Loss 2207.0199462890623 | Act Loss 3.1453408718109133\n",
            "Epoch: [40/100]\n",
            "Train loss: 1810.2583154296874 | Mov Loss 1807.06291015625 | Act Loss 3.1954064917564393\n",
            "Valid loss: 2217.78662109375 | Mov Loss 2214.5788330078126 | Act Loss 3.2077776432037353\n",
            "Epoch: [41/100]\n",
            "Train loss: 1820.7760620117188 | Mov Loss 1817.572158203125 | Act Loss 3.203900947570801\n",
            "Valid loss: 2225.399560546875 | Mov Loss 2222.233642578125 | Act Loss 3.165869379043579\n",
            "Epoch: [42/100]\n",
            "Train loss: 1806.7988806152343 | Mov Loss 1803.6011584472656 | Act Loss 3.197720539569855\n",
            "Valid loss: 2200.72783203125 | Mov Loss 2197.618017578125 | Act Loss 3.109837532043457\n",
            "Epoch: [43/100]\n",
            "Train loss: 1802.9742663574218 | Mov Loss 1799.754052734375 | Act Loss 3.2202220034599303\n",
            "Valid loss: 2249.354150390625 | Mov Loss 2246.225390625 | Act Loss 3.1287251472473145\n",
            "Epoch: [44/100]\n",
            "Train loss: 1781.9095202636718 | Mov Loss 1778.7066687011718 | Act Loss 3.2028522181510923\n",
            "Valid loss: 2187.4237548828123 | Mov Loss 2184.26591796875 | Act Loss 3.1578468799591066\n",
            "Epoch: [45/100]\n",
            "Train loss: 1773.849102783203 | Mov Loss 1770.6504748535156 | Act Loss 3.198630087375641\n",
            "Valid loss: 2210.948583984375 | Mov Loss 2207.7667236328125 | Act Loss 3.1818849086761474\n",
            "Epoch: [46/100]\n",
            "Train loss: 1773.2820227050781 | Mov Loss 1770.1026440429687 | Act Loss 3.1793789792060854\n",
            "Valid loss: 2211.2399658203126 | Mov Loss 2208.1310302734373 | Act Loss 3.1089171409606933\n",
            "Epoch: [47/100]\n",
            "Train loss: 1764.5476867675782 | Mov Loss 1761.3266931152343 | Act Loss 3.2209952211380006\n",
            "Valid loss: 2207.3591552734374 | Mov Loss 2204.235888671875 | Act Loss 3.1232845783233643\n",
            "Epoch: [48/100]\n",
            "Train loss: 1764.766328125 | Mov Loss 1761.5639135742188 | Act Loss 3.2024078798294067\n",
            "Valid loss: 2249.9325439453123 | Mov Loss 2246.7782470703123 | Act Loss 3.154307508468628\n",
            "Epoch: [49/100]\n",
            "Train loss: 1758.5039575195312 | Mov Loss 1755.3197937011719 | Act Loss 3.184166417121887\n",
            "Valid loss: 2183.291259765625 | Mov Loss 2180.1376708984376 | Act Loss 3.1535889148712157\n",
            "Epoch: [50/100]\n",
            "Train loss: 1740.4179296875 | Mov Loss 1737.2344836425782 | Act Loss 3.1834453558921814\n",
            "Valid loss: 2179.4152099609373 | Mov Loss 2176.2254638671875 | Act Loss 3.189732789993286\n",
            "Epoch: [51/100]\n",
            "Train loss: 1733.902041015625 | Mov Loss 1730.7268469238281 | Act Loss 3.1751968383789064\n",
            "Valid loss: 2220.179736328125 | Mov Loss 2216.9096923828124 | Act Loss 3.2700102806091307\n",
            "Epoch: [52/100]\n",
            "Train loss: 1731.914727783203 | Mov Loss 1728.7118054199218 | Act Loss 3.202923858165741\n",
            "Valid loss: 2239.00029296875 | Mov Loss 2235.83017578125 | Act Loss 3.170103406906128\n",
            "Epoch: [53/100]\n",
            "Train loss: 1731.6830358886718 | Mov Loss 1728.4860437011719 | Act Loss 3.196994740962982\n",
            "Valid loss: 2189.2784423828125 | Mov Loss 2186.1095703125 | Act Loss 3.1688864707946776\n",
            "Epoch: [54/100]\n",
            "Train loss: 1721.5647399902343 | Mov Loss 1718.38005859375 | Act Loss 3.1846843457221983\n",
            "Valid loss: 2220.466064453125 | Mov Loss 2217.185693359375 | Act Loss 3.28037691116333\n",
            "Epoch: [55/100]\n",
            "Train loss: 1704.6431799316406 | Mov Loss 1701.433416748047 | Act Loss 3.2097663283348083\n",
            "Valid loss: 2226.65341796875 | Mov Loss 2223.4999755859376 | Act Loss 3.153458595275879\n",
            "Epoch: [56/100]\n",
            "Train loss: 1708.0989392089843 | Mov Loss 1704.9045263671876 | Act Loss 3.1944217181205747\n",
            "Valid loss: 2222.214794921875 | Mov Loss 2218.974365234375 | Act Loss 3.240465259552002\n",
            "Epoch: [57/100]\n",
            "Train loss: 1703.7439965820313 | Mov Loss 1700.541923828125 | Act Loss 3.2020786237716674\n",
            "Valid loss: 2255.304248046875 | Mov Loss 2252.1031494140625 | Act Loss 3.201072025299072\n",
            "Epoch: [58/100]\n",
            "Train loss: 1695.1695556640625 | Mov Loss 1691.9941650390624 | Act Loss 3.175392382144928\n",
            "Valid loss: 2255.67431640625 | Mov Loss 2252.4635498046873 | Act Loss 3.210741949081421\n",
            "Epoch: [59/100]\n",
            "Train loss: 1680.21845703125 | Mov Loss 1677.0353399658204 | Act Loss 3.183111069202423\n",
            "Valid loss: 2191.8263427734373 | Mov Loss 2188.675146484375 | Act Loss 3.1511721134185793\n",
            "Epoch: [60/100]\n",
            "Train loss: 1680.27669921875 | Mov Loss 1677.0826293945313 | Act Loss 3.194064793586731\n",
            "Valid loss: 2209.348779296875 | Mov Loss 2206.1665283203124 | Act Loss 3.1822667598724363\n",
            "Epoch: [61/100]\n",
            "Train loss: 1677.7936083984375 | Mov Loss 1674.5753649902344 | Act Loss 3.218237752914429\n",
            "Valid loss: 2188.4552978515626 | Mov Loss 2185.21181640625 | Act Loss 3.2434744358062746\n",
            "Epoch: [62/100]\n",
            "Train loss: 1671.7961364746093 | Mov Loss 1668.5962622070313 | Act Loss 3.199873099327087\n",
            "Valid loss: 2191.4340576171876 | Mov Loss 2188.2400390625 | Act Loss 3.1940561294555665\n",
            "Epoch: [63/100]\n",
            "Train loss: 1665.3344165039061 | Mov Loss 1662.149775390625 | Act Loss 3.184640953540802\n",
            "Valid loss: 2204.869677734375 | Mov Loss 2201.628955078125 | Act Loss 3.240728521347046\n",
            "Epoch: [64/100]\n",
            "Train loss: 1656.1418115234376 | Mov Loss 1652.9332751464844 | Act Loss 3.2085391974449156\n",
            "Valid loss: 2188.135791015625 | Mov Loss 2184.945458984375 | Act Loss 3.1903437614440917\n",
            "Epoch: [65/100]\n",
            "Train loss: 1654.690391845703 | Mov Loss 1651.5077661132811 | Act Loss 3.182628457546234\n",
            "Valid loss: 2204.079345703125 | Mov Loss 2200.8175537109373 | Act Loss 3.2618178844451906\n",
            "Epoch: [66/100]\n",
            "Train loss: 1649.2372595214845 | Mov Loss 1646.0290454101562 | Act Loss 3.2082083654403686\n",
            "Valid loss: 2270.51748046875 | Mov Loss 2267.2668212890626 | Act Loss 3.250694417953491\n",
            "Epoch: [67/100]\n",
            "Train loss: 1635.2633312988282 | Mov Loss 1632.057296142578 | Act Loss 3.206037096977234\n",
            "Valid loss: 2192.37919921875 | Mov Loss 2189.2357421875 | Act Loss 3.1434578895568848\n",
            "Epoch: [68/100]\n",
            "Train loss: 1641.015662841797 | Mov Loss 1637.7943408203125 | Act Loss 3.2213206362724303\n",
            "Valid loss: 2257.22685546875 | Mov Loss 2254.03076171875 | Act Loss 3.196149158477783\n",
            "Epoch: [69/100]\n",
            "Train loss: 1641.6771752929687 | Mov Loss 1638.4645288085937 | Act Loss 3.2126395320892334\n",
            "Valid loss: 2184.4782958984374 | Mov Loss 2181.3004150390625 | Act Loss 3.177845764160156\n",
            "Epoch: [70/100]\n",
            "Train loss: 1628.5240905761718 | Mov Loss 1625.2937133789062 | Act Loss 3.2303787899017333\n",
            "Valid loss: 2235.575341796875 | Mov Loss 2232.3663330078125 | Act Loss 3.2090057849884035\n",
            "Epoch: [71/100]\n",
            "Train loss: 1627.4192211914062 | Mov Loss 1624.1711108398438 | Act Loss 3.2481052470207215\n",
            "Valid loss: 2210.5994873046875 | Mov Loss 2207.4223876953124 | Act Loss 3.1770756244659424\n",
            "Epoch: [72/100]\n",
            "Train loss: 1610.0768444824218 | Mov Loss 1606.8717993164062 | Act Loss 3.2050426983833313\n",
            "Valid loss: 2164.408349609375 | Mov Loss 2161.13916015625 | Act Loss 3.2691875457763673\n",
            "Epoch: [73/100]\n",
            "Train loss: 1605.8237243652343 | Mov Loss 1602.5663159179687 | Act Loss 3.257407262325287\n",
            "Valid loss: 2194.759375 | Mov Loss 2191.5305908203127 | Act Loss 3.2287646770477294\n",
            "Epoch: [74/100]\n",
            "Train loss: 1609.5160668945311 | Mov Loss 1606.3329772949219 | Act Loss 3.1830832743644715\n",
            "Valid loss: 2224.8931396484377 | Mov Loss 2221.6738525390624 | Act Loss 3.219248056411743\n",
            "Epoch: [75/100]\n",
            "Train loss: 1605.1943115234376 | Mov Loss 1601.9942883300782 | Act Loss 3.200020945072174\n",
            "Valid loss: 2190.0425048828124 | Mov Loss 2186.778173828125 | Act Loss 3.26434440612793\n",
            "Epoch: [76/100]\n",
            "Train loss: 1593.2056225585939 | Mov Loss 1590.002940673828 | Act Loss 3.202682259082794\n",
            "Valid loss: 2197.4995361328124 | Mov Loss 2194.2841064453123 | Act Loss 3.215492343902588\n",
            "Epoch: [77/100]\n",
            "Train loss: 1584.6514965820313 | Mov Loss 1581.422615966797 | Act Loss 3.2288798546791075\n",
            "Valid loss: 2225.74609375 | Mov Loss 2222.4804443359376 | Act Loss 3.265656614303589\n",
            "Epoch: [78/100]\n",
            "Train loss: 1592.777098388672 | Mov Loss 1589.5340905761718 | Act Loss 3.2430055379867553\n",
            "Valid loss: 2276.4481201171875 | Mov Loss 2273.0393310546874 | Act Loss 3.408811569213867\n",
            "Epoch: [79/100]\n",
            "Train loss: 1598.000869140625 | Mov Loss 1594.748319091797 | Act Loss 3.252558345794678\n",
            "Valid loss: 2154.69384765625 | Mov Loss 2151.429296875 | Act Loss 3.264577341079712\n",
            "Epoch: [80/100]\n",
            "Train loss: 1579.940760498047 | Mov Loss 1576.7172174072266 | Act Loss 3.223543043136597\n",
            "Valid loss: 2200.4100830078123 | Mov Loss 2197.0912109375 | Act Loss 3.318876123428345\n",
            "Epoch: [81/100]\n",
            "Train loss: 1572.0788500976562 | Mov Loss 1568.8170178222656 | Act Loss 3.2618304228782655\n",
            "Valid loss: 2189.1759765625 | Mov Loss 2185.96142578125 | Act Loss 3.2145100116729735\n",
            "Epoch: [82/100]\n",
            "Train loss: 1557.233409423828 | Mov Loss 1553.9963977050782 | Act Loss 3.237010142803192\n",
            "Valid loss: 2191.48115234375 | Mov Loss 2188.252197265625 | Act Loss 3.228946399688721\n",
            "Epoch: [83/100]\n",
            "Train loss: 1551.3757971191405 | Mov Loss 1548.1061755371093 | Act Loss 3.269620442390442\n",
            "Valid loss: 2174.5396484375 | Mov Loss 2171.2284912109376 | Act Loss 3.3111315250396727\n",
            "Epoch: [84/100]\n",
            "Train loss: 1551.164503173828 | Mov Loss 1547.9109918212891 | Act Loss 3.253506941795349\n",
            "Valid loss: 2188.4656494140627 | Mov Loss 2185.1859619140623 | Act Loss 3.279663419723511\n",
            "Epoch: [85/100]\n",
            "Train loss: 1548.3667895507813 | Mov Loss 1545.123818359375 | Act Loss 3.2429652404785156\n",
            "Valid loss: 2215.426611328125 | Mov Loss 2212.1829833984375 | Act Loss 3.243663120269775\n",
            "Epoch: [86/100]\n",
            "Train loss: 1549.9819030761719 | Mov Loss 1546.7259155273437 | Act Loss 3.2559875512123106\n",
            "Valid loss: 2177.7064208984375 | Mov Loss 2174.45771484375 | Act Loss 3.248734474182129\n",
            "Epoch: [87/100]\n",
            "Train loss: 1551.430831298828 | Mov Loss 1548.2050341796876 | Act Loss 3.22579630613327\n",
            "Valid loss: 2223.689990234375 | Mov Loss 2220.4165283203124 | Act Loss 3.273457670211792\n",
            "Epoch: [88/100]\n",
            "Train loss: 1534.9730432128906 | Mov Loss 1531.7233135986328 | Act Loss 3.2497278380393984\n",
            "Valid loss: 2182.9820068359377 | Mov Loss 2179.7019775390627 | Act Loss 3.280028820037842\n",
            "Epoch: [89/100]\n",
            "Train loss: 1517.3147729492186 | Mov Loss 1514.019345703125 | Act Loss 3.29542622089386\n",
            "Valid loss: 2196.0845703125 | Mov Loss 2192.7849853515627 | Act Loss 3.299636793136597\n",
            "Epoch: [90/100]\n",
            "Train loss: 1534.5461059570312 | Mov Loss 1531.2710913085937 | Act Loss 3.2750121116638184\n",
            "Valid loss: 2234.5765380859375 | Mov Loss 2231.268115234375 | Act Loss 3.308418893814087\n",
            "Epoch: [91/100]\n",
            "Train loss: 1534.857186279297 | Mov Loss 1531.600557861328 | Act Loss 3.256627154350281\n",
            "Valid loss: 2221.2773193359376 | Mov Loss 2217.9323974609374 | Act Loss 3.344941282272339\n",
            "Epoch: [92/100]\n",
            "Train loss: 1529.661658935547 | Mov Loss 1526.4190478515625 | Act Loss 3.242604303359985\n",
            "Valid loss: 2229.32685546875 | Mov Loss 2226.078271484375 | Act Loss 3.2486072063446043\n",
            "Epoch: [93/100]\n",
            "Train loss: 1531.7225634765625 | Mov Loss 1528.4662982177733 | Act Loss 3.256262834072113\n",
            "Valid loss: 2184.94619140625 | Mov Loss 2181.700537109375 | Act Loss 3.245680046081543\n",
            "Epoch: [94/100]\n",
            "Train loss: 1518.011248779297 | Mov Loss 1514.7790014648438 | Act Loss 3.232248160839081\n",
            "Valid loss: 2169.029833984375 | Mov Loss 2165.7565185546873 | Act Loss 3.273323965072632\n",
            "Epoch: [95/100]\n",
            "Train loss: 1510.7703637695313 | Mov Loss 1507.4943139648437 | Act Loss 3.2760505962371824\n",
            "Valid loss: 2184.18056640625 | Mov Loss 2180.826416015625 | Act Loss 3.3541441440582274\n",
            "Epoch: [96/100]\n",
            "Train loss: 1514.3074279785155 | Mov Loss 1511.0095623779298 | Act Loss 3.2978692483901977\n",
            "Valid loss: 2183.603271484375 | Mov Loss 2180.250537109375 | Act Loss 3.3527120113372804\n",
            "Epoch: [97/100]\n",
            "Train loss: 1500.980415649414 | Mov Loss 1497.7051501464844 | Act Loss 3.275269055366516\n",
            "Valid loss: 2182.4474853515626 | Mov Loss 2179.214794921875 | Act Loss 3.232679510116577\n",
            "Epoch: [98/100]\n",
            "Train loss: 1506.9189916992189 | Mov Loss 1503.6450079345702 | Act Loss 3.2739830231666565\n",
            "Valid loss: 2192.703466796875 | Mov Loss 2189.4357666015626 | Act Loss 3.267656755447388\n",
            "Epoch: [99/100]\n",
            "Train loss: 1508.7217260742189 | Mov Loss 1505.4521728515624 | Act Loss 3.2695551896095276\n",
            "Valid loss: 2195.7435302734375 | Mov Loss 2192.42529296875 | Act Loss 3.318253421783447\n",
            "MODEL:256he-512hd\n",
            "The model has 2,261,637 trainable parameters\n",
            "Epoch: [0/100]\n",
            "Train loss: 2621.659884033203 | Mov Loss 2618.1842846679688 | Act Loss 3.475604648590088\n",
            "Valid loss: 2711.501318359375 | Mov Loss 2708.23125 | Act Loss 3.270105075836182\n",
            "Epoch: [1/100]\n",
            "Train loss: 2361.6167956542968 | Mov Loss 2357.7005041503908 | Act Loss 3.9162932729721067\n",
            "Valid loss: 2585.24140625 | Mov Loss 2581.553173828125 | Act Loss 3.688182067871094\n",
            "Epoch: [2/100]\n",
            "Train loss: 2286.225089111328 | Mov Loss 2281.784451904297 | Act Loss 4.440640568733215\n",
            "Valid loss: 2539.083447265625 | Mov Loss 2534.78515625 | Act Loss 4.298294258117676\n",
            "Epoch: [3/100]\n",
            "Train loss: 2226.439981689453 | Mov Loss 2221.726828613281 | Act Loss 4.713149557113647\n",
            "Valid loss: 2486.479296875 | Mov Loss 2481.887158203125 | Act Loss 4.592175388336182\n",
            "Epoch: [4/100]\n",
            "Train loss: 2183.3262133789062 | Mov Loss 2178.6258569335937 | Act Loss 4.700358180999756\n",
            "Valid loss: 2437.146142578125 | Mov Loss 2432.597998046875 | Act Loss 4.548167419433594\n",
            "Epoch: [5/100]\n",
            "Train loss: 2151.721016845703 | Mov Loss 2147.0980822753904 | Act Loss 4.622937216758728\n",
            "Valid loss: 2396.3703857421874 | Mov Loss 2391.6185791015623 | Act Loss 4.751782703399658\n",
            "Epoch: [6/100]\n",
            "Train loss: 2126.988800048828 | Mov Loss 2122.424964599609 | Act Loss 4.563837513923645\n",
            "Valid loss: 2366.328662109375 | Mov Loss 2361.470556640625 | Act Loss 4.8580577850341795\n",
            "Epoch: [7/100]\n",
            "Train loss: 2102.1155700683594 | Mov Loss 2097.6493090820313 | Act Loss 4.466267530918121\n",
            "Valid loss: 2341.03818359375 | Mov Loss 2336.57421875 | Act Loss 4.463940906524658\n",
            "Epoch: [8/100]\n",
            "Train loss: 2073.7018994140626 | Mov Loss 2069.321647949219 | Act Loss 4.380249605178833\n",
            "Valid loss: 2343.276513671875 | Mov Loss 2338.6361083984375 | Act Loss 4.640401458740234\n",
            "Epoch: [9/100]\n",
            "Train loss: 2050.175546875 | Mov Loss 2045.7995971679688 | Act Loss 4.3759465670585636\n",
            "Valid loss: 2320.8074951171875 | Mov Loss 2316.182666015625 | Act Loss 4.624808883666992\n",
            "Epoch: [10/100]\n",
            "Train loss: 2019.5215344238281 | Mov Loss 2015.128876953125 | Act Loss 4.392655816078186\n",
            "Valid loss: 2320.581201171875 | Mov Loss 2316.268408203125 | Act Loss 4.312791538238526\n",
            "Epoch: [11/100]\n",
            "Train loss: 1998.099093017578 | Mov Loss 1993.8803271484376 | Act Loss 4.218765799999237\n",
            "Valid loss: 2331.9852294921875 | Mov Loss 2327.5592041015625 | Act Loss 4.426018238067627\n",
            "Epoch: [12/100]\n",
            "Train loss: 1997.1466967773438 | Mov Loss 1992.8892785644532 | Act Loss 4.257417085170746\n",
            "Valid loss: 2309.2179443359373 | Mov Loss 2304.6033203125 | Act Loss 4.614549350738526\n",
            "Epoch: [13/100]\n",
            "Train loss: 1974.7252355957032 | Mov Loss 1970.5176184082031 | Act Loss 4.207623353004456\n",
            "Valid loss: 2264.2302001953126 | Mov Loss 2259.802880859375 | Act Loss 4.42732572555542\n",
            "Epoch: [14/100]\n",
            "Train loss: 1957.6042407226562 | Mov Loss 1953.468741455078 | Act Loss 4.135500102043152\n",
            "Valid loss: 2265.3836181640627 | Mov Loss 2261.1245361328124 | Act Loss 4.25910062789917\n",
            "Epoch: [15/100]\n",
            "Train loss: 1934.5631787109376 | Mov Loss 1930.3639978027343 | Act Loss 4.199182498455047\n",
            "Valid loss: 2367.80810546875 | Mov Loss 2363.28095703125 | Act Loss 4.5272214889526365\n",
            "Epoch: [16/100]\n",
            "Train loss: 1932.5532263183593 | Mov Loss 1928.399442138672 | Act Loss 4.1537897229194645\n",
            "Valid loss: 2271.1871826171873 | Mov Loss 2266.7026611328124 | Act Loss 4.484507083892822\n",
            "Epoch: [17/100]\n",
            "Train loss: 1907.893153076172 | Mov Loss 1903.7794995117188 | Act Loss 4.113657112121582\n",
            "Valid loss: 2255.947265625 | Mov Loss 2251.5861328125 | Act Loss 4.361135387420655\n",
            "Epoch: [18/100]\n",
            "Train loss: 1897.8114904785157 | Mov Loss 1893.7305529785156 | Act Loss 4.080941896438599\n",
            "Valid loss: 2270.857666015625 | Mov Loss 2266.644140625 | Act Loss 4.213554954528808\n",
            "Epoch: [19/100]\n",
            "Train loss: 1880.20228515625 | Mov Loss 1876.1921545410157 | Act Loss 4.01012925863266\n",
            "Valid loss: 2259.5556396484376 | Mov Loss 2255.3705322265623 | Act Loss 4.185111808776855\n",
            "Epoch: [20/100]\n",
            "Train loss: 1872.2260668945312 | Mov Loss 1868.2252392578125 | Act Loss 4.0008256721496585\n",
            "Valid loss: 2232.3326904296873 | Mov Loss 2228.04453125 | Act Loss 4.288144874572754\n",
            "Epoch: [21/100]\n",
            "Train loss: 1860.3884521484374 | Mov Loss 1856.3753039550782 | Act Loss 4.013146629333496\n",
            "Valid loss: 2219.484423828125 | Mov Loss 2215.42841796875 | Act Loss 4.056090545654297\n",
            "Epoch: [22/100]\n",
            "Train loss: 1841.1070532226563 | Mov Loss 1837.1115673828126 | Act Loss 3.9954919409751892\n",
            "Valid loss: 2217.467626953125 | Mov Loss 2213.367041015625 | Act Loss 4.100591373443604\n",
            "Epoch: [23/100]\n",
            "Train loss: 1827.0430480957032 | Mov Loss 1823.0771142578126 | Act Loss 3.965925924777985\n",
            "Valid loss: 2207.054150390625 | Mov Loss 2202.7749755859377 | Act Loss 4.279162454605102\n",
            "Epoch: [24/100]\n",
            "Train loss: 1818.4319653320313 | Mov Loss 1814.4830017089844 | Act Loss 3.948964419364929\n",
            "Valid loss: 2275.154931640625 | Mov Loss 2270.9650634765626 | Act Loss 4.189885568618775\n",
            "Epoch: [25/100]\n",
            "Train loss: 1795.4472595214843 | Mov Loss 1791.5293798828125 | Act Loss 3.917879068851471\n",
            "Valid loss: 2233.0439208984376 | Mov Loss 2228.8856689453123 | Act Loss 4.158275890350342\n",
            "Epoch: [26/100]\n",
            "Train loss: 1794.9151843261718 | Mov Loss 1791.0057495117187 | Act Loss 3.9094462752342225\n",
            "Valid loss: 2211.5285400390626 | Mov Loss 2207.4450439453126 | Act Loss 4.083480453491211\n",
            "Epoch: [27/100]\n",
            "Train loss: 1782.7130285644532 | Mov Loss 1778.7906884765625 | Act Loss 3.9223422765731812\n",
            "Valid loss: 2208.998193359375 | Mov Loss 2204.8555419921877 | Act Loss 4.142637157440186\n",
            "Epoch: [28/100]\n",
            "Train loss: 1780.5904565429687 | Mov Loss 1776.716336669922 | Act Loss 3.8741269063949586\n",
            "Valid loss: 2282.0523681640625 | Mov Loss 2277.8666015625 | Act Loss 4.185753202438354\n",
            "Epoch: [29/100]\n",
            "Train loss: 1764.1849255371094 | Mov Loss 1760.3239123535157 | Act Loss 3.861012692451477\n",
            "Valid loss: 2269.870849609375 | Mov Loss 2265.794775390625 | Act Loss 4.076070880889892\n",
            "Epoch: [30/100]\n",
            "Train loss: 1747.379649658203 | Mov Loss 1743.5570874023438 | Act Loss 3.822562081813812\n",
            "Valid loss: 2217.1734375 | Mov Loss 2212.9986328125 | Act Loss 4.174795246124267\n",
            "Epoch: [31/100]\n",
            "Train loss: 1748.5163208007812 | Mov Loss 1744.6487451171874 | Act Loss 3.867573802471161\n",
            "Valid loss: 2220.736328125 | Mov Loss 2216.3987548828127 | Act Loss 4.337581729888916\n",
            "Epoch: [32/100]\n",
            "Train loss: 1728.9020703125 | Mov Loss 1725.065830078125 | Act Loss 3.83623925447464\n",
            "Valid loss: 2271.1081787109374 | Mov Loss 2267.0752685546877 | Act Loss 4.032886600494384\n",
            "Epoch: [33/100]\n",
            "Train loss: 1733.4832543945313 | Mov Loss 1729.6451647949218 | Act Loss 3.8380809593200684\n",
            "Valid loss: 2234.14658203125 | Mov Loss 2230.0079833984373 | Act Loss 4.1385993480682375\n",
            "Epoch: [34/100]\n",
            "Train loss: 1726.018271484375 | Mov Loss 1722.136427001953 | Act Loss 3.881847155094147\n",
            "Valid loss: 2271.4993896484375 | Mov Loss 2267.4095703125 | Act Loss 4.089817714691162\n",
            "Epoch: [35/100]\n",
            "Train loss: 1700.9655249023438 | Mov Loss 1697.1091711425781 | Act Loss 3.8563541841506956\n",
            "Valid loss: 2240.6075439453125 | Mov Loss 2236.5419921875 | Act Loss 4.065547275543213\n",
            "Epoch: [36/100]\n",
            "Train loss: 1698.0361083984376 | Mov Loss 1694.2663781738281 | Act Loss 3.769728908538818\n",
            "Valid loss: 2216.184033203125 | Mov Loss 2212.1254638671876 | Act Loss 4.058630084991455\n",
            "Epoch: [37/100]\n",
            "Train loss: 1682.6293395996095 | Mov Loss 1678.727822265625 | Act Loss 3.9015174198150633\n",
            "Valid loss: 2241.1482421875 | Mov Loss 2236.98564453125 | Act Loss 4.162613296508789\n",
            "Epoch: [38/100]\n",
            "Train loss: 1675.4331079101562 | Mov Loss 1671.6456018066406 | Act Loss 3.787509458065033\n",
            "Valid loss: 2280.2229248046874 | Mov Loss 2275.9275634765627 | Act Loss 4.29536542892456\n",
            "Epoch: [39/100]\n",
            "Train loss: 1679.0080773925781 | Mov Loss 1675.174862060547 | Act Loss 3.8332164597511293\n",
            "Valid loss: 2234.4421875 | Mov Loss 2230.3082275390625 | Act Loss 4.133937120437622\n",
            "Epoch: [40/100]\n",
            "Train loss: 1678.2925024414062 | Mov Loss 1674.4743859863281 | Act Loss 3.8181155848503114\n",
            "Valid loss: 2266.4377685546874 | Mov Loss 2262.430712890625 | Act Loss 4.007054853439331\n",
            "Epoch: [41/100]\n",
            "Train loss: 1657.4471215820313 | Mov Loss 1653.711925048828 | Act Loss 3.7351956081390383\n",
            "Valid loss: 2276.743896484375 | Mov Loss 2272.5908203125 | Act Loss 4.153097295761109\n",
            "Epoch: [42/100]\n",
            "Train loss: 1652.3834936523438 | Mov Loss 1648.658944091797 | Act Loss 3.7245496678352357\n",
            "Valid loss: 2219.7943359375 | Mov Loss 2215.5389892578123 | Act Loss 4.2553871154785154\n",
            "Epoch: [43/100]\n",
            "Train loss: 1641.6605114746094 | Mov Loss 1637.8787670898437 | Act Loss 3.7817476963996888\n",
            "Valid loss: 2252.651220703125 | Mov Loss 2248.3736572265625 | Act Loss 4.2775678634643555\n",
            "Epoch: [44/100]\n",
            "Train loss: 1633.4674279785156 | Mov Loss 1629.7171350097656 | Act Loss 3.7502916836738587\n",
            "Valid loss: 2323.719482421875 | Mov Loss 2319.6230224609376 | Act Loss 4.096425151824951\n",
            "Epoch: [45/100]\n",
            "Train loss: 1618.578038330078 | Mov Loss 1614.7818786621094 | Act Loss 3.796158618927002\n",
            "Valid loss: 2229.8613525390624 | Mov Loss 2225.6225341796876 | Act Loss 4.238836097717285\n",
            "Epoch: [46/100]\n",
            "Train loss: 1615.5286938476563 | Mov Loss 1611.7094287109376 | Act Loss 3.8192692399024963\n",
            "Valid loss: 2315.77021484375 | Mov Loss 2311.5742919921877 | Act Loss 4.1959068298339846\n",
            "Epoch: [47/100]\n",
            "Train loss: 1596.3129150390625 | Mov Loss 1592.5723449707032 | Act Loss 3.740572502613068\n",
            "Valid loss: 2179.3575439453125 | Mov Loss 2175.2189208984373 | Act Loss 4.138600254058838\n",
            "Epoch: [48/100]\n",
            "Train loss: 1594.0056848144532 | Mov Loss 1590.2894482421875 | Act Loss 3.7162359666824343\n",
            "Valid loss: 2233.73701171875 | Mov Loss 2229.7368408203124 | Act Loss 4.000178337097168\n",
            "Epoch: [49/100]\n",
            "Train loss: 1604.6830895996093 | Mov Loss 1600.8946911621094 | Act Loss 3.788397686481476\n",
            "Valid loss: 2355.748681640625 | Mov Loss 2351.464013671875 | Act Loss 4.284624004364014\n",
            "Epoch: [50/100]\n",
            "Train loss: 1583.5209509277345 | Mov Loss 1579.79794921875 | Act Loss 3.7229998469352723\n",
            "Valid loss: 2206.9448974609377 | Mov Loss 2202.8217041015623 | Act Loss 4.1231952667236325\n",
            "Epoch: [51/100]\n",
            "Train loss: 1579.8295568847657 | Mov Loss 1576.0753283691406 | Act Loss 3.7542281699180604\n",
            "Valid loss: 2258.503759765625 | Mov Loss 2254.305078125 | Act Loss 4.19868745803833\n",
            "Epoch: [52/100]\n",
            "Train loss: 1563.698212890625 | Mov Loss 1559.920595703125 | Act Loss 3.777617356777191\n",
            "Valid loss: 2276.654443359375 | Mov Loss 2272.6150634765627 | Act Loss 4.03938627243042\n",
            "Epoch: [53/100]\n",
            "Train loss: 1565.3917150878906 | Mov Loss 1561.714364013672 | Act Loss 3.6773511171340942\n",
            "Valid loss: 2256.3671142578123 | Mov Loss 2252.2829345703126 | Act Loss 4.08421630859375\n",
            "Epoch: [54/100]\n",
            "Train loss: 1566.0841882324219 | Mov Loss 1562.3648510742187 | Act Loss 3.719334087371826\n",
            "Valid loss: 2278.767236328125 | Mov Loss 2274.80703125 | Act Loss 3.9601916790008547\n",
            "Epoch: [55/100]\n",
            "Train loss: 1547.9139233398437 | Mov Loss 1544.1844018554686 | Act Loss 3.729515860080719\n",
            "Valid loss: 2271.54296875 | Mov Loss 2267.42255859375 | Act Loss 4.120453643798828\n",
            "Epoch: [56/100]\n",
            "Train loss: 1527.1020080566407 | Mov Loss 1523.3826721191406 | Act Loss 3.719341428279877\n",
            "Valid loss: 2434.682861328125 | Mov Loss 2430.396435546875 | Act Loss 4.286417865753174\n",
            "Epoch: [57/100]\n",
            "Train loss: 1530.512822265625 | Mov Loss 1526.8232305908202 | Act Loss 3.689592669010162\n",
            "Valid loss: 2189.4247314453123 | Mov Loss 2185.548486328125 | Act Loss 3.876241445541382\n",
            "Epoch: [58/100]\n",
            "Train loss: 1543.9345446777343 | Mov Loss 1540.3101745605468 | Act Loss 3.6243690991401674\n",
            "Valid loss: 2226.2685546875 | Mov Loss 2222.366357421875 | Act Loss 3.902201271057129\n",
            "Epoch: [59/100]\n",
            "Train loss: 1538.9649621582032 | Mov Loss 1535.3232739257812 | Act Loss 3.6416871881484987\n",
            "Valid loss: 2288.850146484375 | Mov Loss 2284.929150390625 | Act Loss 3.921018648147583\n",
            "Epoch: [60/100]\n",
            "Train loss: 1547.5201977539064 | Mov Loss 1543.8607373046875 | Act Loss 3.6594673466682432\n",
            "Valid loss: 2213.310498046875 | Mov Loss 2209.291943359375 | Act Loss 4.0184972286224365\n",
            "Epoch: [61/100]\n",
            "Train loss: 1516.1386193847657 | Mov Loss 1512.44369140625 | Act Loss 3.6949247193336485\n",
            "Valid loss: 2232.5034423828124 | Mov Loss 2228.412646484375 | Act Loss 4.090781784057617\n",
            "Epoch: [62/100]\n",
            "Train loss: 1507.3467358398439 | Mov Loss 1503.7068957519532 | Act Loss 3.6398370051383973\n",
            "Valid loss: 2167.5531494140623 | Mov Loss 2163.422021484375 | Act Loss 4.131135272979736\n",
            "Epoch: [63/100]\n",
            "Train loss: 1505.8345776367187 | Mov Loss 1502.1946130371093 | Act Loss 3.6399631094932556\n",
            "Valid loss: 2237.4917236328124 | Mov Loss 2233.50517578125 | Act Loss 3.986583042144775\n",
            "Epoch: [64/100]\n",
            "Train loss: 1507.3170544433594 | Mov Loss 1503.6943859863281 | Act Loss 3.622663505077362\n",
            "Valid loss: 2218.223095703125 | Mov Loss 2214.2260986328124 | Act Loss 3.9969836711883544\n",
            "Epoch: [65/100]\n",
            "Train loss: 1496.7405895996094 | Mov Loss 1493.1094201660155 | Act Loss 3.6311639595031737\n",
            "Valid loss: 2191.7767333984375 | Mov Loss 2187.7635986328123 | Act Loss 4.013170433044434\n",
            "Epoch: [66/100]\n",
            "Train loss: 1485.6997094726562 | Mov Loss 1482.0806030273438 | Act Loss 3.619113426208496\n",
            "Valid loss: 2204.4650390625 | Mov Loss 2200.543408203125 | Act Loss 3.921569585800171\n",
            "Epoch: [67/100]\n",
            "Train loss: 1473.8692309570313 | Mov Loss 1470.2533306884766 | Act Loss 3.6159006762504577\n",
            "Valid loss: 2193.3466796875 | Mov Loss 2189.4829833984377 | Act Loss 3.8637012481689452\n",
            "Epoch: [68/100]\n",
            "Train loss: 1465.8031365966797 | Mov Loss 1462.1420880126952 | Act Loss 3.661050660610199\n",
            "Valid loss: 2202.3578125 | Mov Loss 2198.387451171875 | Act Loss 3.9703879356384277\n",
            "Epoch: [69/100]\n",
            "Train loss: 1480.1450006103516 | Mov Loss 1476.536860961914 | Act Loss 3.608142898082733\n",
            "Valid loss: 2217.3745849609377 | Mov Loss 2213.514111328125 | Act Loss 3.8604476928710936\n",
            "Epoch: [70/100]\n",
            "Train loss: 1479.2480926513672 | Mov Loss 1475.6395056152344 | Act Loss 3.608584027290344\n",
            "Valid loss: 2256.912158203125 | Mov Loss 2253.058984375 | Act Loss 3.8531774520874023\n",
            "Epoch: [71/100]\n",
            "Train loss: 1465.1188153076173 | Mov Loss 1461.391405029297 | Act Loss 3.727410500049591\n",
            "Valid loss: 2233.6619140625 | Mov Loss 2229.73408203125 | Act Loss 3.9277945518493653\n",
            "Epoch: [72/100]\n",
            "Train loss: 1467.124071044922 | Mov Loss 1463.4256463623046 | Act Loss 3.6984198617935182\n",
            "Valid loss: 2295.428759765625 | Mov Loss 2291.3546875 | Act Loss 4.074097299575806\n",
            "Epoch: [73/100]\n",
            "Train loss: 1455.0300408935548 | Mov Loss 1451.2073937988282 | Act Loss 3.822647273540497\n",
            "Valid loss: 2278.3031005859375 | Mov Loss 2273.9913330078125 | Act Loss 4.311788558959961\n",
            "Epoch: [74/100]\n",
            "Train loss: 1473.1276599121093 | Mov Loss 1469.3737426757812 | Act Loss 3.753914408683777\n",
            "Valid loss: 2279.37587890625 | Mov Loss 2275.276220703125 | Act Loss 4.099684858322144\n",
            "Epoch: [75/100]\n",
            "Train loss: 1450.1454455566407 | Mov Loss 1446.442474975586 | Act Loss 3.7029762148857115\n",
            "Valid loss: 2258.710986328125 | Mov Loss 2254.72666015625 | Act Loss 3.984298753738403\n",
            "Epoch: [76/100]\n",
            "Train loss: 1434.8913024902345 | Mov Loss 1431.2209967041015 | Act Loss 3.670301878452301\n",
            "Valid loss: 2201.44482421875 | Mov Loss 2197.4516845703124 | Act Loss 3.9931564807891844\n",
            "Epoch: [77/100]\n",
            "Train loss: 1437.2229119873048 | Mov Loss 1433.490941772461 | Act Loss 3.73197074174881\n",
            "Valid loss: 2207.907080078125 | Mov Loss 2203.777587890625 | Act Loss 4.129505157470703\n",
            "Epoch: [78/100]\n",
            "Train loss: 1448.4997296142578 | Mov Loss 1444.834966430664 | Act Loss 3.664770231246948\n",
            "Valid loss: 2243.269580078125 | Mov Loss 2239.3902587890625 | Act Loss 3.879316234588623\n",
            "Epoch: [79/100]\n",
            "Train loss: 1452.2270092773438 | Mov Loss 1448.5776794433593 | Act Loss 3.649334547519684\n",
            "Valid loss: 2293.030615234375 | Mov Loss 2289.0214111328123 | Act Loss 4.009215307235718\n",
            "Epoch: [80/100]\n",
            "Train loss: 1426.3048388671875 | Mov Loss 1422.624810180664 | Act Loss 3.680032317638397\n",
            "Valid loss: 2275.3829345703125 | Mov Loss 2271.3414306640625 | Act Loss 4.041529989242553\n",
            "Epoch: [81/100]\n",
            "Train loss: 1427.2452478027344 | Mov Loss 1423.4813177490234 | Act Loss 3.763928692340851\n",
            "Valid loss: 2172.6642333984373 | Mov Loss 2168.6648193359374 | Act Loss 3.999393892288208\n",
            "Epoch: [82/100]\n",
            "Train loss: 1430.6883221435546 | Mov Loss 1426.9663061523438 | Act Loss 3.7220178198814393\n",
            "Valid loss: 2353.50498046875 | Mov Loss 2349.43837890625 | Act Loss 4.066586303710937\n",
            "Epoch: [83/100]\n",
            "Train loss: 1427.8145581054687 | Mov Loss 1424.0366003417969 | Act Loss 3.777966938018799\n",
            "Valid loss: 2234.46689453125 | Mov Loss 2230.2302490234374 | Act Loss 4.236616230010986\n",
            "Epoch: [84/100]\n",
            "Train loss: 1414.558759765625 | Mov Loss 1410.7901989746094 | Act Loss 3.7685597252845766\n",
            "Valid loss: 2228.34248046875 | Mov Loss 2224.398828125 | Act Loss 3.9436995506286623\n",
            "Epoch: [85/100]\n",
            "Train loss: 1416.6372924804687 | Mov Loss 1412.866655883789 | Act Loss 3.770636649131775\n",
            "Valid loss: 2224.42236328125 | Mov Loss 2220.308740234375 | Act Loss 4.113679981231689\n",
            "Epoch: [86/100]\n",
            "Train loss: 1424.0162274169923 | Mov Loss 1420.3605487060547 | Act Loss 3.6556845760345458\n",
            "Valid loss: 2233.5414794921876 | Mov Loss 2229.4614501953124 | Act Loss 4.080019569396972\n",
            "Epoch: [87/100]\n",
            "Train loss: 1407.4908721923828 | Mov Loss 1403.7974792480468 | Act Loss 3.6933933782577513\n",
            "Valid loss: 2271.8676025390623 | Mov Loss 2267.9690185546874 | Act Loss 3.898567008972168\n",
            "Epoch: [88/100]\n",
            "Train loss: 1385.6817736816406 | Mov Loss 1381.9794653320312 | Act Loss 3.702316107749939\n",
            "Valid loss: 2180.7201416015623 | Mov Loss 2176.6325927734374 | Act Loss 4.087491989135742\n",
            "Epoch: [89/100]\n",
            "Train loss: 1399.2775750732421 | Mov Loss 1395.571513671875 | Act Loss 3.706058261394501\n",
            "Valid loss: 2237.7685302734376 | Mov Loss 2233.7930419921877 | Act Loss 3.9755043983459473\n",
            "Epoch: [90/100]\n",
            "Train loss: 1391.960537109375 | Mov Loss 1388.3165533447266 | Act Loss 3.643989474773407\n",
            "Valid loss: 2233.8588623046876 | Mov Loss 2229.9219970703125 | Act Loss 3.936826753616333\n",
            "Epoch: [91/100]\n",
            "Train loss: 1377.7260430908202 | Mov Loss 1374.0588122558593 | Act Loss 3.66723840713501\n",
            "Valid loss: 2225.7861083984376 | Mov Loss 2221.7573974609377 | Act Loss 4.028761291503907\n",
            "Epoch: [92/100]\n",
            "Train loss: 1399.646192626953 | Mov Loss 1395.9541162109374 | Act Loss 3.692074794769287\n",
            "Valid loss: 2239.934521484375 | Mov Loss 2235.9523681640626 | Act Loss 3.982204818725586\n",
            "Epoch: [93/100]\n",
            "Train loss: 1407.3709307861327 | Mov Loss 1403.7063458251953 | Act Loss 3.6645851802825926\n",
            "Valid loss: 2250.0308837890625 | Mov Loss 2245.9966064453124 | Act Loss 4.034296178817749\n",
            "Epoch: [94/100]\n",
            "Train loss: 1379.202215576172 | Mov Loss 1375.5646514892578 | Act Loss 3.6375673389434815\n",
            "Valid loss: 2302.614208984375 | Mov Loss 2298.4988525390627 | Act Loss 4.115397310256958\n",
            "Epoch: [95/100]\n",
            "Train loss: 1379.965375366211 | Mov Loss 1376.2383660888672 | Act Loss 3.72701532125473\n",
            "Valid loss: 2222.5666015625 | Mov Loss 2218.5049072265624 | Act Loss 4.061713409423828\n",
            "Epoch: [96/100]\n",
            "Train loss: 1386.1239483642578 | Mov Loss 1382.4378948974609 | Act Loss 3.686053490638733\n",
            "Valid loss: 2208.9753662109374 | Mov Loss 2204.9716796875 | Act Loss 4.003705072402954\n",
            "Epoch: [97/100]\n",
            "Train loss: 1393.3190155029297 | Mov Loss 1389.533779296875 | Act Loss 3.785236265659332\n",
            "Valid loss: 2337.6135009765626 | Mov Loss 2333.5430908203125 | Act Loss 4.070446872711182\n",
            "Epoch: [98/100]\n",
            "Train loss: 1371.5589367675782 | Mov Loss 1367.7872869873047 | Act Loss 3.7716506719589233\n",
            "Valid loss: 2270.2680908203124 | Mov Loss 2265.968408203125 | Act Loss 4.299687004089355\n",
            "Epoch: [99/100]\n",
            "Train loss: 1373.0935150146483 | Mov Loss 1369.3119561767578 | Act Loss 3.781558110713959\n",
            "Valid loss: 2217.8946044921877 | Mov Loss 2213.7609130859373 | Act Loss 4.13368558883667\n",
            "MODEL:512he-2048hd\n",
            "The model has 21,338,245 trainable parameters\n",
            "Epoch: [0/100]\n",
            "Train loss: 2467.2707458496093 | Mov Loss 2463.435538330078 | Act Loss 3.835203778743744\n",
            "Valid loss: 2528.468701171875 | Mov Loss 2525.02099609375 | Act Loss 3.4476879119873045\n",
            "Epoch: [1/100]\n",
            "Train loss: 2240.74380859375 | Mov Loss 2237.342099609375 | Act Loss 3.4017080092430114\n",
            "Valid loss: 2464.36171875 | Mov Loss 2461.1658203125 | Act Loss 3.1959168910980225\n",
            "Epoch: [2/100]\n",
            "Train loss: 2143.2176623535156 | Mov Loss 2139.9675805664065 | Act Loss 3.2500883531570435\n",
            "Valid loss: 2397.599169921875 | Mov Loss 2394.408349609375 | Act Loss 3.1908209800720213\n",
            "Epoch: [3/100]\n",
            "Train loss: 2088.3061010742185 | Mov Loss 2085.1074829101562 | Act Loss 3.1986208152770996\n",
            "Valid loss: 2378.36181640625 | Mov Loss 2375.193359375 | Act Loss 3.168452739715576\n",
            "Epoch: [4/100]\n",
            "Train loss: 2048.1456982421873 | Mov Loss 2045.001904296875 | Act Loss 3.1437850069999693\n",
            "Valid loss: 2353.21904296875 | Mov Loss 2350.154345703125 | Act Loss 3.06465163230896\n",
            "Epoch: [5/100]\n",
            "Train loss: 2012.8888110351563 | Mov Loss 2009.7664965820313 | Act Loss 3.1223102521896364\n",
            "Valid loss: 2347.0787109375 | Mov Loss 2344.0060302734373 | Act Loss 3.0727078914642334\n",
            "Epoch: [6/100]\n",
            "Train loss: 1978.206318359375 | Mov Loss 1975.1078576660157 | Act Loss 3.0984592962265016\n",
            "Valid loss: 2369.733837890625 | Mov Loss 2366.6612548828125 | Act Loss 3.0725878715515136\n",
            "Epoch: [7/100]\n",
            "Train loss: 1962.6532958984376 | Mov Loss 1959.5566711425781 | Act Loss 3.0966238141059876\n",
            "Valid loss: 2353.854296875 | Mov Loss 2350.7587890625 | Act Loss 3.0954630851745604\n",
            "Epoch: [8/100]\n",
            "Train loss: 1916.8147827148437 | Mov Loss 1913.7268994140625 | Act Loss 3.0878836631774904\n",
            "Valid loss: 2255.7391845703123 | Mov Loss 2252.7031494140624 | Act Loss 3.036051845550537\n",
            "Epoch: [9/100]\n",
            "Train loss: 1903.2608569335937 | Mov Loss 1900.1972338867188 | Act Loss 3.06362895488739\n",
            "Valid loss: 2328.890283203125 | Mov Loss 2325.8041015625 | Act Loss 3.0861551761627197\n",
            "Epoch: [10/100]\n",
            "Train loss: 1885.644454345703 | Mov Loss 1882.5792236328125 | Act Loss 3.0652223086357115\n",
            "Valid loss: 2345.8966064453125 | Mov Loss 2342.8058837890626 | Act Loss 3.090725612640381\n",
            "Epoch: [11/100]\n",
            "Train loss: 1887.91015625 | Mov Loss 1884.8487976074218 | Act Loss 3.0613582873344423\n",
            "Valid loss: 2325.257177734375 | Mov Loss 2322.0296875 | Act Loss 3.227586555480957\n",
            "Epoch: [12/100]\n",
            "Train loss: 1844.9528967285157 | Mov Loss 1841.9018237304688 | Act Loss 3.0510722684860228\n",
            "Valid loss: 2272.906787109375 | Mov Loss 2269.879248046875 | Act Loss 3.027547597885132\n",
            "Epoch: [13/100]\n",
            "Train loss: 1835.18841796875 | Mov Loss 1832.1364331054688 | Act Loss 3.0519782853126527\n",
            "Valid loss: 2349.164794921875 | Mov Loss 2346.09345703125 | Act Loss 3.0713247776031496\n",
            "Epoch: [14/100]\n",
            "Train loss: 1831.014073486328 | Mov Loss 1827.9510668945313 | Act Loss 3.0630116748809812\n",
            "Valid loss: 2278.897265625 | Mov Loss 2275.812744140625 | Act Loss 3.0845592498779295\n",
            "Epoch: [15/100]\n",
            "Train loss: 1805.052979736328 | Mov Loss 1802.0242529296875 | Act Loss 3.028734838962555\n",
            "Valid loss: 2278.764697265625 | Mov Loss 2275.7183837890625 | Act Loss 3.046304988861084\n",
            "Epoch: [16/100]\n",
            "Train loss: 1789.2893676757812 | Mov Loss 1786.249775390625 | Act Loss 3.0395837736129763\n",
            "Valid loss: 2314.024267578125 | Mov Loss 2310.90966796875 | Act Loss 3.1146525859832765\n",
            "Epoch: [17/100]\n",
            "Train loss: 1792.9296435546876 | Mov Loss 1789.8919165039063 | Act Loss 3.037722210884094\n",
            "Valid loss: 2289.768408203125 | Mov Loss 2286.681689453125 | Act Loss 3.086781454086304\n",
            "Epoch: [18/100]\n",
            "Train loss: 1767.930869140625 | Mov Loss 1764.9070275878905 | Act Loss 3.0238346529006956\n",
            "Valid loss: 2345.88330078125 | Mov Loss 2342.752294921875 | Act Loss 3.130991315841675\n",
            "Epoch: [19/100]\n",
            "Train loss: 1774.3710717773438 | Mov Loss 1771.3323059082031 | Act Loss 3.0387663769721986\n",
            "Valid loss: 2305.562060546875 | Mov Loss 2302.442724609375 | Act Loss 3.1193081855773928\n",
            "Epoch: [20/100]\n",
            "Train loss: 1748.736580810547 | Mov Loss 1745.715146484375 | Act Loss 3.0214360308647157\n",
            "Valid loss: 2338.994775390625 | Mov Loss 2335.785791015625 | Act Loss 3.2089755535125732\n",
            "Epoch: [21/100]\n",
            "Train loss: 1745.3048254394532 | Mov Loss 1742.1998974609376 | Act Loss 3.104927306175232\n",
            "Valid loss: 2290.6650146484376 | Mov Loss 2287.478271484375 | Act Loss 3.186777877807617\n",
            "Epoch: [22/100]\n",
            "Train loss: 1741.0347412109375 | Mov Loss 1738.0000524902343 | Act Loss 3.0346905469894407\n",
            "Valid loss: 2271.8449951171874 | Mov Loss 2268.5182861328126 | Act Loss 3.3267141819000243\n",
            "Epoch: [23/100]\n",
            "Train loss: 1726.4946752929689 | Mov Loss 1723.4175012207031 | Act Loss 3.0771755623817443\n",
            "Valid loss: 2342.879296875 | Mov Loss 2339.62509765625 | Act Loss 3.254158878326416\n",
            "Epoch: [24/100]\n",
            "Train loss: 1716.871131591797 | Mov Loss 1713.7309411621093 | Act Loss 3.140191447734833\n",
            "Valid loss: 2296.1721435546874 | Mov Loss 2292.8323974609375 | Act Loss 3.3397482872009276\n",
            "Epoch: [25/100]\n",
            "Train loss: 1713.605262451172 | Mov Loss 1710.4694702148438 | Act Loss 3.135795452594757\n",
            "Valid loss: 2289.689404296875 | Mov Loss 2286.387939453125 | Act Loss 3.3014807224273683\n",
            "Epoch: [26/100]\n",
            "Train loss: 1690.7010791015625 | Mov Loss 1687.6314831542968 | Act Loss 3.069594497680664\n",
            "Valid loss: 2274.752001953125 | Mov Loss 2271.5998046875 | Act Loss 3.1522117614746095\n",
            "Epoch: [27/100]\n",
            "Train loss: 1693.9574963378907 | Mov Loss 1690.8963134765625 | Act Loss 3.0611801671981813\n",
            "Valid loss: 2336.4495361328127 | Mov Loss 2333.2675537109376 | Act Loss 3.1819828033447264\n",
            "Epoch: [28/100]\n",
            "Train loss: 1688.864334716797 | Mov Loss 1685.782459716797 | Act Loss 3.081877291202545\n",
            "Valid loss: 2281.5773193359373 | Mov Loss 2278.3171630859374 | Act Loss 3.260172653198242\n",
            "Epoch: [29/100]\n",
            "Train loss: 1686.1643298339843 | Mov Loss 1683.0675622558595 | Act Loss 3.0967716813087462\n",
            "Valid loss: 2292.685595703125 | Mov Loss 2289.445556640625 | Act Loss 3.2400198936462403\n",
            "Epoch: [30/100]\n",
            "Train loss: 1679.70208984375 | Mov Loss 1676.60224609375 | Act Loss 3.0998445892333986\n",
            "Valid loss: 2294.103369140625 | Mov Loss 2290.804248046875 | Act Loss 3.2991204261779785\n",
            "Epoch: [31/100]\n",
            "Train loss: 1678.0331481933595 | Mov Loss 1674.9192211914062 | Act Loss 3.113926045894623\n",
            "Valid loss: 2355.03818359375 | Mov Loss 2351.5701171875 | Act Loss 3.4680465698242187\n",
            "Epoch: [32/100]\n",
            "Train loss: 1660.8494775390625 | Mov Loss 1657.7345593261718 | Act Loss 3.114921326637268\n",
            "Valid loss: 2339.5063720703124 | Mov Loss 2335.928076171875 | Act Loss 3.578316259384155\n",
            "Epoch: [33/100]\n",
            "Train loss: 1645.1195166015625 | Mov Loss 1642.0117028808593 | Act Loss 3.1078122639656067\n",
            "Valid loss: 2236.4123046875 | Mov Loss 2233.143505859375 | Act Loss 3.268803119659424\n",
            "Epoch: [34/100]\n",
            "Train loss: 1638.6150708007813 | Mov Loss 1635.526806640625 | Act Loss 3.0882624316215517\n",
            "Valid loss: 2232.48505859375 | Mov Loss 2229.268408203125 | Act Loss 3.2166755199432373\n",
            "Epoch: [35/100]\n",
            "Train loss: 1638.9012414550782 | Mov Loss 1635.8816735839844 | Act Loss 3.0195715689659117\n",
            "Valid loss: 2290.2673583984374 | Mov Loss 2287.0229248046876 | Act Loss 3.2444369316101076\n",
            "Epoch: [36/100]\n",
            "Train loss: 1630.880050048828 | Mov Loss 1627.7764831542968 | Act Loss 3.103573246002197\n",
            "Valid loss: 2217.123681640625 | Mov Loss 2213.8198974609377 | Act Loss 3.303756761550903\n",
            "Epoch: [37/100]\n",
            "Train loss: 1627.6025659179688 | Mov Loss 1624.478955078125 | Act Loss 3.123613700866699\n",
            "Valid loss: 2274.24404296875 | Mov Loss 2271.0275390625 | Act Loss 3.2165332794189454\n",
            "Epoch: [38/100]\n",
            "Train loss: 1614.8512744140626 | Mov Loss 1611.7484423828125 | Act Loss 3.102831919193268\n",
            "Valid loss: 2317.6303955078124 | Mov Loss 2314.1516845703127 | Act Loss 3.478769636154175\n",
            "Epoch: [39/100]\n",
            "Train loss: 1629.3505041503906 | Mov Loss 1626.1970910644532 | Act Loss 3.153419461250305\n",
            "Valid loss: 2269.451025390625 | Mov Loss 2265.98916015625 | Act Loss 3.461810827255249\n",
            "Epoch: [40/100]\n",
            "Train loss: 1613.1061511230469 | Mov Loss 1609.889912109375 | Act Loss 3.216242861747742\n",
            "Valid loss: 2254.62421875 | Mov Loss 2251.307421875 | Act Loss 3.316828489303589\n",
            "Epoch: [41/100]\n",
            "Train loss: 1584.3476916503905 | Mov Loss 1581.1423022460938 | Act Loss 3.205383403301239\n",
            "Valid loss: 2263.689111328125 | Mov Loss 2260.278125 | Act Loss 3.4110279083251953\n",
            "Epoch: [42/100]\n",
            "Train loss: 1608.9571606445313 | Mov Loss 1605.7913342285156 | Act Loss 3.165830075740814\n",
            "Valid loss: 2389.7728515625 | Mov Loss 2386.455712890625 | Act Loss 3.3171228408813476\n",
            "Epoch: [43/100]\n",
            "Train loss: 1601.8703381347657 | Mov Loss 1598.7379284667968 | Act Loss 3.1324090552330017\n",
            "Valid loss: 2280.087158203125 | Mov Loss 2276.757958984375 | Act Loss 3.3291893005371094\n",
            "Epoch: [44/100]\n",
            "Train loss: 1583.929409790039 | Mov Loss 1580.7484313964844 | Act Loss 3.180981602668762\n",
            "Valid loss: 2292.4115966796876 | Mov Loss 2289.123193359375 | Act Loss 3.2883870601654053\n",
            "Epoch: [45/100]\n",
            "Train loss: 1580.3138928222656 | Mov Loss 1577.177247314453 | Act Loss 3.1366522336006164\n",
            "Valid loss: 2353.780615234375 | Mov Loss 2350.352783203125 | Act Loss 3.427832841873169\n",
            "Epoch: [46/100]\n",
            "Train loss: 1572.7806787109375 | Mov Loss 1569.6182055664062 | Act Loss 3.1624777126312256\n",
            "Valid loss: 2313.8087158203125 | Mov Loss 2310.4902587890624 | Act Loss 3.318442440032959\n",
            "Epoch: [47/100]\n",
            "Train loss: 1570.401602783203 | Mov Loss 1567.2396667480468 | Act Loss 3.1619359636306763\n",
            "Valid loss: 2293.8924560546875 | Mov Loss 2290.4741455078124 | Act Loss 3.4183520793914797\n",
            "Epoch: [48/100]\n",
            "Train loss: 1547.2332958984375 | Mov Loss 1544.0045495605468 | Act Loss 3.228746271133423\n",
            "Valid loss: 2286.848779296875 | Mov Loss 2283.426025390625 | Act Loss 3.422709608078003\n",
            "Epoch: [49/100]\n",
            "Train loss: 1555.4333947753905 | Mov Loss 1552.229783935547 | Act Loss 3.2036119532585143\n",
            "Valid loss: 2263.294921875 | Mov Loss 2260.01416015625 | Act Loss 3.280757999420166\n",
            "Epoch: [50/100]\n",
            "Train loss: 1545.3601489257812 | Mov Loss 1542.1470874023437 | Act Loss 3.213067765235901\n",
            "Valid loss: 2239.5047119140627 | Mov Loss 2236.265234375 | Act Loss 3.2394902229309084\n",
            "Epoch: [51/100]\n",
            "Train loss: 1541.2124365234374 | Mov Loss 1538.0423767089844 | Act Loss 3.1700639176368712\n",
            "Valid loss: 2253.2816162109375 | Mov Loss 2250.044775390625 | Act Loss 3.236828660964966\n",
            "Epoch: [52/100]\n",
            "Train loss: 1538.7719915771484 | Mov Loss 1535.5029162597657 | Act Loss 3.2690767574310304\n",
            "Valid loss: 2243.78134765625 | Mov Loss 2240.4365478515624 | Act Loss 3.344767713546753\n",
            "Epoch: [53/100]\n",
            "Train loss: 1537.2149145507813 | Mov Loss 1534.0373034667969 | Act Loss 3.177612156867981\n",
            "Valid loss: 2315.734619140625 | Mov Loss 2312.3087158203125 | Act Loss 3.4258667469024657\n",
            "Epoch: [54/100]\n",
            "Train loss: 1520.2100891113282 | Mov Loss 1517.002618408203 | Act Loss 3.2074726247787475\n",
            "Valid loss: 2280.6572021484376 | Mov Loss 2277.3597412109375 | Act Loss 3.297521162033081\n",
            "Epoch: [55/100]\n",
            "Train loss: 1515.66521484375 | Mov Loss 1512.492646484375 | Act Loss 3.1725707960128786\n",
            "Valid loss: 2274.85849609375 | Mov Loss 2271.5303466796877 | Act Loss 3.3281410694122315\n",
            "Epoch: [56/100]\n",
            "Train loss: 1522.6651770019532 | Mov Loss 1519.4967407226563 | Act Loss 3.1684412217140197\n",
            "Valid loss: 2235.173388671875 | Mov Loss 2231.6727294921875 | Act Loss 3.5007110595703126\n",
            "Epoch: [57/100]\n",
            "Train loss: 1505.1414459228515 | Mov Loss 1501.9365240478517 | Act Loss 3.20491929769516\n",
            "Valid loss: 2319.80146484375 | Mov Loss 2316.3810546875 | Act Loss 3.4203993320465087\n",
            "Epoch: [58/100]\n",
            "Train loss: 1496.0656475830078 | Mov Loss 1492.8214270019532 | Act Loss 3.244215588569641\n",
            "Valid loss: 2225.469580078125 | Mov Loss 2222.0492431640623 | Act Loss 3.420375347137451\n",
            "Epoch: [59/100]\n",
            "Train loss: 1509.6848937988282 | Mov Loss 1506.4042645263671 | Act Loss 3.280626726150513\n",
            "Valid loss: 2257.8404052734377 | Mov Loss 2254.4365478515624 | Act Loss 3.403841733932495\n",
            "Epoch: [60/100]\n",
            "Train loss: 1489.4501403808595 | Mov Loss 1486.2246875 | Act Loss 3.2254510045051576\n",
            "Valid loss: 2267.823681640625 | Mov Loss 2264.518017578125 | Act Loss 3.3056970596313477\n",
            "Epoch: [61/100]\n",
            "Train loss: 1492.7446252441407 | Mov Loss 1489.51060546875 | Act Loss 3.234015870094299\n",
            "Valid loss: 2189.4147216796873 | Mov Loss 2185.9443359375 | Act Loss 3.470364141464233\n",
            "Epoch: [62/100]\n",
            "Train loss: 1493.1759509277344 | Mov Loss 1489.8377685546875 | Act Loss 3.3381892943382265\n",
            "Valid loss: 2289.279833984375 | Mov Loss 2285.9224365234377 | Act Loss 3.3574118614196777\n",
            "Epoch: [63/100]\n",
            "Train loss: 1483.757998046875 | Mov Loss 1480.4833264160156 | Act Loss 3.274669988155365\n",
            "Valid loss: 2224.616064453125 | Mov Loss 2221.250341796875 | Act Loss 3.365645503997803\n",
            "Epoch: [64/100]\n",
            "Train loss: 1500.1370959472656 | Mov Loss 1496.7424768066405 | Act Loss 3.394620518684387\n",
            "Valid loss: 2273.277734375 | Mov Loss 2269.757470703125 | Act Loss 3.5202420711517335\n",
            "Epoch: [65/100]\n",
            "Train loss: 1465.0496557617187 | Mov Loss 1461.6522778320314 | Act Loss 3.397375214099884\n",
            "Valid loss: 2210.6785888671875 | Mov Loss 2207.1472412109374 | Act Loss 3.5313665866851807\n",
            "Epoch: [66/100]\n",
            "Train loss: 1460.3766198730468 | Mov Loss 1457.0595532226562 | Act Loss 3.317064607143402\n",
            "Valid loss: 2304.544580078125 | Mov Loss 2301.168212890625 | Act Loss 3.3764231204986572\n",
            "Epoch: [67/100]\n",
            "Train loss: 1473.2077673339843 | Mov Loss 1469.8453002929687 | Act Loss 3.3624668312072754\n",
            "Valid loss: 2267.9035400390626 | Mov Loss 2264.412939453125 | Act Loss 3.4906408309936525\n",
            "Epoch: [68/100]\n",
            "Train loss: 1477.3662225341798 | Mov Loss 1473.9504107666016 | Act Loss 3.4158060431480406\n",
            "Valid loss: 2276.309130859375 | Mov Loss 2272.68447265625 | Act Loss 3.6246595859527586\n",
            "Epoch: [69/100]\n",
            "Train loss: 1458.734613647461 | Mov Loss 1455.3621691894532 | Act Loss 3.372446472644806\n",
            "Valid loss: 2217.3299560546875 | Mov Loss 2213.9520263671875 | Act Loss 3.377970314025879\n",
            "Epoch: [70/100]\n",
            "Train loss: 1440.73279296875 | Mov Loss 1437.3930432128907 | Act Loss 3.339750392436981\n",
            "Valid loss: 2291.2635986328123 | Mov Loss 2287.8652099609376 | Act Loss 3.3984062671661377\n",
            "Epoch: [71/100]\n",
            "Train loss: 1455.1023706054686 | Mov Loss 1451.7239489746094 | Act Loss 3.3784153032302857\n",
            "Valid loss: 2410.41220703125 | Mov Loss 2406.954833984375 | Act Loss 3.45740065574646\n",
            "Epoch: [72/100]\n",
            "Train loss: 1460.25416015625 | Mov Loss 1456.853095703125 | Act Loss 3.4010653924942016\n",
            "Valid loss: 2302.498583984375 | Mov Loss 2298.884912109375 | Act Loss 3.6136766910552978\n",
            "Epoch: [73/100]\n",
            "Train loss: 1447.5511877441406 | Mov Loss 1444.180439453125 | Act Loss 3.3707479071617126\n",
            "Valid loss: 2288.7750732421873 | Mov Loss 2285.2499267578123 | Act Loss 3.525144672393799\n",
            "Epoch: [74/100]\n",
            "Train loss: 1445.828905029297 | Mov Loss 1442.4469647216797 | Act Loss 3.381940860748291\n",
            "Valid loss: 2320.264306640625 | Mov Loss 2316.682666015625 | Act Loss 3.5816730499267577\n",
            "Epoch: [75/100]\n",
            "Train loss: 1449.7404553222657 | Mov Loss 1446.3443981933594 | Act Loss 3.396060197353363\n",
            "Valid loss: 2290.962939453125 | Mov Loss 2287.4076416015623 | Act Loss 3.5552125930786134\n",
            "Epoch: [76/100]\n",
            "Train loss: 1455.1302178955077 | Mov Loss 1451.759962158203 | Act Loss 3.3702544498443605\n",
            "Valid loss: 2303.9330322265623 | Mov Loss 2300.4666015625 | Act Loss 3.4664817333221434\n",
            "Epoch: [77/100]\n",
            "Train loss: 1437.1060168457032 | Mov Loss 1433.782178955078 | Act Loss 3.3238374328613283\n",
            "Valid loss: 2285.6515380859373 | Mov Loss 2282.0954345703126 | Act Loss 3.556118392944336\n",
            "Epoch: [78/100]\n",
            "Train loss: 1433.2150378417969 | Mov Loss 1429.8841528320313 | Act Loss 3.3308861804008485\n",
            "Valid loss: 2246.0453369140623 | Mov Loss 2242.446728515625 | Act Loss 3.598661756515503\n",
            "Epoch: [79/100]\n",
            "Train loss: 1446.710810546875 | Mov Loss 1443.2910498046874 | Act Loss 3.419769518375397\n",
            "Valid loss: 2261.368359375 | Mov Loss 2257.6491943359374 | Act Loss 3.719185733795166\n",
            "Epoch: [80/100]\n",
            "Train loss: 1435.532020263672 | Mov Loss 1432.1294885253906 | Act Loss 3.402530529499054\n",
            "Valid loss: 2207.1746337890627 | Mov Loss 2203.707568359375 | Act Loss 3.467089605331421\n",
            "Epoch: [81/100]\n",
            "Train loss: 1424.7324536132812 | Mov Loss 1421.3501177978515 | Act Loss 3.3823435068130494\n",
            "Valid loss: 2277.1424560546875 | Mov Loss 2273.6806884765624 | Act Loss 3.4617595195770265\n",
            "Epoch: [82/100]\n",
            "Train loss: 1433.7834228515626 | Mov Loss 1430.3717236328125 | Act Loss 3.4116954612731933\n",
            "Valid loss: 2213.48984375 | Mov Loss 2210.024853515625 | Act Loss 3.4650319099426268\n",
            "Epoch: [83/100]\n",
            "Train loss: 1423.7115362548827 | Mov Loss 1420.2942083740234 | Act Loss 3.4173295736312865\n",
            "Valid loss: 2203.5677001953127 | Mov Loss 2200.115869140625 | Act Loss 3.4517753601074217\n",
            "Epoch: [84/100]\n",
            "Train loss: 1436.1727294921875 | Mov Loss 1432.8265704345704 | Act Loss 3.3461649203300476\n",
            "Valid loss: 2266.4892578125 | Mov Loss 2263.002587890625 | Act Loss 3.486681413650513\n",
            "Epoch: [85/100]\n",
            "Train loss: 1420.8099291992187 | Mov Loss 1417.453421020508 | Act Loss 3.3565049815177916\n",
            "Valid loss: 2271.6884521484376 | Mov Loss 2268.2281005859377 | Act Loss 3.460326814651489\n",
            "Epoch: [86/100]\n",
            "Train loss: 1441.4732818603516 | Mov Loss 1437.9918615722656 | Act Loss 3.4814224672317504\n",
            "Valid loss: 2262.5001953125 | Mov Loss 2258.8593505859376 | Act Loss 3.640860986709595\n",
            "Epoch: [87/100]\n",
            "Train loss: 1433.8788720703126 | Mov Loss 1430.4303161621094 | Act Loss 3.4485545349121094\n",
            "Valid loss: 2257.290625 | Mov Loss 2253.6552001953123 | Act Loss 3.6354242324829102\n",
            "Epoch: [88/100]\n",
            "Train loss: 1440.5812701416016 | Mov Loss 1437.0840753173827 | Act Loss 3.4971910953521728\n",
            "Valid loss: 2226.328125 | Mov Loss 2222.690625 | Act Loss 3.6374855041503906\n",
            "Epoch: [89/100]\n",
            "Train loss: 1408.608441772461 | Mov Loss 1405.2108666992187 | Act Loss 3.3975732922554016\n",
            "Valid loss: 2234.1138916015625 | Mov Loss 2230.6682861328127 | Act Loss 3.4455796241760255\n",
            "Epoch: [90/100]\n",
            "Train loss: 1411.5601440429687 | Mov Loss 1408.0965832519532 | Act Loss 3.4635665369033815\n",
            "Valid loss: 2301.7306640625 | Mov Loss 2298.3030517578127 | Act Loss 3.427680778503418\n",
            "Epoch: [91/100]\n",
            "Train loss: 1438.5642974853515 | Mov Loss 1435.1592755126953 | Act Loss 3.4050289011001587\n",
            "Valid loss: 2226.150830078125 | Mov Loss 2222.710791015625 | Act Loss 3.439985227584839\n",
            "Epoch: [92/100]\n",
            "Train loss: 1424.5654125976562 | Mov Loss 1421.1961669921875 | Act Loss 3.369237859249115\n",
            "Valid loss: 2302.025927734375 | Mov Loss 2298.5142822265625 | Act Loss 3.5116676807403566\n",
            "Epoch: [93/100]\n",
            "Train loss: 1416.7720922851563 | Mov Loss 1413.445313720703 | Act Loss 3.326773211956024\n",
            "Valid loss: 2298.3145751953125 | Mov Loss 2294.8482177734377 | Act Loss 3.4663532257080076\n",
            "Epoch: [94/100]\n",
            "Train loss: 1442.6932202148437 | Mov Loss 1439.2844543457031 | Act Loss 3.4087721610069277\n",
            "Valid loss: 2343.42509765625 | Mov Loss 2339.8697265625 | Act Loss 3.555355739593506\n",
            "Epoch: [95/100]\n",
            "Train loss: 1433.8976727294921 | Mov Loss 1430.4039532470704 | Act Loss 3.493712100982666\n",
            "Valid loss: 2390.3101806640625 | Mov Loss 2386.620458984375 | Act Loss 3.68969349861145\n",
            "Epoch: [96/100]\n",
            "Train loss: 1418.7515637207032 | Mov Loss 1415.3323461914063 | Act Loss 3.4192300772666933\n",
            "Valid loss: 2314.609716796875 | Mov Loss 2311.1082763671875 | Act Loss 3.5014772415161133\n",
            "Epoch: [97/100]\n",
            "Train loss: 1395.110034790039 | Mov Loss 1391.7206353759766 | Act Loss 3.3893981122970582\n",
            "Valid loss: 2285.202685546875 | Mov Loss 2281.6310302734373 | Act Loss 3.5716391086578367\n",
            "Epoch: [98/100]\n",
            "Train loss: 1425.2417333984374 | Mov Loss 1421.8285668945311 | Act Loss 3.4131661748886106\n",
            "Valid loss: 2378.002685546875 | Mov Loss 2374.423779296875 | Act Loss 3.5789417743682863\n",
            "Epoch: [99/100]\n",
            "Train loss: 1403.7442224121094 | Mov Loss 1400.4006005859376 | Act Loss 3.3436295557022095\n",
            "Valid loss: 2265.941259765625 | Mov Loss 2262.3917724609373 | Act Loss 3.5495538234710695\n"
          ]
        }
      ],
      "source": [
        "if TUNE_HIDDEN:\n",
        "    for (hidden_size_encoder,hidden_size_decoder) in zip(hidden_size_encoder_ls,hidden_size_decoder_ls):\n",
        "        # Model\n",
        "        model = Stroke2Stroke(encoding_size=encoding_size, delta_size=DELTA_SIZE, class_size=CLASS_SIZE, hidden_size_encoder=hidden_size_encoder, hidden_size_decoder=hidden_size_decoder,\n",
        "                            encoder_layers=encoder_layers,decoder_layers=decoder_layers,latent_size=latent_size,\n",
        "                            device=device).to(device)\n",
        "        print(f\"MODEL:{hidden_size_encoder}he-{hidden_size_decoder}hd\")\n",
        "        print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
        "        model.apply(init_weights)\n",
        "        # Optimizer\n",
        "        optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "        # Set names to save file to\n",
        "        model_bt_file = model_name +f\"-{hidden_size_encoder}he-{hidden_size_decoder}hd\"+ \"-bt.pt\"\n",
        "        model_bv_file = model_name +f\"-{hidden_size_encoder}he-{hidden_size_decoder}hd\"+ \"-bv.pt\"\n",
        "\n",
        "        # TRAIN\n",
        "        train(model,optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TUNE_LR:\n",
        "    for learning_rate in learning_rate_ls:\n",
        "        # Model\n",
        "        model = Stroke2Stroke(encoding_size=encoding_size, delta_size=DELTA_SIZE, class_size=CLASS_SIZE, hidden_size_encoder=hidden_size_encoder, hidden_size_decoder=hidden_size_decoder,\n",
        "                            encoder_layers=encoder_layers,decoder_layers=decoder_layers,latent_size=latent_size,\n",
        "                            device=device).to(device)\n",
        "        print(f\"MODEL:{learning_rate}lr\")\n",
        "        print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
        "        model.apply(init_weights)\n",
        "        # Optimizer\n",
        "        optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "        # Set names to save file to\n",
        "        model_bt_file = model_name +f\"-{learning_rate}lr\"+ \"-bt.pt\"\n",
        "        model_bv_file = model_name +f\"-{learning_rate}lr\"+ \"-bv.pt\"\n",
        "\n",
        "        # TRAIN\n",
        "        train(model,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_MODEL:\n",
        "    # Model\n",
        "    model = Stroke2Stroke(encoding_size=encoding_size, delta_size=DELTA_SIZE, class_size=CLASS_SIZE, hidden_size_encoder=hidden_size_encoder, hidden_size_decoder=hidden_size_decoder,\n",
        "                        encoder_layers=encoder_layers,decoder_layers=decoder_layers,latent_size=latent_size,\n",
        "                        device=device).to(device)\n",
        "    print(f\"MODEL\")\n",
        "    print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
        "    model.apply(init_weights)\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_MODEL:\n",
        "    # Train\n",
        "    train(model,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "if LOAD_MODEL:\n",
        "    model.load_state_dict(torch.load(model_bv_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_seq = train_dataset[5].unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "oUdIUih1BWIX"
      },
      "outputs": [],
      "source": [
        "if TEST_MODEL:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = test_seq.to(device)\n",
        "        trg = test_seq.to(device)\n",
        "        src = torch.movedim(src,0,1)\n",
        "        trg = torch.movedim(trg,0,1)\n",
        "        prediction_dlt, prediction_act = model(src, trg, 0) # turn off teacher forcing\n",
        "        prediction = torch.cat( (prediction_dlt,prediction_act) ,2)\n",
        "        prediction = prediction.squeeze()\n",
        "        action = prediction[:,DELTA_SIZE:]\n",
        "        action = action.argmax(dim=1)\n",
        "        best_guess = F.one_hot(DELTA_SIZE+action, num_classes=5).float()\n",
        "        best_guess[:,:DELTA_SIZE] = prediction[:,:DELTA_SIZE]\n",
        "    best_guess = best_guess[1:]\n",
        "    render_sequence(best_guess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TEST_MODEL:\n",
        "    print(render_sequence(test_seq[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
