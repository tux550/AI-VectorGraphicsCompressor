{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI_OL6WFnwvQ"
      },
      "source": [
        "# Sketch AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VERSION\n",
        "model_version = 8\n",
        "model_name    = f\"v{model_version}-s2s\"\n",
        "model_bt_file = model_name + \"-bt.pt\"\n",
        "model_bv_file = model_name + \"-bv.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODE\n",
        "TUNE_HIDDEN = True\n",
        "TUNE_LR     = False\n",
        "TUNE_FINAL  = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parametros testeados para tuning:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Division de ejecucion\n",
        "- Eric: Learning Rate\n",
        "- Grover: Hidden sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TUNE HYPERPARAMETERS\n",
        "hidden_size_encoder_ls = [128,256,512]\n",
        "hidden_size_decoder_ls = [256,512,2048]\n",
        "learning_rate_ls       = [1e-2,1e-3,1e-4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL HYPERPARAMETERS\n",
        "#Encoder-Decoder config\n",
        "encoding_size       = 5\n",
        "latent_size         = 128\n",
        "hidden_size_encoder = 512\n",
        "hidden_size_decoder = 2048\n",
        "encoder_layers      = 1\n",
        "decoder_layers      = 1\n",
        "# TRAINING HYPERPARAMETERS\n",
        "num_epochs    = 100\n",
        "batch_size    = 64\n",
        "learning_rate = 1e-3\n",
        "CLASS_WEIGTHS = (1,10,100)\n",
        "MOV_WEIGHT    = 1\n",
        "ACT_WEIGHT    = 10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DATASET CONFIG\n",
        "TRAIN_MAX_SIZE = 6400\n",
        "VALID_MAX_SIZE = 320\n",
        "TEST_MAX_SIZE = 320"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CANVAS CONFIG\n",
        "WIDTH  = 512\n",
        "HEIGHT = 256\n",
        "#WIDTH  = 1084\n",
        "#HEIGHT = 526"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RANDOM CONFIG\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ENCODING CONFIG\n",
        "DELTA_SIZE = 2\n",
        "CLASS_SIZE = 3\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "START_TOKEN         = (0,0,1,0,0)\n",
        "PAD_MARKER          = -100\n",
        "PAD_TOKEN           = (0,0,PAD_MARKER,PAD_MARKER,PAD_MARKER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H-BhfDgoFDb"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "kgTu6s6HqYje"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N-RzpO-vjIk",
        "outputId": "06c1b78c-3632-4dd5-9c9a-192aa509a882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU drawsvg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "LZqKBDt4vjsa"
      },
      "outputs": [],
      "source": [
        "import drawsvg as draw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ZpmLfCDTTaqA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Runtime Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seed\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9g4wWGQ2rNm",
        "outputId": "8b9a6c58-4a5d-4afe-abdf-57afdbc022a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#device = torch.device('cpu')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qfnR3ZDoJec"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E72l7SyPrMvR"
      },
      "source": [
        "### Local storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "sMhkCyxVrBaS"
      },
      "outputs": [],
      "source": [
        "dataset_dir = \"dataset\"\n",
        "dataset_filename = \"dataset.npz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "SQpM5fWrqxTq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ya existe el subdirectorio o el archivo dataset.\n"
          ]
        }
      ],
      "source": [
        "!mkdir $dataset_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jGLIoVqrTHc"
      },
      "source": [
        "### Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "rAVso7DooI_U"
      },
      "outputs": [],
      "source": [
        "dataset_url = \"https://storage.googleapis.com/quickdraw_dataset/sketchrnn/airplane.npz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3sdqAZfri6l",
        "outputId": "eda3d62a-6b26-4076-c3a9-451f9fae30b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0 11.0M    0  1926    0     0   4697      0  0:41:09 --:--:--  0:41:09  4709\n",
            "  5 11.0M    5  613k    0     0   410k      0  0:00:27  0:00:01  0:00:26  410k\n",
            " 12 11.0M   12 1406k    0     0   582k      0  0:00:19  0:00:02  0:00:17  582k\n",
            " 18 11.0M   18 2142k    0     0   628k      0  0:00:18  0:00:03  0:00:15  628k\n",
            " 24 11.0M   24 2814k    0     0   631k      0  0:00:17  0:00:04  0:00:13  631k\n",
            " 31 11.0M   31 3533k    0     0   652k      0  0:00:17  0:00:05  0:00:12  705k\n",
            " 37 11.0M   37 4263k    0     0   664k      0  0:00:17  0:00:06  0:00:11  741k\n",
            " 45 11.0M   45 5168k    0     0   694k      0  0:00:16  0:00:07  0:00:09  747k\n",
            " 52 11.0M   52 5966k    0     0   707k      0  0:00:16  0:00:08  0:00:08  761k\n",
            " 59 11.0M   59 6766k    0     0   718k      0  0:00:15  0:00:09  0:00:06  796k\n",
            " 66 11.0M   66 7518k    0     0   722k      0  0:00:15  0:00:10  0:00:05  798k\n",
            " 72 11.0M   72 8174k    0     0   714k      0  0:00:15  0:00:11  0:00:04  778k\n",
            " 80 11.0M   80 9076k    0     0   730k      0  0:00:15  0:00:12  0:00:03  785k\n",
            " 85 11.0M   85 9686k    0     0   722k      0  0:00:15  0:00:13  0:00:02  747k\n",
            " 92 11.0M   92 10.2M    0     0   723k      0  0:00:15  0:00:14  0:00:01  731k\n",
            "100 11.0M  100 11.0M    0     0   740k      0  0:00:15  0:00:15 --:--:--  777k\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"FILE SIZE:\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\"du\" no se reconoce como un comando interno o externo,\n",
            "programa o archivo por lotes ejecutable.\n"
          ]
        }
      ],
      "source": [
        "!curl -o \"$dataset_dir\"/\"$dataset_filename\" \"$dataset_url\"\n",
        "!echo \"FILE SIZE:\"\n",
        "!du -h \"$dataset_dir\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOjjpsUEt6xT"
      },
      "source": [
        "### Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "N2bXk3Rtt2PI"
      },
      "outputs": [],
      "source": [
        "dataset = np.load(dataset_dir+\"/\"+dataset_filename, encoding='latin1', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "dRq4oFkMuG0i"
      },
      "outputs": [],
      "source": [
        "raw_train_dataset = dataset[\"train\"][:TRAIN_MAX_SIZE]\n",
        "raw_valid_dataset = dataset[\"valid\"][:VALID_MAX_SIZE]\n",
        "raw_test_dataset = dataset[\"test\"][:TEST_MAX_SIZE]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99\n"
          ]
        }
      ],
      "source": [
        "max_len = 0\n",
        "for stroke3 in raw_train_dataset:\n",
        "    max_len = max(max_len,stroke3.shape[0])\n",
        "for stroke3 in raw_valid_dataset:\n",
        "    max_len = max(max_len,stroke3.shape[0])\n",
        "for stroke3 in raw_test_dataset:\n",
        "    max_len = max(max_len,stroke3.shape[0])\n",
        "print(max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "6ecNdoCgxdxR"
      },
      "outputs": [],
      "source": [
        "def render_sheep(stroke_3):\n",
        "  d = draw.Drawing(WIDTH, HEIGHT, origin=\"center\")\n",
        "  d.append(draw.Rectangle(-WIDTH/2,-HEIGHT/2,WIDTH,HEIGHT, fill='rgb(255,255,255)'))\n",
        "  x=0\n",
        "  y=0\n",
        "  svg_points = [0,0]\n",
        "  for dx,dy,end in stroke_3:\n",
        "    x+=dx\n",
        "    y+=dy\n",
        "    svg_points.append(x)\n",
        "    svg_points.append(y)\n",
        "    if end:\n",
        "      lines = draw.Lines(*svg_points,fill='none', stroke='black')\n",
        "      svg_points = []\n",
        "      d.append(lines)\n",
        "  # use d.as_svg() to extract full svg\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "HtJd6XQFuru9",
        "outputId": "769c795e-f828-490d-fc28-e1a7d55c6b2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 6400\n",
            "Test size: 320\n",
            "Validate size: 320\n",
            "Stroke 3 input shape: n x 3\n",
            "*(n max = 250)\n"
          ]
        },
        {
          "data": {
            "image/svg+xml": [
              "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
              "     width=\"512\" height=\"256\" viewBox=\"-256.0 -128.0 512 256\">\n",
              "<defs>\n",
              "</defs>\n",
              "<rect x=\"-256.0\" y=\"-128.0\" width=\"512\" height=\"256\" fill=\"rgb(255,255,255)\" />\n",
              "<path d=\"M0,0 L48,0 L154,12 L297,37 L337,48 L406,77 L434,95 L398,133 L372,149 L349,154 L298,153 L210,136 L102,125 L-32,101 L-120,94 L-153,82 L-163,71 L-161,57 L-143,19 L-121,1 L-103,-6 L-72,-6 L-42,9 L-25,12 L-1,12 L22,3\" fill=\"none\" stroke=\"black\" />\n",
              "<path d=\"M97,-9 L230,-181 L247,-208 L253,-92 L251,10\" fill=\"none\" stroke=\"black\" />\n",
              "<path d=\"M69,92 L-2,184 L-81,295 L-74,294 L-40,276 L33,231 L128,178 L161,154 L185,129 L202,116\" fill=\"none\" stroke=\"black\" />\n",
              "<path d=\"M323,69 L321,97 L326,101 L340,106 L386,113\" fill=\"none\" stroke=\"black\" />\n",
              "</svg>"
            ],
            "text/plain": [
              "<drawsvg.drawing.Drawing at 0x1da8f320400>"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Train size:\", raw_train_dataset.shape[0])\n",
        "print(\"Test size:\", raw_test_dataset.shape[0])\n",
        "print(\"Validate size:\", raw_valid_dataset.shape[0])\n",
        "print(\"Stroke 3 input shape: n x\",raw_train_dataset[0].shape[1])\n",
        "print(\"*(n max = 250)\")\n",
        "\n",
        "d = render_sheep(raw_train_dataset[5]) #100\n",
        "d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kECiQL_I6Qou"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Xa1weZDw8aDj"
      },
      "outputs": [],
      "source": [
        "def render_sequence(sequence):\n",
        "  d = draw.Drawing(WIDTH, HEIGHT, origin=\"center\")\n",
        "  d.append(draw.Rectangle(-WIDTH/2,-HEIGHT/2,WIDTH,HEIGHT, fill='rgb(255,255,255)'))\n",
        "  x=0\n",
        "  y=0\n",
        "  svg_points = []\n",
        "  for i, (dx,dy,line,lift,end) in enumerate(sequence):\n",
        "    x+=dx.item()\n",
        "    y+=dy.item()\n",
        "    svg_points.append(x)\n",
        "    svg_points.append(y)\n",
        "    if lift or end:\n",
        "      lines = draw.Lines(*svg_points,fill='none', stroke='black')\n",
        "      svg_points = []\n",
        "      d.append(lines)\n",
        "    if end:\n",
        "      break\n",
        "    if i == len(sequence)-1:\n",
        "      # Force draw\n",
        "      lines = draw.Lines(*svg_points,fill='none', stroke='black')\n",
        "      svg_points = []\n",
        "      d.append(lines)\n",
        "  # use d.as_svg() to extract full svg\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "eGCnxaXF6QLb"
      },
      "outputs": [],
      "source": [
        "def preprocess_sequence(stroke_3):\n",
        "  result = [START_TOKEN,]\n",
        "  for i in range(len(stroke_3)):\n",
        "    dx     = stroke_3[i][0]\n",
        "    dy     = stroke_3[i][1]\n",
        "    action = stroke_3[i][2]\n",
        "\n",
        "    end = int(i == len(stroke_3)-1)\n",
        "    lift = action if i != len(stroke_3)-1 else 0\n",
        "    line = 1-action if i != len(stroke_3)-1 else 0\n",
        "\n",
        "    fv = (dx,dy,line,lift,end)\n",
        "    result.append(fv)\n",
        "  # PAD ENDING\n",
        "  for i in range(MAX_SEQUENCE_LENGTH-len(stroke_3)):\n",
        "    fv = PAD_TOKEN\n",
        "    result.append(fv)\n",
        "  return np.array(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "y3GRelPo9aOO"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(dataset):\n",
        "  new_dataset = np.array([preprocess_sequence(elem) for elem in dataset])\n",
        "  new_dataset = torch.from_numpy(new_dataset)\n",
        "  new_dataset = new_dataset.float()\n",
        "  return new_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Y3IR_AXX9__7"
      },
      "outputs": [],
      "source": [
        "train_dataset = preprocess_dataset(raw_train_dataset)\n",
        "valid_dataset = preprocess_dataset(raw_valid_dataset)\n",
        "test_dataset = preprocess_dataset(raw_test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmVeQ425Drbc",
        "outputId": "05ce3417-bb9a-44df-8d77-e963f23260ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-24.,  -6.,   1.,   0.,   0.])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#With padding\n",
        "train_dataset.shape\n",
        "train_dataset[0][3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "EdJSwOIr8SD4",
        "outputId": "a87b7d5e-9671-4f13-e3fd-27f4be532b6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[   0.,    0.,    1.,    0.,    0.],\n",
            "        [  48.,    0.,    1.,    0.,    0.],\n",
            "        [ 106.,   12.,    1.,    0.,    0.],\n",
            "        [ 143.,   25.,    1.,    0.,    0.],\n",
            "        [  40.,   11.,    1.,    0.,    0.],\n",
            "        [  69.,   29.,    1.,    0.,    0.],\n",
            "        [  28.,   18.,    1.,    0.,    0.],\n",
            "        [ -36.,   38.,    1.,    0.,    0.],\n",
            "        [ -26.,   16.,    1.,    0.,    0.],\n",
            "        [ -23.,    5.,    1.,    0.,    0.],\n",
            "        [ -51.,   -1.,    1.,    0.,    0.],\n",
            "        [ -88.,  -17.,    1.,    0.,    0.],\n",
            "        [-108.,  -11.,    1.,    0.,    0.],\n",
            "        [-134.,  -24.,    1.,    0.,    0.],\n",
            "        [ -88.,   -7.,    1.,    0.,    0.],\n",
            "        [ -33.,  -12.,    1.,    0.,    0.],\n",
            "        [ -10.,  -11.,    1.,    0.,    0.],\n",
            "        [   2.,  -14.,    1.,    0.,    0.],\n",
            "        [  18.,  -38.,    1.,    0.,    0.],\n",
            "        [  22.,  -18.,    1.,    0.,    0.],\n",
            "        [  18.,   -7.,    1.,    0.,    0.],\n",
            "        [  31.,    0.,    1.,    0.,    0.],\n",
            "        [  30.,   15.,    1.,    0.,    0.],\n",
            "        [  17.,    3.,    1.,    0.,    0.],\n",
            "        [  24.,    0.,    1.,    0.,    0.],\n",
            "        [  23.,   -9.,    0.,    1.,    0.],\n",
            "        [  75.,  -12.,    1.,    0.,    0.],\n",
            "        [ 133., -172.,    1.,    0.,    0.],\n",
            "        [  17.,  -27.,    1.,    0.,    0.],\n",
            "        [   6.,  116.,    1.,    0.,    0.],\n",
            "        [  -2.,  102.,    0.,    1.,    0.],\n",
            "        [-182.,   82.,    1.,    0.,    0.],\n",
            "        [ -71.,   92.,    1.,    0.,    0.],\n",
            "        [ -79.,  111.,    1.,    0.,    0.],\n",
            "        [   7.,   -1.,    1.,    0.,    0.],\n",
            "        [  34.,  -18.,    1.,    0.,    0.],\n",
            "        [  73.,  -45.,    1.,    0.,    0.],\n",
            "        [  95.,  -53.,    1.,    0.,    0.],\n",
            "        [  33.,  -24.,    1.,    0.,    0.],\n",
            "        [  24.,  -25.,    1.,    0.,    0.],\n",
            "        [  17.,  -13.,    0.,    1.,    0.],\n",
            "        [ 121.,  -47.,    1.,    0.,    0.],\n",
            "        [  -2.,   28.,    1.,    0.,    0.],\n",
            "        [   5.,    4.,    1.,    0.,    0.],\n",
            "        [  14.,    5.,    1.,    0.,    0.],\n",
            "        [  46.,    7.,    0.,    0.,    1.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.],\n",
            "        [   0.,    0., -100., -100., -100.]], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/svg+xml": [
              "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
              "     width=\"512\" height=\"256\" viewBox=\"-256.0 -128.0 512 256\">\n",
              "<defs>\n",
              "</defs>\n",
              "<rect x=\"-256.0\" y=\"-128.0\" width=\"512\" height=\"256\" fill=\"rgb(255,255,255)\" />\n",
              "<path d=\"M0.0,0.0 L48.0,0.0 L154.0,12.0 L297.0,37.0 L337.0,48.0 L406.0,77.0 L434.0,95.0 L398.0,133.0 L372.0,149.0 L349.0,154.0 L298.0,153.0 L210.0,136.0 L102.0,125.0 L-32.0,101.0 L-120.0,94.0 L-153.0,82.0 L-163.0,71.0 L-161.0,57.0 L-143.0,19.0 L-121.0,1.0 L-103.0,-6.0 L-72.0,-6.0 L-42.0,9.0 L-25.0,12.0 L-1.0,12.0 L22.0,3.0\" fill=\"none\" stroke=\"black\" />\n",
              "<path d=\"M97.0,-9.0 L230.0,-181.0 L247.0,-208.0 L253.0,-92.0 L251.0,10.0\" fill=\"none\" stroke=\"black\" />\n",
              "<path d=\"M69.0,92.0 L-2.0,184.0 L-81.0,295.0 L-74.0,294.0 L-40.0,276.0 L33.0,231.0 L128.0,178.0 L161.0,154.0 L185.0,129.0 L202.0,116.0\" fill=\"none\" stroke=\"black\" />\n",
              "<path d=\"M323.0,69.0 L321.0,97.0 L326.0,101.0 L340.0,106.0 L386.0,113.0\" fill=\"none\" stroke=\"black\" />\n",
              "</svg>"
            ],
            "text/plain": [
              "<drawsvg.drawing.Drawing at 0x1da23569270>"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#s = preprocess_sequence(raw_train_dataset[100])\n",
        "s = train_dataset[5]\n",
        "s=s.to(device)\n",
        "print(s)\n",
        "render_sequence(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUyMn7NC-qVy"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Referencias:\n",
        "https://github.com/bentrevett/pytorch-seq2seq/blob/rewrite/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "gBjsAQsWmN5v"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size_in, rnn_layers, latent_size, dropout=0.1):\n",
        "    super(Encoder,self).__init__()\n",
        "    # Input config\n",
        "    self.input_size  = input_size\n",
        "    # RNN config\n",
        "    self.hidden_size_in  = hidden_size_in\n",
        "    self.latent_size = latent_size\n",
        "    self.rnn_layers  = rnn_layers\n",
        "    # Layers\n",
        "    self.rnn = nn.LSTM(self.input_size,\n",
        "                       self.hidden_size_in,\n",
        "                       self.rnn_layers,\n",
        "                       bidirectional=True)\n",
        "    self.fc_latent = nn.Linear(2*2*self.rnn_layers*self.hidden_size_in,\n",
        "                      self.latent_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x):\n",
        "    # x shape: (seq_length, batch, input_size)\n",
        "    num_batch = x.shape[1]\n",
        "    # RNN\n",
        "    # output shape: (seq_length, batch, D * hidden_size)\n",
        "    # hidden shape: (D * num_layers, batch, hidden_size)\n",
        "    # cell   shape: (D * num_layers, batch, hidden_size)\n",
        "    # * Note: D=2 if bidiretional=True else 1\n",
        "    outputs, (h, c) = self.rnn(x)\n",
        "    # hc   shape: (D * num_layers, batch, 2*hidden_size)\n",
        "    hc = torch.cat( (h,c), 2 )\n",
        "    hc = torch.movedim(hc,0,2)\n",
        "    # hc   shape: (batch, 2*D*num_layers*hidden_size)\n",
        "    hc = hc.reshape(num_batch,-1)\n",
        "    # context  shape: (batch, latent_size)\n",
        "    context =  self.fc_latent(self.dropout(hc))\n",
        "\n",
        "    ### RETURN CONTEXT VECTOR\n",
        "    return context.contiguous()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "djBHv5ZRs7_F"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_size, output_size_delta, output_size_class, hidden_size, rnn_layers, latent_size, dropout=0.1):\n",
        "    super(Decoder,self).__init__()\n",
        "    # Input-Output config\n",
        "    self.input_size        = input_size\n",
        "    self.latent_size       = latent_size\n",
        "    self.output_size_delta = output_size_delta\n",
        "    self.output_size_class = output_size_class\n",
        "    # RNN config\n",
        "    self.hidden_size = hidden_size\n",
        "    self.rnn_layers  = rnn_layers\n",
        "    # Layers\n",
        "    self.fc_hidden = nn.Linear(self.latent_size,\n",
        "                      self.hidden_size*self.rnn_layers)\n",
        "    self.fc_cell   = nn.Linear(self.latent_size,\n",
        "                      self.hidden_size*self.rnn_layers)\n",
        "    self.rnn = nn.LSTM(self.input_size+self.latent_size,\n",
        "                       self.hidden_size,\n",
        "                       self.rnn_layers)\n",
        "    self.fc_dlt  = nn.Linear(self.hidden_size,\n",
        "                        self.output_size_delta)\n",
        "    self.fc_cls  = nn.Linear(self.hidden_size,\n",
        "                        self.output_size_class)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x, context, hidden, cell):\n",
        "    ### GET RNN INPUT\n",
        "    # x shape:         (batch, input_size) but need N batches of seq_length 1\n",
        "    x      = x.unsqueeze(0) #(1, batch, input_size)\n",
        "    # context shape:  (batch, latent_size) but need N batches of seq_length 1\n",
        "    context = context.unsqueeze(0) #(1, batch, input_size)\n",
        "    # rnn_input shape: (1, batch, input_size+latent_size)\n",
        "    rnn_input = torch.cat((x, context),2).contiguous()\n",
        "\n",
        "    ### RNN\n",
        "    # output shape: (1, batch, D * hidden_size)\n",
        "    # hidden shape: (D * num_layers, batch, hidden_size)\n",
        "    # cell   shape: (D * num_layers, batch, hidden_size)\n",
        "    # * Note: D=2 if bidiretional=True else 1\n",
        "    output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "\n",
        "    ### GET PREDICTIONS\n",
        "    # pred_delta shape: (1, batch, output_size_delta)\n",
        "    # pred_class shape: (1, batch, output_size_class)\n",
        "    pred_delta =  self.fc_dlt(self.dropout(output))\n",
        "    pred_class =  self.fc_cls(self.dropout((output))) #torch.sigmoid(self.fc_cls(self.dropout((output))))\n",
        "    # pred_delta shape: (batch, output_size_delta)\n",
        "    # pred_class shape: (batch, output_size_class)\n",
        "    pred_delta = pred_delta.squeeze(0)\n",
        "    pred_class = pred_class.squeeze(0)\n",
        "\n",
        "    ### RETURN PREDICTIONS AND STATE\n",
        "    return pred_delta, pred_class, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "b9oIC3Giw-qH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Stroke2Stroke(nn.Module):\n",
        "  def __init__(self, encoding_size, delta_size, class_size, hidden_size_encoder, hidden_size_decoder, encoder_layers, decoder_layers, latent_size, device, dropout=0.1):\n",
        "    super(Stroke2Stroke, self).__init__()\n",
        "    assert encoding_size == delta_size+class_size\n",
        "    # SIZE CONFIG\n",
        "    self.encoding_size = encoding_size\n",
        "    self.delta_size    = delta_size\n",
        "    self.class_size    = class_size\n",
        "    self.latent_size   = latent_size\n",
        "    # ENCODER/DECODER\n",
        "    # Encoder: (input_size, hidden_size_in, rnn_layers, latent_size, dropout=0.1)\n",
        "    # Decoder: (input_size, output_size_delta, output_size_class, hidden_size, rnn_layers, latent_size, dropout=0.1):\n",
        "    self.encoder = Encoder(encoding_size, hidden_size_encoder, encoder_layers, latent_size)\n",
        "    self.decoder = Decoder(encoding_size, self.delta_size, self.class_size, hidden_size_decoder, decoder_layers, latent_size)\n",
        "    # DEVICE\n",
        "    self.device = device\n",
        "    # INITIAL STATE LAYERS\n",
        "    self.fc_hidden = nn.Linear(self.latent_size,\n",
        "                      hidden_size_decoder*decoder_layers)\n",
        "    self.fc_cell   = nn.Linear(self.latent_size,\n",
        "                      hidden_size_decoder*decoder_layers)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def get_initial_states(self, context):\n",
        "    num_batch = context.shape[0]\n",
        "    ### EXTRACT HIDDEN AND CELL FROM CONTEXT VECTOR\n",
        "    # context shape: (batch, latent_size)\n",
        "    # hidden  shape: (batch, num_layers*hidden_size)\n",
        "    # cell    shape: (batch, num_layers*hidden_size)\n",
        "    hidden =  self.fc_hidden(self.dropout(context))\n",
        "    cell   =  self.fc_cell(self.dropout(context))\n",
        "    # hidden  shape: (num_layers, batch, hidden_size)\n",
        "    # cell    shape: (num_layers, batch, hidden_size)\n",
        "    hidden = hidden.reshape(num_batch,self.decoder.rnn_layers,self.decoder.hidden_size)\n",
        "    cell   = cell.reshape(num_batch,self.decoder.rnn_layers,self.decoder.hidden_size)\n",
        "    hidden = torch.movedim(hidden,0,1).contiguous()\n",
        "    cell   = torch.movedim(cell,0,1).contiguous()\n",
        "    ### RETURN INITIAL STATES\n",
        "    return hidden, cell\n",
        "\n",
        "  def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "    ### GET SHAPES\n",
        "    # source shape: (src_seq_length, batch, encoding_size)\n",
        "    # target shape: (trg_seq_length, batch, encoding_size)\n",
        "    batch_size  = source.shape[1]\n",
        "    target_len  = target.shape[0]\n",
        "    output_size = target.shape[2]\n",
        "\n",
        "    ### RESREVE DELTA AND CLASS PREDICTION TENSORS\n",
        "    # predictions_dlt shape: (src_seq_length, batch, output_size_delta)\n",
        "    # predictions_cls shape: (src_seq_length, batch, output_size_class)\n",
        "    predictions_dlt = torch.zeros(target_len, batch_size, self.delta_size).to(device)\n",
        "    predictions_cls = torch.zeros(target_len, batch_size, self.class_size).to(device)\n",
        "\n",
        "    ### GET CONTEXT FROM ENCODER\n",
        "    # hidden shape: (D * num_layers, batch, hidden_size)\n",
        "    # cell   shape: (D * num_layers, batch, hidden_size)\n",
        "    context = self.encoder(source)\n",
        "\n",
        "    ### GET INITIAL STATES\n",
        "    hidden, cell = self.get_initial_states(context)\n",
        "\n",
        "    ### START TOKEN\n",
        "    x = target[0]\n",
        "    ### GENERATE\n",
        "    for t in range(1, target_len):\n",
        "      # pred_delta shape: (batch, output_size_delta)\n",
        "      # pred_class shape: (batch, output_size_class)\n",
        "      pred_delta, pred_class, hidden, cell = self.decoder(x, context, hidden, cell)\n",
        "      predictions_dlt[t] = pred_delta\n",
        "      predictions_cls[t] = pred_class\n",
        "\n",
        "      # Get max from one hot encoded actions\n",
        "      #action = pred_class.argmax(1)\n",
        "      #best_action_guess = F.one_hot(action, num_classes=self.class_size).float()\n",
        "\n",
        "      # Create best_guess\n",
        "      # best_guess shape: (batch, encoding_size)\n",
        "      #best_guess = torch.cat((pred_delta, best_action_guess),1)\n",
        "      best_guess = torch.cat((pred_delta, pred_class),1)\n",
        "\n",
        "      # Select next input\n",
        "      x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "    return predictions_dlt, predictions_cls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbnW2lw2BW__"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IzsFqHzOdgs",
        "outputId": "607a32ed-e146-4128-8319-3e23ed0eab71"
      },
      "outputs": [],
      "source": [
        "# Parameter count\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "bKT5p24NS_cT"
      },
      "outputs": [],
      "source": [
        "# DATA LOADERS\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DATA LOADERS\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "c_weights = torch.tensor(CLASS_WEIGTHS)\n",
        "mse_fn = nn.MSELoss(reduction=\"none\")\n",
        "cross_entropy_fn =  nn.CrossEntropyLoss(ignore_index=PAD_MARKER)#, weight=c_weights)\n",
        "\n",
        "def test_fn(model, data_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_mov_loss = 0\n",
        "    test_act_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "            src = batch.to(device)\n",
        "            trg = batch.to(device)\n",
        "            src = torch.movedim(src,0,1)\n",
        "            trg = torch.movedim(trg,0,1)\n",
        "            predictions_dlt, predictions_cls = model(src, trg, 0) # turn off teacher forcing\n",
        "\n",
        "            encoding_size = src.shape[2]\n",
        "            delta_size    = predictions_dlt.shape[2]\n",
        "            class_size    = predictions_cls.shape[2]\n",
        "\n",
        "            # Compute loss\n",
        "            # mask   = [(trg_length-1), batch_size]\n",
        "            # counts = [1]\n",
        "            mask = ~trg[1:,:,-1].eq(PAD_MARKER)       #[(trg_length-1), batch_size]\n",
        "            counts = torch.sum(mask)                  \n",
        "            ## For one-hot\n",
        "            # action_pred = [(trg length - 1) * batch_size, action_size]\n",
        "            action_pred = predictions_cls[1:].view(-1, class_size)\n",
        "            # action_trg = [(trg length - 1) * batch_size]\n",
        "            action_trg  = trg[1:,:,delta_size:].reshape(-1, class_size) # [(trg length - 1) * batch_size, action_size]\n",
        "            action_trg = action_trg.argmax(1)                           # [(trg length - 1) * batch_size]\n",
        "            # action_mask \n",
        "            action_mask = mask.reshape(-1) # [(trg length - 1) * batch_size]\n",
        "            #            Index  if mask else PAD_MARKER\n",
        "            action_trg = action_trg*action_mask + PAD_MARKER*(~action_mask) # [(trg length - 1) * batch_size]\n",
        "            # ACTION LOSS\n",
        "            action_loss = cross_entropy_fn(action_pred, action_trg)\n",
        "            action_loss = ACT_WEIGHT*action_loss\n",
        "            ## For regression\n",
        "            # movement_prd = [(trg_length-1), batch_size, movement_size]\n",
        "            movement_pred = predictions_dlt[1:]\n",
        "            # movement_trg\n",
        "            movement_trg  = trg[1:,:,:delta_size]\n",
        "            # mv_mask   = [(trg_length-1), batch_size, movement_size]\n",
        "            mv_mask = mask.expand((delta_size,-1,-1))    #[movement_size, (trg_length-1), batch_size]\n",
        "            mv_mask = torch.movedim(mv_mask,0,2)            #[(trg_length-1), batch_size, movement_size]\n",
        "            # Apply mask\n",
        "            # https://discuss.pytorch.org/t/how-to-correctly-weight-mse-loss-for-padded-sequences/176211\n",
        "            movement_pred_masked = movement_pred * mv_mask\n",
        "            movement_trg_masked  = movement_trg  * mv_mask\n",
        "            # Get MSE\n",
        "            movement_loss = mse_fn(movement_pred_masked, movement_trg_masked)\n",
        "            movement_loss = torch.sum(movement_loss)/(counts*DELTA_SIZE)\n",
        "            movement_loss = MOV_WEIGHT*movement_loss\n",
        "            # Total loss\n",
        "            loss = movement_loss + action_loss\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            test_mov_loss += movement_loss.item()\n",
        "            test_act_loss += action_loss.item()\n",
        "    return test_loss / len(data_loader), test_mov_loss/ len(data_loader), test_act_loss/ len(data_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "oUdIUih1BWIX"
      },
      "outputs": [],
      "source": [
        "TEST_ITEM_ID = 5 #1 #5\n",
        "def display_test(model, name):\n",
        "    # Load test\n",
        "    test_seq = test_dataset[TEST_ITEM_ID].unsqueeze(0)\n",
        "    # From model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = test_seq.to(device)\n",
        "        trg = test_seq.to(device)\n",
        "        src = torch.movedim(src,0,1)\n",
        "        trg = torch.movedim(trg,0,1)\n",
        "        prediction_dlt, prediction_act = model(src, trg, 0) # turn off teacher forcing\n",
        "        prediction = torch.cat( (prediction_dlt,prediction_act) ,2)\n",
        "        prediction = prediction.squeeze()\n",
        "        action = prediction[:,DELTA_SIZE:]\n",
        "        action = action.argmax(dim=1)\n",
        "        best_guess = F.one_hot(DELTA_SIZE+action, num_classes=5).float()\n",
        "        best_guess[:,:DELTA_SIZE] = prediction[:,:DELTA_SIZE]\n",
        "    best_guess = best_guess[1:]\n",
        "    # Model\n",
        "    d = render_sequence(best_guess) \n",
        "    d.save_svg(f'test_{name}.svg')\n",
        "    d = render_sequence(test_seq[0])\n",
        "    d.save_svg(f'test_original.svg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL:128he-256hd\n",
            "Test Loss: 2175.151025390625 | Test Mov Loss 2171.7437744140625 | Test Act Loss 3.4072475910186766\n",
            "MODEL:256he-512hd\n",
            "Test Loss: 2228.0291748046875 | Test Mov Loss 2223.7459716796875 | Test Act Loss 4.283202123641968\n",
            "MODEL:512he-2048hd\n",
            "Test Loss: 2240.905029296875 | Test Mov Loss 2237.27001953125 | Test Act Loss 3.6349520683288574\n"
          ]
        }
      ],
      "source": [
        "if TUNE_HIDDEN:\n",
        "    for (hidden_size_encoder,hidden_size_decoder) in zip(hidden_size_encoder_ls,hidden_size_decoder_ls):\n",
        "        # Model\n",
        "        model = Stroke2Stroke(encoding_size=encoding_size, delta_size=DELTA_SIZE, class_size=CLASS_SIZE, hidden_size_encoder=hidden_size_encoder, hidden_size_decoder=hidden_size_decoder,\n",
        "                            encoder_layers=encoder_layers,decoder_layers=decoder_layers,latent_size=latent_size,\n",
        "                            device=device).to(device)\n",
        "        print(f\"MODEL:{hidden_size_encoder}he-{hidden_size_decoder}hd\")\n",
        "        # Set names of import file\n",
        "        m_name = f\"-{hidden_size_encoder}he-{hidden_size_decoder}hd\"\n",
        "        model_bt_file = model_name + m_name + \"-bt.pt\"\n",
        "        model_bv_file = model_name + m_name + \"-bv.pt\"\n",
        "        # Load model\n",
        "        model.load_state_dict(torch.load(model_bv_file))\n",
        "        # TEST\n",
        "        test_loss, test_mov_loss, test_act_loss = test_fn(model, test_dataloader)\n",
        "        print(f\"Test Loss: {test_loss} | Test Mov Loss {test_mov_loss} | Test Act Loss {test_act_loss}\")\n",
        "        # SHOW\n",
        "        display_test(model,m_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TUNE_LR:\n",
        "    for learning_rate in learning_rate_ls:\n",
        "        # Model\n",
        "        model = Stroke2Stroke(encoding_size=encoding_size, delta_size=DELTA_SIZE, class_size=CLASS_SIZE, hidden_size_encoder=hidden_size_encoder, hidden_size_decoder=hidden_size_decoder,\n",
        "                            encoder_layers=encoder_layers,decoder_layers=decoder_layers,latent_size=latent_size,\n",
        "                            device=device).to(device)\n",
        "        print(f\"MODEL:{learning_rate}lr\")\n",
        "        # Set names of import file\n",
        "        m_name = f\"-{learning_rate}lr\"\n",
        "        model_bt_file = model_name + m_name + \"-bt.pt\"\n",
        "        model_bv_file = model_name + m_name + \"-bv.pt\"\n",
        "        # Load model\n",
        "        model.load_state_dict(torch.load(model_bv_file))\n",
        "        # TEST\n",
        "        test_loss, test_mov_loss, test_act_loss = test_fn(model, test_dataloader)\n",
        "        print(f\"Test Loss: {test_loss} | Test Mov Loss {test_mov_loss} | Test Act Loss {test_act_loss}\")\n",
        "        # SHOW\n",
        "        display_test(model,m_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TUNE_FINAL:\n",
        "    # Model\n",
        "    model = Stroke2Stroke(encoding_size=encoding_size, delta_size=DELTA_SIZE, class_size=CLASS_SIZE, hidden_size_encoder=hidden_size_encoder, hidden_size_decoder=hidden_size_decoder,\n",
        "                        encoder_layers=encoder_layers,decoder_layers=decoder_layers,latent_size=latent_size,\n",
        "                        device=device).to(device)\n",
        "    print(f\"MODEL: FINAL\")\n",
        "    # Set names of import file\n",
        "    model_bt_file = \"BEST-\" + model_name + \"-bt.pt\"\n",
        "    model_bv_file = \"BEST-\" + model_name + \"-bv.pt\"\n",
        "    # Load model\n",
        "    model.load_state_dict(torch.load(model_bv_file))\n",
        "    # TEST\n",
        "    test_loss, test_mov_loss, test_act_loss = test_fn(model, test_dataloader)\n",
        "    print(f\"Test Loss: {test_loss} | Test Mov Loss {test_mov_loss} | Test Act Loss {test_act_loss}\")\n",
        "    # SHOW\n",
        "    display_test(model,\"BEST\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
