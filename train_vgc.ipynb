{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LI_OL6WFnwvQ"
   },
   "source": [
    "# Sketch AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION\n",
    "model_version = 8\n",
    "model_name    = f\"vgc-v{model_version}\"\n",
    "model_bt_file = model_name + \"-bt.pt\"\n",
    "model_bv_file = model_name + \"-bv.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODE\n",
    "TUNE_HIDDEN = False  # Grover esto en True\n",
    "TUNE_LR     = False  # Eric   esto en True\n",
    "TRAIN_MODEL = True   # Rodrigo esto en True\n",
    "LOAD_MODEL  = False\n",
    "TEST_MODEL  = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametros testeados para tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division de ejecucion\n",
    "- Eric: Learning Rate\n",
    "- Grover: Hidden sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNE HYPERPARAMETERS\n",
    "hidden_size_encoder_ls = [128,256,512]\n",
    "hidden_size_decoder_ls = [256,512,2048]\n",
    "learning_rate_ls       = [1e-2,1e-3,1e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL HYPERPARAMETERS\n",
    "#Encoder-Decoder config\n",
    "encoding_size       = 5\n",
    "latent_size         = 128\n",
    "hidden_size_encoder = 512\n",
    "hidden_size_decoder = 2048\n",
    "encoder_layers      = 1\n",
    "decoder_layers      = 1\n",
    "# TRAINING HYPERPARAMETERS\n",
    "num_epochs    = 100\n",
    "batch_size    = 64\n",
    "learning_rate = 1e-3\n",
    "CLASS_WEIGTHS = (1,10,100)\n",
    "MOV_WEIGHT    = 1\n",
    "ACT_WEIGHT    = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CONFIG\n",
    "TRAIN_MAX_SIZE = 6400\n",
    "VALID_MAX_SIZE = 320\n",
    "TEST_MAX_SIZE = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CANVAS CONFIG\n",
    "WIDTH  = 1084\n",
    "HEIGHT = 526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM CONFIG\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING CONFIG\n",
    "DELTA_SIZE = 2\n",
    "CLASS_SIZE = 3\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "START_TOKEN         = (0,0,1,0,0)\n",
    "PAD_MARKER          = -100\n",
    "PAD_TOKEN           = (0,0,PAD_MARKER,PAD_MARKER,PAD_MARKER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H-BhfDgoFDb"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kgTu6s6HqYje"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1N-RzpO-vjIk",
    "outputId": "06c1b78c-3632-4dd5-9c9a-192aa509a882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU drawsvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LZqKBDt4vjsa"
   },
   "outputs": [],
   "source": [
    "import drawsvg as draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZpmLfCDTTaqA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9g4wWGQ2rNm",
    "outputId": "8b9a6c58-4a5d-4afe-abdf-57afdbc022a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qfnR3ZDoJec"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E72l7SyPrMvR"
   },
   "source": [
    "### Local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sMhkCyxVrBaS"
   },
   "outputs": [],
   "source": [
    "dataset_dir = \"dataset\"\n",
    "dataset_filename = \"dataset.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SQpM5fWrqxTq"
   },
   "outputs": [],
   "source": [
    "!mkdir $dataset_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jGLIoVqrTHc"
   },
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rAVso7DooI_U"
   },
   "outputs": [],
   "source": [
    "dataset_url = \"https://storage.googleapis.com/quickdraw_dataset/sketchrnn/airplane.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3sdqAZfri6l",
    "outputId": "eda3d62a-6b26-4076-c3a9-451f9fae30b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 11.0M  100 11.0M    0     0  6779k      0  0:00:01  0:00:01 --:--:-- 6775k\n",
      "FILE SIZE:\n",
      "12M\tdataset\n"
     ]
    }
   ],
   "source": [
    "!curl -o \"$dataset_dir\"/\"$dataset_filename\" \"$dataset_url\"\n",
    "!echo \"FILE SIZE:\"\n",
    "!du -h \"$dataset_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOjjpsUEt6xT"
   },
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "N2bXk3Rtt2PI"
   },
   "outputs": [],
   "source": [
    "dataset = np.load(dataset_dir+\"/\"+dataset_filename, encoding='latin1', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dRq4oFkMuG0i"
   },
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    raw_train_dataset = dataset[\"train\"]\n",
    "    raw_valid_dataset = dataset[\"valid\"]\n",
    "    raw_test_dataset = dataset[\"test\"]\n",
    "else:\n",
    "    raw_train_dataset = dataset[\"train\"][:TRAIN_MAX_SIZE]\n",
    "    raw_valid_dataset = dataset[\"valid\"][:VALID_MAX_SIZE]\n",
    "    raw_test_dataset = dataset[\"test\"][:TEST_MAX_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for stroke3 in raw_train_dataset:\n",
    "    max_len = max(max_len,stroke3.shape[0])\n",
    "for stroke3 in raw_valid_dataset:\n",
    "    max_len = max(max_len,stroke3.shape[0])\n",
    "for stroke3 in raw_test_dataset:\n",
    "    max_len = max(max_len,stroke3.shape[0])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6ecNdoCgxdxR"
   },
   "outputs": [],
   "source": [
    "def render_sheep(stroke_3):\n",
    "  d = draw.Drawing(WIDTH, HEIGHT, origin=\"center\")\n",
    "  d.append(draw.Rectangle(-WIDTH/2,-HEIGHT/2,WIDTH,HEIGHT, fill='rgb(255,255,255)'))\n",
    "  x=0\n",
    "  y=0\n",
    "  svg_points = [0,0]\n",
    "  for dx,dy,end in stroke_3:\n",
    "    x+=dx\n",
    "    y+=dy\n",
    "    svg_points.append(x)\n",
    "    svg_points.append(y)\n",
    "    if end:\n",
    "      lines = draw.Lines(*svg_points,fill='none', stroke='black')\n",
    "      svg_points = []\n",
    "      d.append(lines)\n",
    "  # use d.as_svg() to extract full svg\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "id": "HtJd6XQFuru9",
    "outputId": "769c795e-f828-490d-fc28-e1a7d55c6b2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 70000\n",
      "Test size: 2500\n",
      "Validate size: 2500\n",
      "Stroke 3 input shape: n x 3\n",
      "*(n max = 250)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
       "     width=\"1084\" height=\"526\" viewBox=\"-542.0 -263.0 1084 526\">\n",
       "<defs>\n",
       "</defs>\n",
       "<rect x=\"-542.0\" y=\"-263.0\" width=\"1084\" height=\"526\" fill=\"rgb(255,255,255)\" />\n",
       "<path d=\"M0,0 L48,0 L154,12 L297,37 L337,48 L406,77 L434,95 L398,133 L372,149 L349,154 L298,153 L210,136 L102,125 L-32,101 L-120,94 L-153,82 L-163,71 L-161,57 L-143,19 L-121,1 L-103,-6 L-72,-6 L-42,9 L-25,12 L-1,12 L22,3\" fill=\"none\" stroke=\"black\" />\n",
       "<path d=\"M97,-9 L230,-181 L247,-208 L253,-92 L251,10\" fill=\"none\" stroke=\"black\" />\n",
       "<path d=\"M69,92 L-2,184 L-81,295 L-74,294 L-40,276 L33,231 L128,178 L161,154 L185,129 L202,116\" fill=\"none\" stroke=\"black\" />\n",
       "<path d=\"M323,69 L321,97 L326,101 L340,106 L386,113\" fill=\"none\" stroke=\"black\" />\n",
       "</svg>"
      ],
      "text/plain": [
       "<drawsvg.drawing.Drawing at 0x7fae18be5c90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train size:\", raw_train_dataset.shape[0])\n",
    "print(\"Test size:\", raw_test_dataset.shape[0])\n",
    "print(\"Validate size:\", raw_valid_dataset.shape[0])\n",
    "print(\"Stroke 3 input shape: n x\",raw_train_dataset[0].shape[1])\n",
    "print(\"*(n max = 250)\")\n",
    "\n",
    "d = render_sheep(raw_train_dataset[5]) #100\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kECiQL_I6Qou"
   },
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Xa1weZDw8aDj"
   },
   "outputs": [],
   "source": [
    "def render_sequence(sequence):\n",
    "  d = draw.Drawing(WIDTH, HEIGHT, origin=\"center\")\n",
    "  d.append(draw.Rectangle(-WIDTH/2,-HEIGHT/2,WIDTH,HEIGHT, fill='rgb(255,255,255)'))\n",
    "  x=0\n",
    "  y=0\n",
    "  svg_points = []\n",
    "  for i, (dx,dy,line,lift,end) in enumerate(sequence):\n",
    "    x+=dx.item()\n",
    "    y+=dy.item()\n",
    "    svg_points.append(x)\n",
    "    svg_points.append(y)\n",
    "    if lift or end:\n",
    "      lines = draw.Lines(*svg_points,fill='none', stroke='black')\n",
    "      svg_points = []\n",
    "      d.append(lines)\n",
    "    if end:\n",
    "      break\n",
    "    if i == len(sequence)-1:\n",
    "      # Force draw\n",
    "      lines = draw.Lines(*svg_points,fill='none', stroke='black')\n",
    "      svg_points = []\n",
    "      d.append(lines)\n",
    "  # use d.as_svg() to extract full svg\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "eGCnxaXF6QLb"
   },
   "outputs": [],
   "source": [
    "def preprocess_sequence(stroke_3):\n",
    "  result = [START_TOKEN,]\n",
    "  for i in range(len(stroke_3)):\n",
    "    dx     = stroke_3[i][0]\n",
    "    dy     = stroke_3[i][1]\n",
    "    action = stroke_3[i][2]\n",
    "\n",
    "    end = int(i == len(stroke_3)-1)\n",
    "    lift = action if i != len(stroke_3)-1 else 0\n",
    "    line = 1-action if i != len(stroke_3)-1 else 0\n",
    "\n",
    "    fv = (dx,dy,line,lift,end)\n",
    "    result.append(fv)\n",
    "  # PAD ENDING\n",
    "  for i in range(MAX_SEQUENCE_LENGTH-len(stroke_3)):\n",
    "    fv = PAD_TOKEN\n",
    "    result.append(fv)\n",
    "  return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "y3GRelPo9aOO"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "  new_dataset = np.array([preprocess_sequence(elem) for elem in dataset])\n",
    "  new_dataset = torch.from_numpy(new_dataset)\n",
    "  new_dataset = new_dataset.float()\n",
    "  return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Y3IR_AXX9__7"
   },
   "outputs": [],
   "source": [
    "train_dataset = preprocess_dataset(raw_train_dataset)\n",
    "valid_dataset = preprocess_dataset(raw_valid_dataset)\n",
    "test_dataset = preprocess_dataset(raw_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UmVeQ425Drbc",
    "outputId": "05ce3417-bb9a-44df-8d77-e963f23260ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-24.,  -6.,   1.,   0.,   0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#With padding\n",
    "train_dataset.shape\n",
    "train_dataset[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "EdJSwOIr8SD4",
    "outputId": "a87b7d5e-9671-4f13-e3fd-27f4be532b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0.,    0.,    1.,    0.,    0.],\n",
      "        [  48.,    0.,    1.,    0.,    0.],\n",
      "        [ 106.,   12.,    1.,    0.,    0.],\n",
      "        [ 143.,   25.,    1.,    0.,    0.],\n",
      "        [  40.,   11.,    1.,    0.,    0.],\n",
      "        [  69.,   29.,    1.,    0.,    0.],\n",
      "        [  28.,   18.,    1.,    0.,    0.],\n",
      "        [ -36.,   38.,    1.,    0.,    0.],\n",
      "        [ -26.,   16.,    1.,    0.,    0.],\n",
      "        [ -23.,    5.,    1.,    0.,    0.],\n",
      "        [ -51.,   -1.,    1.,    0.,    0.],\n",
      "        [ -88.,  -17.,    1.,    0.,    0.],\n",
      "        [-108.,  -11.,    1.,    0.,    0.],\n",
      "        [-134.,  -24.,    1.,    0.,    0.],\n",
      "        [ -88.,   -7.,    1.,    0.,    0.],\n",
      "        [ -33.,  -12.,    1.,    0.,    0.],\n",
      "        [ -10.,  -11.,    1.,    0.,    0.],\n",
      "        [   2.,  -14.,    1.,    0.,    0.],\n",
      "        [  18.,  -38.,    1.,    0.,    0.],\n",
      "        [  22.,  -18.,    1.,    0.,    0.],\n",
      "        [  18.,   -7.,    1.,    0.,    0.],\n",
      "        [  31.,    0.,    1.,    0.,    0.],\n",
      "        [  30.,   15.,    1.,    0.,    0.],\n",
      "        [  17.,    3.,    1.,    0.,    0.],\n",
      "        [  24.,    0.,    1.,    0.,    0.],\n",
      "        [  23.,   -9.,    0.,    1.,    0.],\n",
      "        [  75.,  -12.,    1.,    0.,    0.],\n",
      "        [ 133., -172.,    1.,    0.,    0.],\n",
      "        [  17.,  -27.,    1.,    0.,    0.],\n",
      "        [   6.,  116.,    1.,    0.,    0.],\n",
      "        [  -2.,  102.,    0.,    1.,    0.],\n",
      "        [-182.,   82.,    1.,    0.,    0.],\n",
      "        [ -71.,   92.,    1.,    0.,    0.],\n",
      "        [ -79.,  111.,    1.,    0.,    0.],\n",
      "        [   7.,   -1.,    1.,    0.,    0.],\n",
      "        [  34.,  -18.,    1.,    0.,    0.],\n",
      "        [  73.,  -45.,    1.,    0.,    0.],\n",
      "        [  95.,  -53.,    1.,    0.,    0.],\n",
      "        [  33.,  -24.,    1.,    0.,    0.],\n",
      "        [  24.,  -25.,    1.,    0.,    0.],\n",
      "        [  17.,  -13.,    0.,    1.,    0.],\n",
      "        [ 121.,  -47.,    1.,    0.,    0.],\n",
      "        [  -2.,   28.,    1.,    0.,    0.],\n",
      "        [   5.,    4.,    1.,    0.,    0.],\n",
      "        [  14.,    5.,    1.,    0.,    0.],\n",
      "        [  46.,    7.,    0.,    0.,    1.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.],\n",
      "        [   0.,    0., -100., -100., -100.]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
       "     width=\"1084\" height=\"526\" viewBox=\"-542.0 -263.0 1084 526\">\n",
       "<defs>\n",
       "</defs>\n",
       "<rect x=\"-542.0\" y=\"-263.0\" width=\"1084\" height=\"526\" fill=\"rgb(255,255,255)\" />\n",
       "<path d=\"M0.0,0.0 L48.0,0.0 L154.0,12.0 L297.0,37.0 L337.0,48.0 L406.0,77.0 L434.0,95.0 L398.0,133.0 L372.0,149.0 L349.0,154.0 L298.0,153.0 L210.0,136.0 L102.0,125.0 L-32.0,101.0 L-120.0,94.0 L-153.0,82.0 L-163.0,71.0 L-161.0,57.0 L-143.0,19.0 L-121.0,1.0 L-103.0,-6.0 L-72.0,-6.0 L-42.0,9.0 L-25.0,12.0 L-1.0,12.0 L22.0,3.0\" fill=\"none\" stroke=\"black\" />\n",
       "<path d=\"M97.0,-9.0 L230.0,-181.0 L247.0,-208.0 L253.0,-92.0 L251.0,10.0\" fill=\"none\" stroke=\"black\" />\n",
       "<path d=\"M69.0,92.0 L-2.0,184.0 L-81.0,295.0 L-74.0,294.0 L-40.0,276.0 L33.0,231.0 L128.0,178.0 L161.0,154.0 L185.0,129.0 L202.0,116.0\" fill=\"none\" stroke=\"black\" />\n",
       "<path d=\"M323.0,69.0 L321.0,97.0 L326.0,101.0 L340.0,106.0 L386.0,113.0\" fill=\"none\" stroke=\"black\" />\n",
       "</svg>"
      ],
      "text/plain": [
       "<drawsvg.drawing.Drawing at 0x7faeb801cfd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#s = preprocess_sequence(raw_train_dataset[100])\n",
    "s = train_dataset[5]\n",
    "s=s.to(device)\n",
    "print(s)\n",
    "render_sequence(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUyMn7NC-qVy"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias:\n",
    "https://github.com/bentrevett/pytorch-seq2seq/blob/rewrite/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "gBjsAQsWmN5v"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size_in, rnn_layers, latent_size, dropout=0.1):\n",
    "    super(Encoder,self).__init__()\n",
    "    # Input config\n",
    "    self.input_size  = input_size\n",
    "    # RNN config\n",
    "    self.hidden_size_in  = hidden_size_in\n",
    "    self.latent_size = latent_size\n",
    "    self.rnn_layers  = rnn_layers\n",
    "    # Layers\n",
    "    self.rnn = nn.LSTM(self.input_size,\n",
    "                       self.hidden_size_in,\n",
    "                       self.rnn_layers,\n",
    "                       bidirectional=True)\n",
    "    self.fc_latent = nn.Linear(2*2*self.rnn_layers*self.hidden_size_in,\n",
    "                      self.latent_size)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  def forward(self, x):\n",
    "    # x shape: (seq_length, batch, input_size)\n",
    "    num_batch = x.shape[1]\n",
    "    # RNN\n",
    "    # output shape: (seq_length, batch, D * hidden_size)\n",
    "    # hidden shape: (D * num_layers, batch, hidden_size)\n",
    "    # cell   shape: (D * num_layers, batch, hidden_size)\n",
    "    # * Note: D=2 if bidiretional=True else 1\n",
    "    outputs, (h, c) = self.rnn(x)\n",
    "    # hc   shape: (D * num_layers, batch, 2*hidden_size)\n",
    "    hc = torch.cat( (h,c), 2 )\n",
    "    hc = torch.movedim(hc,0,2)\n",
    "    # hc   shape: (batch, 2*D*num_layers*hidden_size)\n",
    "    hc = hc.reshape(num_batch,-1)\n",
    "    # context  shape: (batch, latent_size)\n",
    "    context =  self.fc_latent(self.dropout(hc))\n",
    "\n",
    "    ### RETURN CONTEXT VECTOR\n",
    "    return context.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "djBHv5ZRs7_F"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, input_size, output_size_delta, output_size_class, hidden_size, rnn_layers, latent_size, dropout=0.1):\n",
    "    super(Decoder,self).__init__()\n",
    "    # Input-Output config\n",
    "    self.input_size        = input_size\n",
    "    self.latent_size       = latent_size\n",
    "    self.output_size_delta = output_size_delta\n",
    "    self.output_size_class = output_size_class\n",
    "    # RNN config\n",
    "    self.hidden_size = hidden_size\n",
    "    self.rnn_layers  = rnn_layers\n",
    "    # Layers\n",
    "    self.fc_hidden = nn.Linear(self.latent_size,\n",
    "                      self.hidden_size*self.rnn_layers)\n",
    "    self.fc_cell   = nn.Linear(self.latent_size,\n",
    "                      self.hidden_size*self.rnn_layers)\n",
    "    self.rnn = nn.LSTM(self.input_size+self.latent_size,\n",
    "                       self.hidden_size,\n",
    "                       self.rnn_layers)\n",
    "    self.fc_dlt  = nn.Linear(self.hidden_size,\n",
    "                        self.output_size_delta)\n",
    "    self.fc_cls  = nn.Linear(self.hidden_size,\n",
    "                        self.output_size_class)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  def forward(self, x, context, hidden, cell):\n",
    "    ### GET RNN INPUT\n",
    "    # x shape:         (batch, input_size) but need N batches of seq_length 1\n",
    "    x      = x.unsqueeze(0) #(1, batch, input_size)\n",
    "    # context shape:  (batch, latent_size) but need N batches of seq_length 1\n",
    "    context = context.unsqueeze(0) #(1, batch, input_size)\n",
    "    # rnn_input shape: (1, batch, input_size+latent_size)\n",
    "    rnn_input = torch.cat((x, context),2).contiguous()\n",
    "\n",
    "    ### RNN\n",
    "    # output shape: (1, batch, D * hidden_size)\n",
    "    # hidden shape: (D * num_layers, batch, hidden_size)\n",
    "    # cell   shape: (D * num_layers, batch, hidden_size)\n",
    "    # * Note: D=2 if bidiretional=True else 1\n",
    "    output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "\n",
    "    ### GET PREDICTIONS\n",
    "    # pred_delta shape: (1, batch, output_size_delta)\n",
    "    # pred_class shape: (1, batch, output_size_class)\n",
    "    pred_delta =  self.fc_dlt(self.dropout(output))\n",
    "    pred_class =  self.fc_cls(self.dropout((output))) #torch.sigmoid(self.fc_cls(self.dropout((output))))\n",
    "    # pred_delta shape: (batch, output_size_delta)\n",
    "    # pred_class shape: (batch, output_size_class)\n",
    "    pred_delta = pred_delta.squeeze(0)\n",
    "    pred_class = pred_class.squeeze(0)\n",
    "\n",
    "    ### RETURN PREDICTIONS AND STATE\n",
    "    return pred_delta, pred_class, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "b9oIC3Giw-qH"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Stroke2Stroke(nn.Module):\n",
    "  def __init__(self, encoding_size, delta_size, class_size, hidden_size_encoder, hidden_size_decoder, encoder_layers, decoder_layers, latent_size, device, dropout=0.1):\n",
    "    super(Stroke2Stroke, self).__init__()\n",
    "    assert encoding_size == delta_size+class_size\n",
    "    # SIZE CONFIG\n",
    "    self.encoding_size = encoding_size\n",
    "    self.delta_size    = delta_size\n",
    "    self.class_size    = class_size\n",
    "    self.latent_size   = latent_size\n",
    "    # ENCODER/DECODER\n",
    "    # Encoder: (input_size, hidden_size_in, rnn_layers, latent_size, dropout=0.1)\n",
    "    # Decoder: (input_size, output_size_delta, output_size_class, hidden_size, rnn_layers, latent_size, dropout=0.1):\n",
    "    self.encoder = Encoder(encoding_size, hidden_size_encoder, encoder_layers, latent_size)\n",
    "    self.decoder = Decoder(encoding_size, self.delta_size, self.class_size, hidden_size_decoder, decoder_layers, latent_size)\n",
    "    # DEVICE\n",
    "    self.device = device\n",
    "    # INITIAL STATE LAYERS\n",
    "    self.fc_hidden = nn.Linear(self.latent_size,\n",
    "                      hidden_size_decoder*decoder_layers)\n",
    "    self.fc_cell   = nn.Linear(self.latent_size,\n",
    "                      hidden_size_decoder*decoder_layers)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def get_initial_states(self, context):\n",
    "    num_batch = context.shape[0]\n",
    "    ### EXTRACT HIDDEN AND CELL FROM CONTEXT VECTOR\n",
    "    # context shape: (batch, latent_size)\n",
    "    # hidden  shape: (batch, num_layers*hidden_size)\n",
    "    # cell    shape: (batch, num_layers*hidden_size)\n",
    "    hidden =  self.fc_hidden(self.dropout(context))\n",
    "    cell   =  self.fc_cell(self.dropout(context))\n",
    "    # hidden  shape: (num_layers, batch, hidden_size)\n",
    "    # cell    shape: (num_layers, batch, hidden_size)\n",
    "    hidden = hidden.reshape(num_batch,self.decoder.rnn_layers,self.decoder.hidden_size)\n",
    "    cell   = cell.reshape(num_batch,self.decoder.rnn_layers,self.decoder.hidden_size)\n",
    "    hidden = torch.movedim(hidden,0,1).contiguous()\n",
    "    cell   = torch.movedim(cell,0,1).contiguous()\n",
    "    ### RETURN INITIAL STATES\n",
    "    return hidden, cell\n",
    "\n",
    "  def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "    ### GET SHAPES\n",
    "    # source shape: (src_seq_length, batch, encoding_size)\n",
    "    # target shape: (trg_seq_length, batch, encoding_size)\n",
    "    batch_size  = source.shape[1]\n",
    "    target_len  = target.shape[0]\n",
    "    output_size = target.shape[2]\n",
    "\n",
    "    ### RESREVE DELTA AND CLASS PREDICTION TENSORS\n",
    "    # predictions_dlt shape: (src_seq_length, batch, output_size_delta)\n",
    "    # predictions_cls shape: (src_seq_length, batch, output_size_class)\n",
    "    predictions_dlt = torch.zeros(target_len, batch_size, self.delta_size).to(device)\n",
    "    predictions_cls = torch.zeros(target_len, batch_size, self.class_size).to(device)\n",
    "\n",
    "    ### GET CONTEXT FROM ENCODER\n",
    "    # hidden shape: (D * num_layers, batch, hidden_size)\n",
    "    # cell   shape: (D * num_layers, batch, hidden_size)\n",
    "    context = self.encoder(source)\n",
    "\n",
    "    ### GET INITIAL STATES\n",
    "    hidden, cell = self.get_initial_states(context)\n",
    "\n",
    "    ### START TOKEN\n",
    "    x = target[0]\n",
    "    ### GENERATE\n",
    "    for t in range(1, target_len):\n",
    "      # pred_delta shape: (batch, output_size_delta)\n",
    "      # pred_class shape: (batch, output_size_class)\n",
    "      pred_delta, pred_class, hidden, cell = self.decoder(x, context, hidden, cell)\n",
    "      predictions_dlt[t] = pred_delta\n",
    "      predictions_cls[t] = pred_class\n",
    "\n",
    "      # Get max from one hot encoded actions\n",
    "      #action = pred_class.argmax(1)\n",
    "      #best_action_guess = F.one_hot(action, num_classes=self.class_size).float()\n",
    "\n",
    "      # Create best_guess\n",
    "      # best_guess shape: (batch, encoding_size)\n",
    "      #best_guess = torch.cat((pred_delta, best_action_guess),1)\n",
    "      best_guess = torch.cat((pred_delta, pred_class),1)\n",
    "\n",
    "      # Select next input\n",
    "      x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "    return predictions_dlt, predictions_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbnW2lw2BW__"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IzsFqHzOdgs",
    "outputId": "607a32ed-e146-4128-8319-3e23ed0eab71"
   },
   "outputs": [],
   "source": [
    "# Parameter count\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "bKT5p24NS_cT"
   },
   "outputs": [],
   "source": [
    "# DATA LOADERS\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "G9Ty529kSkHs"
   },
   "outputs": [],
   "source": [
    "# Train function\n",
    "c_weights = torch.tensor(CLASS_WEIGTHS)\n",
    "mse_fn = nn.MSELoss(reduction=\"none\")\n",
    "cross_entropy_fn =  nn.CrossEntropyLoss(ignore_index=PAD_MARKER)#, weight=c_weights)\n",
    "\n",
    "def train_fn(model, data_loader, optimizer, clip=5.0, teacher_forcing_ratio=0.5): #clip=1.0\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_mov_loss = 0\n",
    "    epoch_act_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        # src = [batch_size, src_length, encoding_size]\n",
    "        # trg = [batch_size, trg_length, encoding_size]\n",
    "        src = batch.to(device)\n",
    "        trg = batch.to(device)\n",
    "\n",
    "        # src = [src_length, batch_size, encoding_size]\n",
    "        # trg = [trg_length, batch_size, encoding_size]\n",
    "        src = torch.movedim(src,0,1)\n",
    "        trg = torch.movedim(trg,0,1)\n",
    "\n",
    "        # Zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # predictions_dlt = [trg_length, batch_size, delta_size]\n",
    "        # predictions_cls = [trg_length, batch_size, class_size]\n",
    "        predictions_dlt, predictions_cls = model(src, trg, teacher_forcing_ratio)\n",
    "\n",
    "        encoding_size = src.shape[2]\n",
    "        delta_size    = predictions_dlt.shape[2]\n",
    "        class_size    = predictions_cls.shape[2]\n",
    "\n",
    "\n",
    "        # Compute loss\n",
    "        # mask   = [(trg_length-1), batch_size]\n",
    "        # counts = [1]\n",
    "        mask = ~trg[1:,:,-1].eq(PAD_MARKER)       #[(trg_length-1), batch_size]\n",
    "        counts = torch.sum(mask)                  \n",
    "        ## For one-hot\n",
    "        # action_pred = [(trg length - 1) * batch_size, action_size]\n",
    "        action_pred = predictions_cls[1:].view(-1, class_size)\n",
    "        # action_trg = [(trg length - 1) * batch_size]\n",
    "        action_trg  = trg[1:,:,delta_size:].reshape(-1, class_size) # [(trg length - 1) * batch_size, action_size]\n",
    "        action_trg = action_trg.argmax(1)                           # [(trg length - 1) * batch_size]\n",
    "        # action_mask \n",
    "        action_mask = mask.reshape(-1) # [(trg length - 1) * batch_size]\n",
    "        #            Index  if mask else PAD_MARKER\n",
    "        action_trg = action_trg*action_mask + PAD_MARKER*(~action_mask) # [(trg length - 1) * batch_size]\n",
    "        # ACTION LOSS\n",
    "        action_loss = cross_entropy_fn(action_pred, action_trg)\n",
    "        action_loss = ACT_WEIGHT*action_loss\n",
    "        ## For regression\n",
    "        # movement_prd = [(trg_length-1), batch_size, movement_size]\n",
    "        movement_pred = predictions_dlt[1:]\n",
    "        # movement_trg\n",
    "        movement_trg  = trg[1:,:,:delta_size]\n",
    "        # mv_mask   = [(trg_length-1), batch_size, movement_size]\n",
    "        mv_mask = mask.expand((delta_size,-1,-1))    #[movement_size, (trg_length-1), batch_size]\n",
    "        mv_mask = torch.movedim(mv_mask,0,2)            #[(trg_length-1), batch_size, movement_size]\n",
    "        # Apply mask\n",
    "        # https://discuss.pytorch.org/t/how-to-correctly-weight-mse-loss-for-padded-sequences/176211\n",
    "        movement_pred_masked = movement_pred * mv_mask\n",
    "        movement_trg_masked  = movement_trg  * mv_mask\n",
    "        # Get MSE\n",
    "        movement_loss = mse_fn(movement_pred_masked, movement_trg_masked)\n",
    "        movement_loss = torch.sum(movement_loss)/(counts*DELTA_SIZE)\n",
    "        movement_loss = MOV_WEIGHT*movement_loss\n",
    "        # Total loss\n",
    "        loss = movement_loss + action_loss\n",
    "\n",
    "        # Backward propagation\n",
    "        loss.backward()\n",
    "        # Clip to avoid gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Running total\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_mov_loss += movement_loss.item()\n",
    "        epoch_act_loss += action_loss.item()\n",
    "    return epoch_loss / len(data_loader), epoch_mov_loss/ len(data_loader), epoch_act_loss/ len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_mov_loss = 0\n",
    "    epoch_act_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src = batch.to(device)\n",
    "            trg = batch.to(device)\n",
    "            src = torch.movedim(src,0,1)\n",
    "            trg = torch.movedim(trg,0,1)\n",
    "            predictions_dlt, predictions_cls = model(src, trg, 0) # turn off teacher forcing\n",
    "\n",
    "            encoding_size = src.shape[2]\n",
    "            delta_size    = predictions_dlt.shape[2]\n",
    "            class_size    = predictions_cls.shape[2]\n",
    "\n",
    "            # Compute loss\n",
    "            # mask   = [(trg_length-1), batch_size]\n",
    "            # counts = [1]\n",
    "            mask = ~trg[1:,:,-1].eq(PAD_MARKER)       #[(trg_length-1), batch_size]\n",
    "            counts = torch.sum(mask)                  \n",
    "            ## For one-hot\n",
    "            # action_pred = [(trg length - 1) * batch_size, action_size]\n",
    "            action_pred = predictions_cls[1:].view(-1, class_size)\n",
    "            # action_trg = [(trg length - 1) * batch_size]\n",
    "            action_trg  = trg[1:,:,delta_size:].reshape(-1, class_size) # [(trg length - 1) * batch_size, action_size]\n",
    "            action_trg = action_trg.argmax(1)                           # [(trg length - 1) * batch_size]\n",
    "            # action_mask \n",
    "            action_mask = mask.reshape(-1) # [(trg length - 1) * batch_size]\n",
    "            #            Index  if mask else PAD_MARKER\n",
    "            action_trg = action_trg*action_mask + PAD_MARKER*(~action_mask) # [(trg length - 1) * batch_size]\n",
    "            # ACTION LOSS\n",
    "            action_loss = cross_entropy_fn(action_pred, action_trg)\n",
    "            action_loss = ACT_WEIGHT*action_loss\n",
    "            ## For regression\n",
    "            # movement_prd = [(trg_length-1), batch_size, movement_size]\n",
    "            movement_pred = predictions_dlt[1:]\n",
    "            # movement_trg\n",
    "            movement_trg  = trg[1:,:,:delta_size]\n",
    "            # mv_mask   = [(trg_length-1), batch_size, movement_size]\n",
    "            mv_mask = mask.expand((delta_size,-1,-1))    #[movement_size, (trg_length-1), batch_size]\n",
    "            mv_mask = torch.movedim(mv_mask,0,2)            #[(trg_length-1), batch_size, movement_size]\n",
    "            # Apply mask\n",
    "            # https://discuss.pytorch.org/t/how-to-correctly-weight-mse-loss-for-padded-sequences/176211\n",
    "            movement_pred_masked = movement_pred * mv_mask\n",
    "            movement_trg_masked  = movement_trg  * mv_mask\n",
    "            # Get MSE\n",
    "            movement_loss = mse_fn(movement_pred_masked, movement_trg_masked)\n",
    "            movement_loss = torch.sum(movement_loss)/(counts*DELTA_SIZE)\n",
    "            movement_loss = MOV_WEIGHT*movement_loss\n",
    "            # Total loss\n",
    "            loss = movement_loss + action_loss\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_mov_loss += movement_loss.item()\n",
    "            epoch_act_loss += action_loss.item()\n",
    "    return epoch_loss / len(data_loader), epoch_mov_loss/ len(data_loader), epoch_act_loss/ len(data_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFijN-Gik-GV",
    "outputId": "f2d92c3f-911b-4145-d2a3-9a8f446ee6a1"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer):\n",
    "  best_valid_loss = float(\"inf\")\n",
    "  best_train_loss = float(\"inf\")\n",
    "  train_losses = []\n",
    "  valid_losses = []\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    print(f\"Epoch: [{epoch}/{num_epochs}]\")\n",
    "    \n",
    "    train_loss, train_mov_loss, train_act_loss = train_fn(\n",
    "      model,\n",
    "      train_dataloader,\n",
    "      optimizer,\n",
    "    )\n",
    "    print(f\"Train loss: {train_loss} | Mov Loss {train_mov_loss} | Act Loss {train_act_loss}\")\n",
    "  \n",
    "    valid_loss, valid_mov_loss, valid_act_loss = evaluate_fn(\n",
    "      model,\n",
    "      valid_dataloader,\n",
    "    )\n",
    "    print(f\"Valid loss: {valid_loss} | Mov Loss {valid_mov_loss} | Act Loss {valid_act_loss}\")    \n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "      best_valid_loss = valid_loss\n",
    "      torch.save(model.state_dict(), model_bv_file)\n",
    "\n",
    "    if train_loss < best_train_loss:\n",
    "      best_train_loss = train_loss\n",
    "      torch.save(model.state_dict(), model_bt_file)\n",
    "\n",
    "  # Convierte las listas a un DataFrame de Pandas\n",
    "  df = pd.DataFrame({\n",
    "    'epoch': range(1, num_epochs+1),\n",
    "    'train_loss': train_losses,\n",
    "    'valid_loss': valid_losses\n",
    "  })\n",
    "\n",
    "  # Guarda el DataFrame como un archivo CSV\n",
    "  df.to_csv('losses.csv', index=False)      \n",
    "  return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TUNE_HIDDEN:\n",
    "    for (hidden_size_encoder,hidden_size_decoder) in zip(hidden_size_encoder_ls,hidden_size_decoder_ls):\n",
    "        # Model\n",
    "        model = Stroke2Stroke(encoding_size=encoding_size, delta_size=DELTA_SIZE, class_size=CLASS_SIZE, hidden_size_encoder=hidden_size_encoder, hidden_size_decoder=hidden_size_decoder,\n",
    "                            encoder_layers=encoder_layers,decoder_layers=decoder_layers,latent_size=latent_size,\n",
    "                            device=device).to(device)\n",
    "        print(f\"MODEL:{hidden_size_encoder}he-{hidden_size_decoder}hd\")\n",
    "        print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
    "        model.apply(init_weights)\n",
    "        # Optimizer\n",
    "        optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "        # Set names to save file to\n",
    "        model_bt_file = model_name +f\"-{hidden_size_encoder}he-{hidden_size_decoder}hd\"+ \"-bt.pt\"\n",
    "        model_bv_file = model_name +f\"-{hidden_size_encoder}he-{hidden_size_decoder}hd\"+ \"-bv.pt\"\n",
    "\n",
    "        # TRAIN\n",
    "        train(model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TUNE_LR:\n",
    "    for learning_rate in learning_rate_ls:\n",
    "        # Model\n",
    "        model = Stroke2Stroke(encoding_size=encoding_size, delta_size=DELTA_SIZE, class_size=CLASS_SIZE, hidden_size_encoder=hidden_size_encoder, hidden_size_decoder=hidden_size_decoder,\n",
    "                            encoder_layers=encoder_layers,decoder_layers=decoder_layers,latent_size=latent_size,\n",
    "                            device=device).to(device)\n",
    "        print(f\"MODEL:{learning_rate}lr\")\n",
    "        print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
    "        model.apply(init_weights)\n",
    "        # Optimizer\n",
    "        optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "        # Set names to save file to\n",
    "        model_bt_file = model_name +f\"-{learning_rate}lr\"+ \"-bt.pt\"\n",
    "        model_bv_file = model_name +f\"-{learning_rate}lr\"+ \"-bv.pt\"\n",
    "\n",
    "        # TRAIN\n",
    "        train(model,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST MODEL\n",
      "The model has 21,338,245 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    model_bt_file = model_name + \"-BEST\"  + \"-bt.pt\"\n",
    "    model_bv_file = model_name + \"-BEST\"  + \"-bv.pt\"\n",
    "    # Model\n",
    "    model = Stroke2Stroke(encoding_size=encoding_size, delta_size=DELTA_SIZE, class_size=CLASS_SIZE, hidden_size_encoder=hidden_size_encoder, hidden_size_decoder=hidden_size_decoder,\n",
    "                        encoder_layers=encoder_layers,decoder_layers=decoder_layers,latent_size=latent_size,\n",
    "                        device=device).to(device)\n",
    "    print(f\"BEST MODEL\")\n",
    "    print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
    "    model.apply(init_weights)\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/100]\n",
      "Train loss: 2025.1447614429203 | Mov Loss 2021.506580080803 | Act Loss 3.63818059303207\n",
      "Valid loss: 2286.734930419922 | Mov Loss 2283.5121643066404 | Act Loss 3.2227860629558562\n",
      "Epoch: [1/100]\n",
      "Train loss: 1788.7216539121416 | Mov Loss 1785.5569932229776 | Act Loss 3.164660825807826\n",
      "Valid loss: 2160.3998779296876 | Mov Loss 2157.1137969970705 | Act Loss 3.2860714316368105\n",
      "Epoch: [2/100]\n",
      "Train loss: 1704.194408465565 | Mov Loss 1701.031607674942 | Act Loss 3.162798586013766\n",
      "Valid loss: 2296.641793823242 | Mov Loss 2292.733746337891 | Act Loss 3.9080497086048127\n",
      "Epoch: [3/100]\n",
      "Train loss: 1646.1596156927524 | Mov Loss 1642.9211899445306 | Act Loss 3.2384262610179197\n",
      "Valid loss: 2223.489370727539 | Mov Loss 2219.9336639404296 | Act Loss 3.5557147681713106\n",
      "Epoch: [4/100]\n",
      "Train loss: 1592.418937892321 | Mov Loss 1589.1180658148674 | Act Loss 3.3008729156236107\n",
      "Valid loss: 2158.4141174316405 | Mov Loss 2154.743621826172 | Act Loss 3.67049954533577\n",
      "Epoch: [5/100]\n",
      "Train loss: 1545.6416111027302 | Mov Loss 1542.2917945764182 | Act Loss 3.3498170587218876\n",
      "Valid loss: 2113.665237426758 | Mov Loss 2109.99990234375 | Act Loss 3.6653190672397615\n",
      "Epoch: [6/100]\n",
      "Train loss: 1500.7635816612453 | Mov Loss 1497.4054939228176 | Act Loss 3.35808889499748\n",
      "Valid loss: 2126.4114990234375 | Mov Loss 2122.350875854492 | Act Loss 4.060622411966324\n",
      "Epoch: [7/100]\n",
      "Train loss: 1461.7143395683665 | Mov Loss 1458.3426537661928 | Act Loss 3.3716849788233394\n",
      "Valid loss: 2114.731773376465 | Mov Loss 2110.942149353027 | Act Loss 3.789641034603119\n",
      "Epoch: [8/100]\n",
      "Train loss: 1433.5409077813465 | Mov Loss 1430.1191904462016 | Act Loss 3.4217174059730127\n",
      "Valid loss: 2122.562368774414 | Mov Loss 2118.9042419433595 | Act Loss 3.6581228494644167\n",
      "Epoch: [9/100]\n",
      "Train loss: 1405.5289832190142 | Mov Loss 1402.0480411397039 | Act Loss 3.4809420167637044\n",
      "Valid loss: 2171.9042083740233 | Mov Loss 2167.911163330078 | Act Loss 3.99304980635643\n",
      "Epoch: [10/100]\n",
      "Train loss: 1394.1548269435737 | Mov Loss 1390.6244684250742 | Act Loss 3.530358840604151\n",
      "Valid loss: 2154.677630615234 | Mov Loss 2151.0575744628904 | Act Loss 3.6200286209583283\n",
      "Epoch: [11/100]\n",
      "Train loss: 1383.1022855490173 | Mov Loss 1379.5599053363696 | Act Loss 3.542381070431669\n",
      "Valid loss: 2192.433572387695 | Mov Loss 2188.1235015869142 | Act Loss 4.310079419612885\n",
      "Epoch: [12/100]\n",
      "Train loss: 1367.9674793640918 | Mov Loss 1364.3940998195949 | Act Loss 3.5733791572302307\n",
      "Valid loss: 2127.3164764404296 | Mov Loss 2123.173989868164 | Act Loss 4.142479920387268\n",
      "Epoch: [13/100]\n",
      "Train loss: 1368.1647685328176 | Mov Loss 1364.5842974730776 | Act Loss 3.580468641120729\n",
      "Valid loss: 2171.4884643554688 | Mov Loss 2167.338494873047 | Act Loss 4.149959045648575\n",
      "Epoch: [14/100]\n",
      "Train loss: 1357.6228236001414 | Mov Loss 1353.968917874575 | Act Loss 3.6539037929374514\n",
      "Valid loss: 2166.8769775390624 | Mov Loss 2162.589028930664 | Act Loss 4.28793877363205\n",
      "Epoch: [15/100]\n",
      "Train loss: 1368.4296303144104 | Mov Loss 1364.7684857300474 | Act Loss 3.6611453303471344\n",
      "Valid loss: 2134.493536376953 | Mov Loss 2130.477673339844 | Act Loss 4.015865474939346\n",
      "Epoch: [16/100]\n",
      "Train loss: 1359.717898185772 | Mov Loss 1356.0262450613966 | Act Loss 3.6916531858322172\n",
      "Valid loss: 2223.0980682373047 | Mov Loss 2218.58078918457 | Act Loss 4.517276781797409\n",
      "Epoch: [17/100]\n",
      "Train loss: 1367.6975488191986 | Mov Loss 1363.9587246687365 | Act Loss 3.738823241028097\n",
      "Valid loss: 2146.5405029296876 | Mov Loss 2142.305355834961 | Act Loss 4.235132640600204\n",
      "Epoch: [18/100]\n",
      "Train loss: 1362.5201270401587 | Mov Loss 1358.7263118318492 | Act Loss 3.7938165427126003\n",
      "Valid loss: 2187.84401550293 | Mov Loss 2183.240121459961 | Act Loss 4.603907179832459\n",
      "Epoch: [19/100]\n",
      "Train loss: 1398.5930573011883 | Mov Loss 1394.6922614116772 | Act Loss 3.900794267436486\n",
      "Valid loss: 2225.658224487305 | Mov Loss 2221.040328979492 | Act Loss 4.617888283729553\n",
      "Epoch: [20/100]\n",
      "Train loss: 1396.624364765732 | Mov Loss 1392.6855248934173 | Act Loss 3.9388391751475917\n",
      "Valid loss: 2356.95432434082 | Mov Loss 2352.281359863281 | Act Loss 4.672988951206207\n",
      "Epoch: [21/100]\n",
      "Train loss: 1405.4541098195411 | Mov Loss 1401.5118026036014 | Act Loss 3.9423080669678545\n",
      "Valid loss: 2139.3893920898436 | Mov Loss 2134.938236999512 | Act Loss 4.451146429777145\n",
      "Epoch: [22/100]\n",
      "Train loss: 1395.9908876297025 | Mov Loss 1392.027438203857 | Act Loss 3.963448347513593\n",
      "Valid loss: 2247.9766174316405 | Mov Loss 2243.813040161133 | Act Loss 4.163585525751114\n",
      "Epoch: [23/100]\n",
      "Train loss: 1395.577717615438 | Mov Loss 1391.6000581005592 | Act Loss 3.9776612263057087\n",
      "Valid loss: 2225.0550079345703 | Mov Loss 2220.3266418457033 | Act Loss 4.728364777565003\n",
      "Epoch: [24/100]\n",
      "Train loss: 1415.7861817968394 | Mov Loss 1411.7187105001 | Act Loss 4.067469431233798\n",
      "Valid loss: 2257.818408203125 | Mov Loss 2253.4496673583985 | Act Loss 4.368748527765274\n",
      "Epoch: [25/100]\n",
      "Train loss: 1408.5187813098078 | Mov Loss 1404.426967028072 | Act Loss 4.091814525602505\n",
      "Valid loss: 2190.735711669922 | Mov Loss 2186.0803970336915 | Act Loss 4.655312156677246\n",
      "Epoch: [26/100]\n",
      "Train loss: 1428.683550567906 | Mov Loss 1424.4593176135634 | Act Loss 4.224231112591745\n",
      "Valid loss: 2289.266696166992 | Mov Loss 2283.3099731445313 | Act Loss 5.956722891330719\n",
      "Epoch: [27/100]\n",
      "Train loss: 1451.2375169715672 | Mov Loss 1446.9918105772251 | Act Loss 4.245704949011096\n",
      "Valid loss: 2360.3539123535156 | Mov Loss 2355.537991333008 | Act Loss 4.815924447774887\n",
      "Epoch: [28/100]\n",
      "Train loss: 1469.1866710042168 | Mov Loss 1464.9090241984868 | Act Loss 4.277648976123747\n",
      "Valid loss: 2268.4388122558594 | Mov Loss 2263.7021728515624 | Act Loss 4.736650598049164\n",
      "Epoch: [29/100]\n",
      "Train loss: 1552.847803203018 | Mov Loss 1547.806161270281 | Act Loss 5.041640596590269\n",
      "Valid loss: 2364.4484680175783 | Mov Loss 2357.2841461181642 | Act Loss 7.164334225654602\n",
      "Epoch: [30/100]\n",
      "Train loss: 1594.5298491483197 | Mov Loss 1589.6573553835017 | Act Loss 4.872492516498461\n",
      "Valid loss: 2306.2864151000977 | Mov Loss 2301.147605895996 | Act Loss 5.138824361562729\n",
      "Epoch: [31/100]\n",
      "Train loss: 1540.5348030383234 | Mov Loss 1535.687399688107 | Act Loss 4.847401777830594\n",
      "Valid loss: 2328.436538696289 | Mov Loss 2322.698516845703 | Act Loss 5.738019418716431\n",
      "Epoch: [32/100]\n",
      "Train loss: 1543.9008592678815 | Mov Loss 1539.1313981469414 | Act Loss 4.769462147186181\n",
      "Valid loss: 2331.7167907714843 | Mov Loss 2326.4108154296873 | Act Loss 5.30597015619278\n",
      "Epoch: [33/100]\n",
      "Train loss: 1586.22522976891 | Mov Loss 1581.3647188120394 | Act Loss 4.860509390804841\n",
      "Valid loss: 2402.251791381836 | Mov Loss 2397.205529785156 | Act Loss 5.046262502670288\n",
      "Epoch: [34/100]\n",
      "Train loss: 1611.5401325679131 | Mov Loss 1606.6972870486745 | Act Loss 4.842845681598461\n",
      "Valid loss: 2549.65791015625 | Mov Loss 2544.276123046875 | Act Loss 5.3817769408226015\n",
      "Epoch: [35/100]\n",
      "Train loss: 1614.5137986317413 | Mov Loss 1609.6112560432616 | Act Loss 4.902544464446073\n",
      "Valid loss: 2403.654800415039 | Mov Loss 2398.5759674072265 | Act Loss 5.078811848163605\n",
      "Epoch: [36/100]\n",
      "Train loss: 1665.9636290164926 | Mov Loss 1660.900102353837 | Act Loss 5.063525273551453\n",
      "Valid loss: 2379.6641174316405 | Mov Loss 2374.1551177978517 | Act Loss 5.509003293514252\n",
      "Epoch: [37/100]\n",
      "Train loss: 1664.5168010704697 | Mov Loss 1659.4506064908378 | Act Loss 5.066195595199055\n",
      "Valid loss: 2359.295654296875 | Mov Loss 2353.916644287109 | Act Loss 5.378997015953064\n",
      "Epoch: [38/100]\n",
      "Train loss: 1692.7711511922275 | Mov Loss 1687.574786365795 | Act Loss 5.196363922446912\n",
      "Valid loss: 2420.3735229492186 | Mov Loss 2414.231652832031 | Act Loss 6.141885697841644\n",
      "Epoch: [39/100]\n",
      "Train loss: 2238.9754802696884 | Mov Loss 2226.2977233551974 | Act Loss 12.677756203158028\n",
      "Valid loss: 2633.180804443359 | Mov Loss 2626.9123718261717 | Act Loss 6.268438148498535\n",
      "Epoch: [40/100]\n",
      "Train loss: 1768.8162754763198 | Mov Loss 1763.3232142920904 | Act Loss 5.493060741825557\n",
      "Valid loss: 2437.4235961914064 | Mov Loss 2430.8049194335936 | Act Loss 6.618677341938019\n",
      "Epoch: [41/100]\n",
      "Train loss: 1776.093554620551 | Mov Loss 1770.4977094521253 | Act Loss 5.59584616699428\n",
      "Valid loss: 2496.043862915039 | Mov Loss 2488.915621948242 | Act Loss 7.1282384395599365\n",
      "Epoch: [42/100]\n",
      "Train loss: 1733.5241129036576 | Mov Loss 1728.043486600384 | Act Loss 5.480624922014895\n",
      "Valid loss: 2469.105364990234 | Mov Loss 2463.0991821289062 | Act Loss 6.006185066699982\n",
      "Epoch: [43/100]\n",
      "Train loss: 1755.0752044621943 | Mov Loss 1749.4898777600833 | Act Loss 5.585324194139275\n",
      "Valid loss: 2424.675152587891 | Mov Loss 2419.317529296875 | Act Loss 5.357626748085022\n",
      "Epoch: [44/100]\n",
      "Train loss: 1752.3037709684215 | Mov Loss 1746.8291785538306 | Act Loss 5.474592159173606\n",
      "Valid loss: 2516.00524597168 | Mov Loss 2510.6457916259765 | Act Loss 5.359445977210998\n",
      "Epoch: [45/100]\n",
      "Train loss: 1796.530199347292 | Mov Loss 1790.8189577873272 | Act Loss 5.7112403857424665\n",
      "Valid loss: 2636.4919555664064 | Mov Loss 2629.455810546875 | Act Loss 7.036132025718689\n",
      "Epoch: [46/100]\n",
      "Train loss: 1830.5721040547876 | Mov Loss 1824.7528066129528 | Act Loss 5.819298297003375\n",
      "Valid loss: 2513.861508178711 | Mov Loss 2507.94811706543 | Act Loss 5.9133886933326725\n",
      "Epoch: [47/100]\n",
      "Train loss: 1789.6604002790434 | Mov Loss 1783.8245505937928 | Act Loss 5.835848175847334\n",
      "Valid loss: 2458.6102630615233 | Mov Loss 2450.5937225341795 | Act Loss 8.016543829441071\n",
      "Epoch: [48/100]\n",
      "Train loss: 1868.6714865843164 | Mov Loss 1862.0135942141796 | Act Loss 6.657892617708586\n",
      "Valid loss: 2494.8173919677733 | Mov Loss 2486.9261108398437 | Act Loss 7.8912842750549315\n",
      "Epoch: [49/100]\n",
      "Train loss: 1891.650314972649 | Mov Loss 1884.969957090165 | Act Loss 6.680357315858496\n",
      "Valid loss: 2528.899072265625 | Mov Loss 2521.1196166992186 | Act Loss 7.779450607299805\n",
      "Epoch: [50/100]\n",
      "Train loss: 1903.1139078924816 | Mov Loss 1896.8397850580582 | Act Loss 6.274122446938886\n",
      "Valid loss: 2535.1188720703126 | Mov Loss 2527.4227600097656 | Act Loss 7.696121954917908\n",
      "Epoch: [51/100]\n",
      "Train loss: 1938.609326238824 | Mov Loss 1931.910734019724 | Act Loss 6.698593040706906\n",
      "Valid loss: 2647.280450439453 | Mov Loss 2641.0075714111326 | Act Loss 6.272885596752166\n",
      "Epoch: [52/100]\n",
      "Train loss: 1920.606858610889 | Mov Loss 1913.6927326209366 | Act Loss 6.914125777249798\n",
      "Valid loss: 2530.5250854492188 | Mov Loss 2521.104113769531 | Act Loss 9.42095365524292\n",
      "Epoch: [53/100]\n",
      "Train loss: 1978.7987853892325 | Mov Loss 1971.678297639111 | Act Loss 7.120487849062932\n",
      "Valid loss: 2595.9894775390626 | Mov Loss 2589.604119873047 | Act Loss 6.385369002819061\n",
      "Epoch: [54/100]\n",
      "Train loss: 1973.77728383066 | Mov Loss 1966.629707517746 | Act Loss 7.147576338194406\n",
      "Valid loss: 2521.9038513183596 | Mov Loss 2513.1789489746093 | Act Loss 8.724895691871643\n",
      "Epoch: [55/100]\n",
      "Train loss: 1990.0623445667775 | Mov Loss 1982.4196970379985 | Act Loss 7.64264749434574\n",
      "Valid loss: 2709.336279296875 | Mov Loss 2699.342254638672 | Act Loss 9.994026970863342\n",
      "Epoch: [56/100]\n",
      "Train loss: 1967.9303379093708 | Mov Loss 1960.6549395852376 | Act Loss 7.2753967833475395\n",
      "Valid loss: 2571.328173828125 | Mov Loss 2562.846630859375 | Act Loss 8.48155755996704\n",
      "Epoch: [57/100]\n",
      "Train loss: 1939.5693890503599 | Mov Loss 1932.711536916562 | Act Loss 6.85785132483111\n",
      "Valid loss: 2544.9234466552734 | Mov Loss 2536.9121856689453 | Act Loss 8.011284160614014\n",
      "Epoch: [58/100]\n",
      "Train loss: 1933.373606456917 | Mov Loss 1926.7363347083167 | Act Loss 6.637270644552311\n",
      "Valid loss: 2640.0997528076173 | Mov Loss 2633.1901428222654 | Act Loss 6.9096178889274595\n",
      "Epoch: [59/100]\n",
      "Train loss: 1926.193037461973 | Mov Loss 1919.414140941892 | Act Loss 6.778897333842527\n",
      "Valid loss: 2540.008251953125 | Mov Loss 2531.984899902344 | Act Loss 8.02335765361786\n",
      "Epoch: [60/100]\n",
      "Train loss: 1936.0356178632385 | Mov Loss 1929.298771664691 | Act Loss 6.736847382794786\n",
      "Valid loss: 2573.013995361328 | Mov Loss 2565.8215148925783 | Act Loss 7.192486226558685\n",
      "Epoch: [61/100]\n",
      "Train loss: 1951.9816201609276 | Mov Loss 1945.0628554991001 | Act Loss 6.918765685240137\n",
      "Valid loss: 2562.49609375 | Mov Loss 2552.891860961914 | Act Loss 9.604233574867248\n",
      "Epoch: [62/100]\n",
      "Train loss: 1927.4662917449225 | Mov Loss 1920.804906981083 | Act Loss 6.6613854019471885\n",
      "Valid loss: 2620.0356018066404 | Mov Loss 2611.7282958984374 | Act Loss 8.307303416728974\n",
      "Epoch: [63/100]\n",
      "Train loss: 2006.7697832013398 | Mov Loss 1999.3073975948355 | Act Loss 7.462384773128865\n",
      "Valid loss: 2576.74482421875 | Mov Loss 2568.002734375 | Act Loss 8.742097246646882\n",
      "Epoch: [64/100]\n",
      "Train loss: 2018.4771181765595 | Mov Loss 2010.621136039441 | Act Loss 7.855983482636307\n",
      "Valid loss: 2716.667803955078 | Mov Loss 2709.105126953125 | Act Loss 7.562689077854157\n",
      "Epoch: [65/100]\n",
      "Train loss: 2103.24347325522 | Mov Loss 2094.8021659781334 | Act Loss 8.441309187922226\n",
      "Valid loss: 2666.6506408691407 | Mov Loss 2659.1371154785156 | Act Loss 7.5135337829589846\n",
      "Epoch: [66/100]\n",
      "Train loss: 2069.5548225137827 | Mov Loss 2061.2811310539732 | Act Loss 8.273688443182156\n",
      "Valid loss: 2615.3685913085938 | Mov Loss 2605.2668212890626 | Act Loss 10.101773881912232\n",
      "Epoch: [67/100]\n",
      "Train loss: 1999.2287297501643 | Mov Loss 1991.896037155793 | Act Loss 7.332692829738805\n",
      "Valid loss: 2664.394396972656 | Mov Loss 2654.5781982421877 | Act Loss 9.816216504573822\n",
      "Epoch: [68/100]\n",
      "Train loss: 2045.4697656160736 | Mov Loss 2037.6954036621987 | Act Loss 7.774364249579867\n",
      "Valid loss: 2678.1412048339844 | Mov Loss 2670.431433105469 | Act Loss 7.709791731834412\n",
      "Epoch: [69/100]\n",
      "Train loss: 2070.155078727541 | Mov Loss 2061.776167456367 | Act Loss 8.378910286552946\n",
      "Valid loss: 2571.2685485839843 | Mov Loss 2562.45290222168 | Act Loss 8.815642046928406\n",
      "Epoch: [70/100]\n",
      "Train loss: 2023.8595181592423 | Mov Loss 2016.3329116291277 | Act Loss 7.526610717477049\n",
      "Valid loss: 2593.506234741211 | Mov Loss 2584.8610107421873 | Act Loss 8.645213782787323\n",
      "Epoch: [71/100]\n",
      "Train loss: 2047.1118016774737 | Mov Loss 2039.331215811386 | Act Loss 7.780584989561677\n",
      "Valid loss: 2671.6736236572265 | Mov Loss 2662.873110961914 | Act Loss 8.80052672624588\n",
      "Epoch: [72/100]\n",
      "Train loss: 2065.657276551074 | Mov Loss 2057.7181544887953 | Act Loss 7.939122674234171\n",
      "Valid loss: 2637.069985961914 | Mov Loss 2630.1818572998045 | Act Loss 6.8881232142448425\n",
      "Epoch: [73/100]\n",
      "Train loss: 2055.351953035735 | Mov Loss 2047.8217047041032 | Act Loss 7.53024769003692\n",
      "Valid loss: 2590.688616943359 | Mov Loss 2583.0782958984373 | Act Loss 7.610331392288208\n",
      "Epoch: [74/100]\n",
      "Train loss: 2049.0535189055004 | Mov Loss 2041.5970233589464 | Act Loss 7.456494391073474\n",
      "Valid loss: 2598.836834716797 | Mov Loss 2590.857037353516 | Act Loss 7.97979166507721\n",
      "Epoch: [75/100]\n",
      "Train loss: 2035.4185111483446 | Mov Loss 2027.9012456750957 | Act Loss 7.51726796291428\n",
      "Valid loss: 2713.1448181152346 | Mov Loss 2704.946453857422 | Act Loss 8.198341345787048\n",
      "Epoch: [76/100]\n",
      "Train loss: 2081.5902815963495 | Mov Loss 2073.5434447572698 | Act Loss 8.046835854022987\n",
      "Valid loss: 2667.3418090820314 | Mov Loss 2659.5507873535157 | Act Loss 7.791016304492951\n",
      "Epoch: [77/100]\n",
      "Train loss: 2177.901924314621 | Mov Loss 2169.388218184058 | Act Loss 8.513707233739291\n",
      "Valid loss: 2680.1416259765624 | Mov Loss 2671.4996337890625 | Act Loss 8.641971945762634\n",
      "Epoch: [78/100]\n",
      "Train loss: 2089.734238200911 | Mov Loss 2081.34852044979 | Act Loss 8.385721340039966\n",
      "Valid loss: 2649.6936309814455 | Mov Loss 2640.80348815918 | Act Loss 8.890158224105836\n",
      "Epoch: [79/100]\n",
      "Train loss: 2115.6403304244745 | Mov Loss 2107.3299239191756 | Act Loss 8.310405917315858\n",
      "Valid loss: 2665.479113769531 | Mov Loss 2657.632443237305 | Act Loss 7.846668684482575\n",
      "Epoch: [80/100]\n",
      "Train loss: 2065.133246217829 | Mov Loss 2057.0286102015966 | Act Loss 8.10463481890872\n",
      "Valid loss: 2598.6740844726564 | Mov Loss 2589.5278411865233 | Act Loss 9.146232891082764\n",
      "Epoch: [81/100]\n",
      "Train loss: 2067.6568053418146 | Mov Loss 2059.671385826097 | Act Loss 7.985416499090805\n",
      "Valid loss: 2618.4571746826173 | Mov Loss 2611.0483581542967 | Act Loss 7.408798193931579\n",
      "Epoch: [82/100]\n",
      "Train loss: 2120.463203276751 | Mov Loss 2111.963037578018 | Act Loss 8.500162747485982\n",
      "Valid loss: 2792.104522705078 | Mov Loss 2782.7863220214845 | Act Loss 9.318210482597351\n",
      "Epoch: [83/100]\n",
      "Train loss: 2091.117286584495 | Mov Loss 2083.2221699772194 | Act Loss 7.89511629258039\n",
      "Valid loss: 2590.3084228515627 | Mov Loss 2582.2501586914063 | Act Loss 8.058284151554108\n",
      "Epoch: [84/100]\n",
      "Train loss: 2051.503651620701 | Mov Loss 2043.9112623587823 | Act Loss 7.592388444670376\n",
      "Valid loss: 2652.0682373046875 | Mov Loss 2643.8778564453123 | Act Loss 8.190378379821777\n",
      "Epoch: [85/100]\n",
      "Train loss: 2050.4520213460137 | Mov Loss 2042.5956902931116 | Act Loss 7.856329142066635\n",
      "Valid loss: 2602.1769287109373 | Mov Loss 2594.56396484375 | Act Loss 7.61296843290329\n",
      "Epoch: [86/100]\n",
      "Train loss: 2056.92501646945 | Mov Loss 2048.942782497929 | Act Loss 7.982234426128799\n",
      "Valid loss: 2636.768029785156 | Mov Loss 2628.1901306152345 | Act Loss 8.577910053730012\n",
      "Epoch: [87/100]\n",
      "Train loss: 2026.2172217778793 | Mov Loss 2018.2903051847077 | Act Loss 7.926920537320962\n",
      "Valid loss: 2595.2809295654297 | Mov Loss 2585.024737548828 | Act Loss 10.256193923950196\n",
      "Epoch: [88/100]\n",
      "Train loss: 2031.4703187262555 | Mov Loss 2023.6245245506384 | Act Loss 7.845794132902156\n",
      "Valid loss: 2727.7078186035155 | Mov Loss 2720.0880645751954 | Act Loss 7.619755566120148\n",
      "Epoch: [89/100]\n",
      "Train loss: 2086.2195569584096 | Mov Loss 2078.059457726627 | Act Loss 8.160099001645605\n",
      "Valid loss: 2671.822613525391 | Mov Loss 2664.2089294433595 | Act Loss 7.613679850101471\n",
      "Epoch: [90/100]\n",
      "Train loss: 2084.949023482133 | Mov Loss 2076.830404389711 | Act Loss 8.118620870754096\n",
      "Valid loss: 2660.7014556884765 | Mov Loss 2652.6387634277344 | Act Loss 8.062693619728089\n",
      "Epoch: [91/100]\n",
      "Train loss: 2055.0132737517138 | Mov Loss 2046.8256093919604 | Act Loss 8.187666802345289\n",
      "Valid loss: 2618.5471252441407 | Mov Loss 2610.11940612793 | Act Loss 8.427721679210663\n",
      "Epoch: [92/100]\n",
      "Train loss: 2073.2702074347294 | Mov Loss 2065.102593179595 | Act Loss 8.167613871136792\n",
      "Valid loss: 2633.4280029296874 | Mov Loss 2624.0041320800783 | Act Loss 9.423882317543029\n",
      "Epoch: [93/100]\n",
      "Train loss: 2053.3915733323456 | Mov Loss 2045.3905426527508 | Act Loss 8.00103229055457\n",
      "Valid loss: 2683.0178192138674 | Mov Loss 2674.90158996582 | Act Loss 8.116231906414033\n",
      "Epoch: [94/100]\n",
      "Train loss: 2065.642658352198 | Mov Loss 2057.899434816685 | Act Loss 7.743223930407703\n",
      "Valid loss: 2633.3649505615235 | Mov Loss 2626.089822387695 | Act Loss 7.275115704536438\n",
      "Epoch: [95/100]\n",
      "Train loss: 2055.998790120294 | Mov Loss 2047.8885846181588 | Act Loss 8.110205384887333\n",
      "Valid loss: 2660.754440307617 | Mov Loss 2651.6371459960938 | Act Loss 9.11730318069458\n",
      "Epoch: [96/100]\n",
      "Train loss: 2098.790581572426 | Mov Loss 2089.962462039927 | Act Loss 8.828116219919822\n",
      "Valid loss: 2809.1179748535155 | Mov Loss 2800.9208129882813 | Act Loss 8.197150075435639\n",
      "Epoch: [97/100]\n",
      "Train loss: 2095.855462501428 | Mov Loss 2086.8653187307186 | Act Loss 8.990141689450475\n",
      "Valid loss: 2680.386834716797 | Mov Loss 2669.603143310547 | Act Loss 10.783685684204102\n",
      "Epoch: [98/100]\n",
      "Train loss: 2087.929240838701 | Mov Loss 2079.120435418333 | Act Loss 8.80880693500177\n",
      "Valid loss: 2630.045361328125 | Mov Loss 2620.3168823242186 | Act Loss 9.728481936454774\n",
      "Epoch: [99/100]\n",
      "Train loss: 2084.67561644392 | Mov Loss 2075.879997964751 | Act Loss 8.795618623004753\n",
      "Valid loss: 2666.541888427734 | Mov Loss 2657.141943359375 | Act Loss 9.399948048591614\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAIN_MODEL:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     train_losses, valid_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [42], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_bt_file)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Convierte las listas a un DataFrame de Pandas\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     36\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     37\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_losses,\n\u001b[1;32m     38\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: valid_losses\n\u001b[1;32m     39\u001b[0m })\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Guarda el DataFrame como un archivo CSV\u001b[39;00m\n\u001b[1;32m     42\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlosses.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)      \n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # Train\n",
    "    train_losses, valid_losses = train(model,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    model.load_state_dict(torch.load(model_bv_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = train_dataset[5].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "oUdIUih1BWIX"
   },
   "outputs": [],
   "source": [
    "if TEST_MODEL:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = test_seq.to(device)\n",
    "        trg = test_seq.to(device)\n",
    "        src = torch.movedim(src,0,1)\n",
    "        trg = torch.movedim(trg,0,1)\n",
    "        prediction_dlt, prediction_act = model(src, trg, 0) # turn off teacher forcing\n",
    "        prediction = torch.cat( (prediction_dlt,prediction_act) ,2)\n",
    "        prediction = prediction.squeeze()\n",
    "        action = prediction[:,DELTA_SIZE:]\n",
    "        action = action.argmax(dim=1)\n",
    "        best_guess = F.one_hot(DELTA_SIZE+action, num_classes=5).float()\n",
    "        best_guess[:,:DELTA_SIZE] = prediction[:,:DELTA_SIZE]\n",
    "    best_guess = best_guess[1:]\n",
    "    render_sequence(best_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL:\n",
    "    render_sequence(test_seq[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
